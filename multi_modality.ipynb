{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48fcaf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58edf681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8372b180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43c9727e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipVisionConfig:\n",
    "    \n",
    "    def __init__(self,\n",
    "                hidden_size=768,\n",
    "                intermediate_size=3072,\n",
    "                num_hidden_layers=12,\n",
    "                num_attention_heads=12,\n",
    "                num_channels=3,\n",
    "                image_size=224,\n",
    "                patch_size=16,\n",
    "                layer_norm_eps=1e-6,\n",
    "                attention_dropout=0.0,\n",
    "                num_image_tokens: int=None,\n",
    "                **kwargs\n",
    "                ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.num_channels = num_channels\n",
    "        self.patch_size = patch_size\n",
    "        self.image_size = image_size\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.num_image_tokens = num_image_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "909ea800",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipVisionEmbedding(nn.Embedding):\n",
    "    \n",
    "    \n",
    "    def __init__(self,config: SiglipVisionConfig):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.image_size = config.image_size\n",
    "        self.patch_size = config.patch_size\n",
    "        \n",
    "        \n",
    "        self.patch_embedding = nn.Conv2d(\n",
    "            in_channels=config.num_channels,\n",
    "            out_channels=self.embed_dim,\n",
    "            kernel_size=self.patch_size,\n",
    "            stride=self.patch_size,\n",
    "            padding='valid',\n",
    "        )\n",
    "        \n",
    "        self.num_patches = (self.image_size // self.patch_size) ** 2\n",
    "        self.num_positions = self.num_patches\n",
    "        self.position_embedding = nn.Embedding(self.num_positions,self.embed_dim)\n",
    "        \n",
    "        self.register_buffer(\n",
    "          'position_ids',\n",
    "          torch.arange(self.num_positions).expand((1,-1)),\n",
    "          persistent=False\n",
    "        )\n",
    "    \n",
    "    def forward(self,pixel_values: torch.FloatTensor) -> torch.Tensor:\n",
    "        _,_,height,width = pixel_values.shape\n",
    "        \n",
    "        patch_embeds = self.patch_embedding(pixel_values)\n",
    "        \n",
    "        embeddings = patch_embeds.flatten(2)\n",
    "        \n",
    "        embeddings = embeddings.transpose(1,2)\n",
    "        \n",
    "        embeddings = embeddings + self.position_embedding(self.position_ids)\n",
    "        \n",
    "        return embeddings\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15428f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.dropout = config.attention_dropout\n",
    "        \n",
    "        self.k_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.v_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.q_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.out_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "    \n",
    "    def forward(self,hidden_states: torch.Tensor) -> Tuple[torch.Tensor,\n",
    "                                                          Optional[torch.Tensor]]:\n",
    "        # hidden_states: [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        batch_size,seq_len,_ = hidden_states.size()\n",
    "        # query_states: [Batch_Size, Num_Patches, Embed_Dim]\n",
    "\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        \n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        \n",
    "        values_states = self.v_proj(hidden_states)\n",
    "        \n",
    "        # query_states: [Batch_Size, Num_Heads, Num_Patches, Head_Dim]\n",
    "        \n",
    "        query_states = query_states.view(batch_size,seq_len,self.num_heads,self.head_dim).transpose(1,2)\n",
    "        \n",
    "        \n",
    "        key_states = key_states.view(batch_size,seq_len,self.num_heads,self.head_dim).transpose(1,2)\n",
    "\n",
    "        values_states = values_states.view(batch_size,seq_len,self.num_heads,self.head_dim).transpose(1,2)\n",
    "        \n",
    "        \n",
    "        attn_weights = (torch.matmul(query_states,key_states.transpose(2,3))**self.scale)\n",
    "        \n",
    "        if attn_weights.size() != (batch_size, self.num_heads, seq_len, seq_len):\n",
    "            raise ValueError(\n",
    "                f\"Attention weights should be of size {(batch_size, self.num_heads, seq_len, seq_len)}, but is\"\n",
    "                f\" {attn_weights.size()}\"\n",
    "            )\n",
    "        \n",
    "        attn_weights = nn.functional.softmax(attn_weights,dim=-1,dtype=torch.float32).to(query_states.dtype)\n",
    "        \n",
    "        attn_weights = nn.functional.dropout(attn_weights,p=self.dropout,training=self.training)\n",
    "        \n",
    "        attn_output = torch.matmul(attn_weights,values_states)\n",
    "        \n",
    "        if attn_output.size() != (batch_size, self.num_heads, seq_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(batch_size, self.num_heads, seq_len, self.head_dim)}, but is\"\n",
    "                f\" {attn_output.size()}\"\n",
    "            )\n",
    "        \n",
    "        \n",
    "        attn_output = attn_output.transpose(1,2).contiguous()\n",
    "        \n",
    "        attn_output = attn_output.reshape(batch_size,seq_len,self.embed_dim)\n",
    "        \n",
    "        attn_output = self.out_proj(attn_output)\n",
    "        \n",
    "        return attn_output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37e964c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36m.\u001b[m\u001b[m                              \u001b[1m\u001b[36mmarian-finetuned-kde4-en-to-fr\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36m..\u001b[m\u001b[m                             mmdl.ipynb\r\n",
      "\u001b[1m\u001b[36m.git\u001b[m\u001b[m                           multi_modality.ipynb\r\n",
      "\u001b[1m\u001b[36m.ipynb_checkpoints\u001b[m\u001b[m             simple_model_state_dict.pth\r\n",
      "final_data.json                tokens.json\r\n",
      "gpt_from_scratch.ipynb         translate_from_scratch.ipynb\r\n",
      "hg_learn.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6305ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/liuchu/hg_learn\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db6d300",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mem0",
   "language": "python",
   "name": "mem0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
