{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02959335",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8693a52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "547d7168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f778f11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8b1062f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bed2fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_image_token_processor_1 import PaliGemmaProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd39de90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from decoder_1 import KVCache,PaliGemmaForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2f4bce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_hf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e5b307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_inputs_to_device(model_inputs: dict,device: str):\n",
    "    model_inputs = {k:v.to(device) for k,v in model_inputs.items()}\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ecccfcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_inputs(\n",
    "   processor: PaliGemmaProcessor,\n",
    "    prompt: str,\n",
    "    image_file_path: str,\n",
    "    device: str\n",
    "):\n",
    "    image = Image.open(image_file_path)\n",
    "    images = [image]\n",
    "    prompts = [prompt]\n",
    "    model_inputs = processor(text=prompts,images=images)\n",
    "    model_inputs = move_inputs_to_device(model_inputs,device)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a747477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference(\n",
    "   model: PaliGemmaForConditionalGeneration,\n",
    "    processor: PaliGemmaProcessor,\n",
    "    device: str,\n",
    "    prompt: str,\n",
    "    image_file_path: str,\n",
    "    max_tokens_to_generate: int,\n",
    "    temperature: float,\n",
    "    top_p: float,\n",
    "    do_sample:bool\n",
    "):\n",
    "    model_inputs = get_model_inputs(processor,prompt,image_file_path,device)\n",
    "    input_ids = model_inputs['input_ids']\n",
    "    attention_mask = model_inputs['attention_mask']\n",
    "    pixel_values = model_inputs['pixel_values']\n",
    "    kv_cache = KVCache()\n",
    "    stop_token = processor.tokenizer.eos_token_id\n",
    "    generated_tokens = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for _ in range(max_tokens_to_generate):\n",
    "        \n",
    "        outputs = model(\n",
    "           input_ids=input_ids,\n",
    "            pixel_values=pixel_values,\n",
    "            attention_mask=attention_mask,\n",
    "            kv_cache=kv_cache\n",
    "        )\n",
    "        \n",
    "        kv_cache = outputs['kv_cache']\n",
    "        next_token_logits = outputs['logits'][:,-1,:]\n",
    "        if do_sample:\n",
    "            next_token_logits = torch.softmax(next_token_logits / temperature,dim=-1)\n",
    "            next_token = _sample_top_p(next_token_logits,top_p)\n",
    "        else:\n",
    "            next_token = torch.argmax(next_token_logits,dim=-1,keepdim=True)\n",
    "        assert next_token.size() == (1,1)\n",
    "        \n",
    "        next_token = next_token.squeeze(0)\n",
    "        generated_tokens.append(next_token)\n",
    "        \n",
    "        if next_token.item() == stop_token:\n",
    "            break\n",
    "        \n",
    "        input_ids = next_token.unsqueeze(-1)\n",
    "        attention_mask = torch.cat(\n",
    "          [attention_mask,torch.ones((1,1),device=input_ids.device)],dim=-1\n",
    "        )\n",
    "        \n",
    "    end_time = time.time()\n",
    "    latency = end_time - start_time\n",
    "    \n",
    "    generated_tokens = torch.cat(generated_tokens,dim=-1)\n",
    "    \n",
    "    decoded = processor.tokenizer.decode(generated_tokens,skip_special_tokens=True)\n",
    "\n",
    "    print('Result!!!')\n",
    "    print(prompt + decoded)\n",
    "    print(f'infernence latency: {latency:.2f} seconds')\n",
    "    \n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory = process.memory_info().rss / 1024 / 1024 / 1024\n",
    "    print(f'memory usage: {memory:.2f} GB')\n",
    "    \n",
    "    if device == 'cuda':\n",
    "        gpu_memory = torch.cuda.max_memory_allocated() / 1024 / 1024\n",
    "        print(f'gpu memory usage: {gpu_memory:.2f} GB')\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89986ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sample_top_p(probs: torch.Tensor,p: float):\n",
    "    probs_sort,prob_idx = torch.sort(probs,dim=-1,descending=True)\n",
    "    probs_sum = torch.cumsum(probs_sort,dim=-1)\n",
    "    mask = probs_sum - probs_sort > p\n",
    "    probs_sort.div_(probs_sort.sum(dim=-1,keepdim=True))\n",
    "    next_token = torch.multinomial(probs_sort,num_samples=1)\n",
    "    next_token = torch.gather(prob_idx,-1,next_token)\n",
    "    return next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6ee163a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "   model_path: str = None,\n",
    "    prompt: str = None,\n",
    "    image_file_path: str = None,\n",
    "    max_tokens_to_generate: int = 100,\n",
    "    temperature: float = 0.8,\n",
    "    top_p: float = 0.9,\n",
    "    do_sample: bool = False,\n",
    "    only_cpu: bool = False\n",
    "):\n",
    "    device = 'cpu'\n",
    "    if not only_cpu:\n",
    "        if torch.cuda.is_available():\n",
    "            device = 'cuda'\n",
    "        elif torch.backends.mps.is_available():\n",
    "            device = 'mps'\n",
    "    \n",
    "    print('device in use:',device)\n",
    "    print('loading model')\n",
    "    start_time = time.time()\n",
    "    model, tokenizer = load_hf_model(model_path,device)\n",
    "    model = model.to(device).eval()\n",
    "    \n",
    "    \n",
    "    num_image_tokens = model.config.vision_config.num_image_tokens\n",
    "    image_size = model.config.vision_config.image_size\n",
    "    \n",
    "    processor = PaliGemmaProcessor(tokenizer,num_image_tokens,image_size)\n",
    "\n",
    "    print(f'model loaded in {time.time() - start_time:.2f} seconds')\n",
    "    print('running inference')\n",
    "    with torch.no_grad():\n",
    "        test_inference(\n",
    "            model,\n",
    "            processor,\n",
    "            device,\n",
    "            prompt,\n",
    "            image_file_path,\n",
    "            max_tokens_to_generate,\n",
    "            temperature,\n",
    "            top_p,\n",
    "            do_sample\n",
    "        )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eaa2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device in use: mps\n",
      "loading model\n"
     ]
    }
   ],
   "source": [
    "main(\n",
    "    model_path=\"/Users/liuchu/vision-launguage-model-from-scratch/paligemma-3b-pt-224/\",\n",
    "    prompt=\"describe the building:\",\n",
    "    image_file_path=\"/Users/liuchu/vision-launguage-model-from-scratch/test_images/image.png\",\n",
    "    max_tokens_to_generate=100,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "    do_sample=False,\n",
    "    only_cpu=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635c1cf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80767d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed8785b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc700b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283638a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mem0",
   "language": "python",
   "name": "mem0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
