{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adf1de14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a745dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf4d2381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional,Tuple,List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "765d8a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a49fdbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49da85e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28243488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e46affc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KVCache():\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        \n",
    "        self.key_cache :List[torch.Tensor] = []\n",
    "        self.value_cache :List[torch.Tensor] = []\n",
    "    \n",
    "    \n",
    "    def num_items(self) -> int:\n",
    "        if len(self.key_cache) == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return self.key_cache[0].shape[-2]\n",
    "    \n",
    "    \n",
    "    def update(self,key_states:torch.Tensor,value_states:torch.Tensor,\n",
    "              layer_idx:int) -> Tuple[torch.Tensor,torch.Tensor]:\n",
    "        \n",
    "        if len(self.key_cache) <= layer_idx:\n",
    "            self.key_cache.append(key_states)\n",
    "            self.value_cache.append(value_states)\n",
    "        \n",
    "        else:\n",
    "            self.key_cache[layer_idx] = torch.cat([\n",
    "                self.key_cache[layer_idx],\n",
    "                key_states\n",
    "            ],dim=-2)\n",
    "            self.value_cache[layer_idx] = torch.cat(\n",
    "            [\n",
    "                self.value_cache[layer_idx],\n",
    "                value_states\n",
    "            ],dim=-2)\n",
    "        return self.key_cache[layer_idx],self.value_cache[layer_idx]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a98dbd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 key_cache shape: torch.Size([2, 3, 4]), value_cache shape: torch.Size([2, 3, 4])\n",
      "Layer 0 key_cache shape: torch.Size([2, 6, 4]), value_cache shape: torch.Size([2, 6, 4])\n",
      "Layer 1 key_cache shape: torch.Size([2, 3, 4]), value_cache shape: torch.Size([2, 3, 4])\n",
      "Number of items in cache: 6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 初始化 KVCache 实例\n",
    "kv_cache = KVCache()\n",
    "\n",
    "# 创建两个新的张量，假设每个张量的形状为 (2, 3, 4)\n",
    "key_tensor = torch.randn(2, 3, 4)\n",
    "value_tensor = torch.randn(2, 3, 4)\n",
    "\n",
    "# 更新第一个层的缓存\n",
    "key_cache, value_cache = kv_cache.update(key_tensor, value_tensor, 0)\n",
    "\n",
    "# 打印更新后的缓存\n",
    "print(f\"Layer 0 key_cache shape: {key_cache.shape}, value_cache shape: {value_cache.shape}\")\n",
    "\n",
    "# 创建另一个张量，形状也为 (2, 3, 4)\n",
    "new_key_tensor = torch.randn(2, 3, 4)\n",
    "new_value_tensor = torch.randn(2, 3, 4)\n",
    "\n",
    "# 更新第一个层（索引为0），将新张量拼接到原缓存上\n",
    "key_cache, value_cache = kv_cache.update(new_key_tensor, new_value_tensor, 0)\n",
    "\n",
    "# 打印更新后的缓存\n",
    "print(f\"Layer 0 key_cache shape: {key_cache.shape}, value_cache shape: {value_cache.shape}\")\n",
    "\n",
    "# 创建第二个层（索引为1）的新张量\n",
    "key_tensor_layer_1 = torch.randn(2, 3, 4)\n",
    "value_tensor_layer_1 = torch.randn(2, 3, 4)\n",
    "\n",
    "# 更新第二个层的缓存\n",
    "key_cache, value_cache = kv_cache.update(key_tensor_layer_1, value_tensor_layer_1, 1)\n",
    "\n",
    "# 打印第二个层的缓存\n",
    "print(f\"Layer 1 key_cache shape: {key_cache.shape}, value_cache shape: {value_cache.shape}\")\n",
    "\n",
    "# 查看缓存的条目数\n",
    "print(f\"Number of items in cache: {kv_cache.num_items()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "35f13231",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaConfig():\n",
    "    \"\"\"\n",
    "    Configuration class that stores all the hyperparameters needed for the Gemma model.\n",
    "    This includes things like model size (hidden_size), number of layers, attention heads, etc.\n",
    "    Think of it as a recipe card that defines how big and complex the model should be.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        hidden_size,\n",
    "        intermediate_size,\n",
    "        num_hidden_layers,\n",
    "        num_attention_heads,\n",
    "        num_key_value_heads,\n",
    "        head_dim=256,\n",
    "        max_position_embeddings=8192,\n",
    "        rms_norm_eps=1e-6,\n",
    "        rope_theta=10000.0,\n",
    "        attention_bias=False,\n",
    "        attention_dropout=0.0,\n",
    "        pad_token_id=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.num_key_value_heads = num_key_value_heads\n",
    "        self.rms_norm_eps = rms_norm_eps\n",
    "        self.rope_theta = rope_theta\n",
    "        self.attention_bias = attention_bias\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.pad_token_id = pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c401f554",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PaliGemmaConfig():\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vision_config=None,\n",
    "        text_config=None,\n",
    "        ignore_index=-100,\n",
    "        image_token_index=256000,\n",
    "        vocab_size=257152,\n",
    "        projection_dim=2048,\n",
    "        hidden_size=2048,\n",
    "        pad_token_id=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.ignore_index = ignore_index\n",
    "        self.image_token_index = image_token_index\n",
    "        self.vocab_size = vocab_size\n",
    "        self.projection_dim = projection_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vision_config = vision_config\n",
    "        self.is_encoder_decoder = False\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "        self.vision_config = VisionConfig(**vision_config)\n",
    "        self.text_config = text_config\n",
    "\n",
    "        self.text_config = GemmaConfig(**text_config, pad_token_id=pad_token_id)\n",
    "        self.vocab_size = self.text_config.vocab_size\n",
    "\n",
    "        self.text_config.num_image_tokens = (self.vision_config.image_size // self.vision_config.patch_size) ** 2\n",
    "        self.vision_config.projection_dim = projection_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c62e984c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaRMSNorm(nn.Module):\n",
    "    \n",
    "    def __init__(self,dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.zeros(dim))\n",
    "    \n",
    "    \n",
    "    def _norm(self,x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1,keepdim=True)+self.eps)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        output = self._norm(x.float())\n",
    "        \n",
    "        output = output * (1.0 + self.weight.float())\n",
    "        \n",
    "        return output.type_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31c0676c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x:\n",
      "tensor([[[ 1.0647,  0.3436,  0.1405,  0.7307],\n",
      "         [ 0.7142,  0.1823, -0.6825, -1.5986]],\n",
      "\n",
      "        [[-1.3156, -0.3296, -1.3917, -0.6262],\n",
      "         [-0.6318, -1.7972,  0.2147, -2.4969]]])\n",
      "Output after RMSNorm:\n",
      "tensor([[[ 1.5848,  0.5114,  0.2092,  1.0877],\n",
      "         [ 0.7566,  0.1932, -0.7230, -1.6934]],\n",
      "\n",
      "        [[-1.2888, -0.3229, -1.3633, -0.6134],\n",
      "         [-0.4014, -1.1418,  0.1364, -1.5864]]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 初始化 GemmaRMSNorm\n",
    "dim = 4  # 输入张量的特征维度\n",
    "eps = 1e-6\n",
    "rms_norm = GemmaRMSNorm(dim, eps)\n",
    "\n",
    "# 创建一个输入张量，形状为 (batch_size, dim)\n",
    "x = torch.randn(2,2, dim)  # batch_size = 2, dim = 4\n",
    "\n",
    "# 计算归一化后的输出\n",
    "output = rms_norm(x)\n",
    "\n",
    "# 打印输出结果\n",
    "print(\"Input x:\")\n",
    "print(x)\n",
    "print(\"Output after RMSNorm:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "374f590c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 4])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3412350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx = torch.ones(2,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b0550104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6c234323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1, 1, 2, 3, 4, 1])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xxx[None,None,None,None,:,:,:,None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3769e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a28d4095",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaRotaryEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self,dim,max_position_embeddings=2048,base=10000,device=None):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "        \n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0,self.dim,2,dtype=torch.int64).float()/self.dim))\n",
    "        \n",
    "        self.register_buffer('inv_freq',tensor=inv_freq,persistent=False)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def forward(self,x,position_ids,seq_len=None):\n",
    "        \n",
    "        self.inv_freq.to(x.device)\n",
    "        print('xxxx',self.inv_freq)\n",
    "        print('yyyy',self.inv_freq[None,:,None].shape,self.inv_freq[None,:,None])\n",
    "        \n",
    "        inv_freq_expand = self.inv_freq[None,:,None].float().expand(\n",
    "          position_ids.shape[0],-1,1\n",
    "        )\n",
    "        print('zzzzz',inv_freq_expand)\n",
    "        \n",
    "        position_ids_expanded = position_ids[:,None,:].float()\n",
    "        \n",
    "        print('position_ids_expanded',position_ids_expanded.shape,position_ids_expanded)\n",
    "        \n",
    "        device_type = x.device.type\n",
    "        \n",
    "        device_type = device_type if isinstance(device_type,str) and device_type != \"mps\" else \"cpu\"\n",
    "        \n",
    "        with torch.autocast(device_type=device_type,enabled=False):\n",
    "            print('xxxxyyyuuu',inv_freq_expand.shape,position_ids_expanded.shape)\n",
    "            t = (inv_freq_expand.float() @ position_ids_expanded.float())\n",
    "            print('tttt',t.shape,t)\n",
    "            freqs = t.transpose(1,2)\n",
    "            \n",
    "            \n",
    "\n",
    "            emb = torch.cat((freqs,freqs),dim=-1)\n",
    "            \n",
    "            cos = emb.cos()\n",
    "            sin = emb.sin()\n",
    "        \n",
    "        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a66444c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos shape torch.Size([2, 4])\n",
      "pos tensor([[0, 1, 2, 3],\n",
      "        [0, 1, 2, 3]])\n",
      "xxxx tensor([1.0000, 0.1000, 0.0100, 0.0010])\n",
      "yyyy torch.Size([1, 4, 1]) tensor([[[1.0000],\n",
      "         [0.1000],\n",
      "         [0.0100],\n",
      "         [0.0010]]])\n",
      "zzzzz tensor([[[1.0000],\n",
      "         [0.1000],\n",
      "         [0.0100],\n",
      "         [0.0010]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.1000],\n",
      "         [0.0100],\n",
      "         [0.0010]]])\n",
      "position_ids_expanded torch.Size([2, 1, 4]) tensor([[[0., 1., 2., 3.]],\n",
      "\n",
      "        [[0., 1., 2., 3.]]])\n",
      "xxxxyyyuuu torch.Size([2, 4, 1]) torch.Size([2, 1, 4])\n",
      "tttt torch.Size([2, 4, 4]) tensor([[[0.0000e+00, 1.0000e+00, 2.0000e+00, 3.0000e+00],\n",
      "         [0.0000e+00, 1.0000e-01, 2.0000e-01, 3.0000e-01],\n",
      "         [0.0000e+00, 1.0000e-02, 2.0000e-02, 3.0000e-02],\n",
      "         [0.0000e+00, 1.0000e-03, 2.0000e-03, 3.0000e-03]],\n",
      "\n",
      "        [[0.0000e+00, 1.0000e+00, 2.0000e+00, 3.0000e+00],\n",
      "         [0.0000e+00, 1.0000e-01, 2.0000e-01, 3.0000e-01],\n",
      "         [0.0000e+00, 1.0000e-02, 2.0000e-02, 3.0000e-02],\n",
      "         [0.0000e+00, 1.0000e-03, 2.0000e-03, 3.0000e-03]]])\n",
      "Cosine encoding:\n",
      "tensor([[[ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000],\n",
      "         [ 0.5403,  0.9950,  0.9999,  1.0000,  0.5403,  0.9950,  0.9999,\n",
      "           1.0000],\n",
      "         [-0.4161,  0.9801,  0.9998,  1.0000, -0.4161,  0.9801,  0.9998,\n",
      "           1.0000],\n",
      "         [-0.9900,  0.9553,  0.9996,  1.0000, -0.9900,  0.9553,  0.9996,\n",
      "           1.0000]],\n",
      "\n",
      "        [[ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000],\n",
      "         [ 0.5403,  0.9950,  0.9999,  1.0000,  0.5403,  0.9950,  0.9999,\n",
      "           1.0000],\n",
      "         [-0.4161,  0.9801,  0.9998,  1.0000, -0.4161,  0.9801,  0.9998,\n",
      "           1.0000],\n",
      "         [-0.9900,  0.9553,  0.9996,  1.0000, -0.9900,  0.9553,  0.9996,\n",
      "           1.0000]]])\n",
      "Sine encoding:\n",
      "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.8415, 0.0998, 0.0100, 0.0010, 0.8415, 0.0998, 0.0100, 0.0010],\n",
      "         [0.9093, 0.1987, 0.0200, 0.0020, 0.9093, 0.1987, 0.0200, 0.0020],\n",
      "         [0.1411, 0.2955, 0.0300, 0.0030, 0.1411, 0.2955, 0.0300, 0.0030]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.8415, 0.0998, 0.0100, 0.0010, 0.8415, 0.0998, 0.0100, 0.0010],\n",
      "         [0.9093, 0.1987, 0.0200, 0.0020, 0.9093, 0.1987, 0.0200, 0.0020],\n",
      "         [0.1411, 0.2955, 0.0300, 0.0030, 0.1411, 0.2955, 0.0300, 0.0030]]])\n"
     ]
    }
   ],
   "source": [
    "# 初始化 GemmaRotaryEmbedding\n",
    "dim = 8  # 嵌入的维度\n",
    "embedding_layer = GemmaRotaryEmbedding(dim)\n",
    "\n",
    "# 创建一个输入张量，形状为 (batch_size, seq_len, dim)\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "x = torch.randn(batch_size, seq_len, dim)\n",
    "\n",
    "# 创建位置ID，形状为 (batch_size, seq_len)\n",
    "position_ids = torch.arange(0, seq_len).unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "print('pos shape',position_ids.shape)\n",
    "print('pos',position_ids)\n",
    "\n",
    "# 计算旋转位置编码\n",
    "cos, sin = embedding_layer(x, position_ids)\n",
    "\n",
    "# 打印输出结果\n",
    "print(\"Cosine encoding:\")\n",
    "print(cos)\n",
    "print(\"Sine encoding:\")\n",
    "print(sin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a31cfc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inv_freq: torch.Size([4]) tensor([1.0000, 0.1000, 0.0100, 0.0010])\n",
      "inv_freq_expand: torch.Size([2, 4, 1]) tensor([[[1.0000],\n",
      "         [0.1000],\n",
      "         [0.0100],\n",
      "         [0.0010]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.1000],\n",
      "         [0.0100],\n",
      "         [0.0010]]])\n",
      "position_ids_expanded: torch.Size([2, 1, 4])\n",
      "freqs: torch.Size([2, 4, 4]) tensor([[[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [3.0000e+00, 3.0000e-01, 3.0000e-02, 3.0000e-03],\n",
      "         [3.0000e+00, 3.0000e-01, 3.0000e-02, 3.0000e-03],\n",
      "         [2.0000e+00, 2.0000e-01, 2.0000e-02, 2.0000e-03]],\n",
      "\n",
      "        [[3.0000e+00, 3.0000e-01, 3.0000e-02, 3.0000e-03],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [3.0000e+00, 3.0000e-01, 3.0000e-02, 3.0000e-03],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]]])\n",
      "cos: torch.Size([2, 4, 8])\n",
      "sin: torch.Size([2, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 参数初始化\n",
    "dim = 8\n",
    "base = 10000\n",
    "max_position_embeddings = 4\n",
    "\n",
    "# 创建inv_freq\n",
    "inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).float() / dim))\n",
    "print('inv_freq:', inv_freq.shape,inv_freq)\n",
    "\n",
    "# 模拟输入\n",
    "position_ids = torch.randint(0, max_position_embeddings, (2, 4))  # (batch_size, seq_len)\n",
    "x = torch.randn(2, 4, dim)  # 假设输入的x，大小为 (batch_size, seq_len, dim)\n",
    "\n",
    "# 扩展inv_freq\n",
    "inv_freq_expand = inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n",
    "print('inv_freq_expand:', inv_freq_expand.shape,inv_freq_expand)\n",
    "\n",
    "# 扩展position_ids\n",
    "position_ids_expanded = position_ids[:, None, :].float()\n",
    "print('position_ids_expanded:', position_ids_expanded.shape)\n",
    "\n",
    "# 获取设备类型（这里假设设备是CPU）\n",
    "device_type = x.device.type\n",
    "device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n",
    "\n",
    "# 计算频率\n",
    "freqs = (inv_freq_expand.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
    "print('freqs:', freqs.shape,freqs)\n",
    "\n",
    "# 计算embedding（cos和sin）\n",
    "emb = torch.cat((freqs, freqs), dim=-1)\n",
    "cos = emb.cos()\n",
    "sin = emb.sin()\n",
    "print('cos:', cos.shape)\n",
    "print('sin:', sin.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5010003d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592e4953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18624570",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db399b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbd705a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fa4de76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_half(x):\n",
    "    #### B,T,M,2\n",
    "    x1 = x[...,:x.shape[-1] // 2]\n",
    "    x2 = x[...,x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2,x1),dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55d29760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ -3.,  -4.,   1.,   2.],\n",
      "         [ -7.,  -8.,   5.,   6.],\n",
      "         [-11., -12.,   9.,  10.]],\n",
      "\n",
      "        [[-15., -16.,  13.,  14.],\n",
      "         [-19., -20.,  17.,  18.],\n",
      "         [-23., -24.,  21.,  22.]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[[1., 2., 3., 4.],\n",
    "                   [5., 6., 7., 8.],\n",
    "                   [9., 10., 11., 12.]],\n",
    "\n",
    "                  [[13., 14., 15., 16.],\n",
    "                   [17., 18., 19., 20.],\n",
    "                   [21., 22., 23., 24.]]])\n",
    "\n",
    "output = rotate_half(x)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91ecfd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_pos_emb(q,k,cos,sin,unsqueeze_dim=1):\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    \n",
    "    print('q',q.shape,'cos',cos.shape)\n",
    "    \n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    \n",
    "    return q_embed, k_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25500ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q torch.Size([2, 1, 5, 8]) cos torch.Size([2, 1, 5, 8])\n",
      "q_embed shape: torch.Size([2, 1, 5, 8])\n",
      "k_embed shape: torch.Size([2, 1, 5, 8])\n",
      "q_embed: tensor([[[[-7.6948e-01, -5.6814e-01, -1.7095e-01,  7.2094e-01, -1.5539e+00,\n",
      "           -3.3883e-01,  9.2523e-03,  1.2150e+00],\n",
      "          [-1.4535e+00,  7.4666e-01, -2.8978e-01,  1.0115e+00,  8.8850e-02,\n",
      "           -6.9532e-01,  8.8218e-02, -8.7510e-01],\n",
      "          [-2.1211e+00, -1.1748e+00,  2.1315e+00, -9.0456e-01,  1.7848e+00,\n",
      "           -1.6587e+00, -3.1561e+00,  5.0773e-01],\n",
      "          [ 5.0666e-01,  1.5722e+00, -1.4029e+00, -2.9377e-01,  2.1282e+00,\n",
      "           -2.2508e+00,  7.1495e-01,  2.6070e-01],\n",
      "          [-3.5374e-02, -4.6350e-01,  1.2723e+00,  2.4211e-01,  3.5788e-03,\n",
      "            8.9395e-02, -1.0620e+00,  3.7735e+00]]],\n",
      "\n",
      "\n",
      "        [[[-1.2408e+00, -6.1835e-01, -1.1471e+00, -1.0227e+00, -9.1090e-01,\n",
      "           -1.6743e+00,  3.6710e-01,  8.3530e-03],\n",
      "          [ 5.8127e-01,  3.2002e+00, -4.4563e-01,  1.5812e-01, -3.2814e-01,\n",
      "           -8.5364e-02,  9.8953e-01,  6.4932e-01],\n",
      "          [ 4.0861e-01, -1.3278e+00,  6.6144e-02, -7.7243e-01, -2.7251e-01,\n",
      "            2.8736e-01,  1.0075e+00, -3.9374e-01],\n",
      "          [ 2.1842e+00,  1.0641e+00,  7.6195e-01, -1.4947e+00,  2.4034e+00,\n",
      "            1.6202e+00, -1.4854e+00,  1.4114e+00],\n",
      "          [-9.1011e-01, -8.8139e-01, -1.1705e+00, -4.2159e-01, -8.9662e-01,\n",
      "           -1.7307e-01, -7.3452e-01,  1.3843e+00]]]])\n",
      "k_embed: tensor([[[[-0.5390, -1.1697, -0.7226, -0.5153, -1.0286,  1.5845, -0.0260,\n",
      "           -0.3014],\n",
      "          [ 0.7480, -0.2919,  1.8324,  0.3894, -0.1794,  0.2168, -0.8449,\n",
      "           -0.0765],\n",
      "          [ 1.7400,  1.1883,  0.1611, -0.2963, -1.4945,  1.3941, -0.2931,\n",
      "            0.1537],\n",
      "          [ 0.0267, -0.2383,  4.8144,  1.2961,  0.2167,  0.6696, -2.4041,\n",
      "           -1.0089],\n",
      "          [ 0.0978,  0.8418, -0.4292, -1.2239,  0.4847, -0.1534,  0.6521,\n",
      "           -1.3731]]],\n",
      "\n",
      "\n",
      "        [[[ 1.7292,  0.3658,  0.0855, -0.7442, -0.4827,  1.3374, -0.1301,\n",
      "           -0.3810],\n",
      "          [-1.7297,  0.1675,  0.6617, -0.1676, -0.3643, -0.2546, -1.5367,\n",
      "           -1.1164],\n",
      "          [ 0.0616,  0.7448,  1.3073, -0.6139,  0.3058, -0.2613, -0.8716,\n",
      "           -1.7685],\n",
      "          [-0.8559,  0.2076, -1.8712,  0.3752, -1.8084,  0.4239, -0.0196,\n",
      "           -0.3457],\n",
      "          [ 0.7512, -0.0680,  0.1379,  2.5362, -0.0835,  0.8605,  0.1164,\n",
      "           -0.1830]]]])\n"
     ]
    }
   ],
   "source": [
    "# 模拟数据\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "dim = 8\n",
    "\n",
    "# 随机生成查询和键张量\n",
    "q = torch.randn(batch_size, 1,seq_len, dim)\n",
    "k = torch.randn(batch_size, 1,seq_len, dim)\n",
    "\n",
    "# 旋转位置编码的余弦和正弦值\n",
    "# 这里我们假设 dim 为 8，所以下面 dim // 2 = 4\n",
    "cos = torch.randn(batch_size,seq_len, dim)\n",
    "sin = torch.randn(batch_size,seq_len, dim)\n",
    "\n",
    "# 使用 apply_rotary_pos_emb 函数\n",
    "q_embed, k_embed = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "\n",
    "# 打印结果\n",
    "print(\"q_embed shape:\", q_embed.shape)\n",
    "print(\"k_embed shape:\", k_embed.shape)\n",
    "\n",
    "# 查看 q_embed 和 k_embed 的具体值\n",
    "print(\"q_embed:\", q_embed)\n",
    "print(\"k_embed:\", k_embed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74457c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaMLP(nn.Module):\n",
    "    \n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        self.gate_proj = nn.Linear(self.hidden_size,self.intermediate_size,bias=False)\n",
    "        self.up_proj = nn.Linear(self.hidden_size,self.intermediate_size,bias=False)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size,self.hidden_size,bias=False)\n",
    "    \n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.down_proj(nn.functional.gelu(self.gate_proj(x),approximate='tanh')*self.up_proj(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b6139ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 模拟一个配置对象\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.hidden_size = 512\n",
    "        self.intermediate_size = 1024\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# 创建 GemmaMLP 模型\n",
    "model = GemmaMLP(config)\n",
    "\n",
    "# 随机输入数据\n",
    "x = torch.randn(2, 10, config.hidden_size)  # batch_size=2, seq_len=10, hidden_size=512\n",
    "\n",
    "# 前向传播\n",
    "output = model(x)\n",
    "print(output.shape)  # 应该是 (2, 10, 512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "749584f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a637f00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result shape: torch.Size([2, 6, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个形状为 (2, 3, 4, 5) 的随机张量，表示一个 batch size 为 2，3 个键值头，序列长度为 4，head_dim 为 5 的输入\n",
    "hidden_states = torch.randn(2, 3, 4, 5)\n",
    "\n",
    "# 设置重复次数为 2\n",
    "n_rep = 2\n",
    "\n",
    "# 调用 repeat_kv 函数\n",
    "result = repeat_kv(hidden_states, n_rep)\n",
    "\n",
    "print(\"Result shape:\", result.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92fc15a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self,config: GemmaConfig, layer_idx: Optional[int] = None):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "        \n",
    "        self.attention_dropout = config.attention_dropout\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_heads\n",
    "        self.head_dim = config.head_dim\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
    "        self.max_position_embeddings = config.max_position_embeddings\n",
    "        self.rope_theta = config.rope_theta\n",
    "        self.is_casual = True\n",
    "        \n",
    "        assert self.hidden_size % self.num_heads == 0\n",
    "        print('xxx',self.num_heads * self.head_dim)\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim,bias=config.attention_bias)\n",
    "        self.k_proj = nn.Linear(self.hidden_size,self.num_key_value_heads*self.head_dim,bias=config.attention_bias)\n",
    "        self.v_proj = nn.Linear(self.hidden_size,self.num_key_value_heads*self.head_dim,bias=config.attention_bias)\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim,self.hidden_size,bias=config.attention_bias)\n",
    "        \n",
    "        self.rotary_emb = GemmaRotaryEmbedding(\n",
    "            self.head_dim,\n",
    "            max_position_embeddings=self.max_position_embeddings,\n",
    "            base=self.rope_theta,\n",
    "        )\n",
    "        \n",
    "    \n",
    "    \n",
    "    def forward(self,hidden_states: torch.Tensor,\n",
    "                     attention_mask: Optional[torch.Tensor] = None,\n",
    "                     position_ids: Optional[torch.LongTensor] = None,\n",
    "                     kv_cache: Optional[KVCache] = None,\n",
    "                     **kwargs) -> Tuple[torch.Tensor,Optional[torch.Tensor],\n",
    "                                       Optional[Tuple[torch.Tensor]]]:\n",
    "        \n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "        \n",
    "        # [Batch_Size, Num_Heads_Q, Seq_Len, Head_Dim]\n",
    "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        key_states = key_states.view(bsz,q_len,self.num_key_value_heads, self.head_dim).transpose(1,2)\n",
    "        value_states = value_states.view(bsz,q_len,self.num_key_value_heads, self.head_dim).transpose(1,2)\n",
    "        \n",
    "        cos,sin = self.rotary_emb(value_states,position_ids,seq_len=None)\n",
    "        \n",
    "        query_states,key_states = apply_rotary_pos_emb(query_states,key_states,cos,sin)\n",
    "        \n",
    "        if kv_cache is not None:\n",
    "            \n",
    "            key_states, value_states = kv_cache.update(key_states,value_states,self.layer_idx)\n",
    "        \n",
    "        \n",
    "        key_states = repeat_kv(key_states,self.num_key_value_groups)\n",
    "        value_states = repeat_kv(value_states,self.num_key_value_groups)\n",
    "        \n",
    "        attn_weights = torch.matmul(query_states,key_states.transpose(2,3)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        assert attention_mask is not None\n",
    "        attn_weights = attn_weights + attention_mask\n",
    "        \n",
    "        attn_weights = nn.functional.softmax(attn_weights,dim=-1,dtype=torch.float32).to(query_states.dtype)\n",
    "        \n",
    "        attn_weights = nn.functional.dropout(attn_weights,p=self.attention_dropout,training=self.training)\n",
    "        \n",
    "        attn_output = torch.matmul(attn_weights, value_states)\n",
    "        \n",
    "        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n",
    "                f\" {attn_output.size()}\"\n",
    "            )\n",
    "        \n",
    "        \n",
    "        attn_output = attn_output.transpose(1,2).contiguous()\n",
    "        \n",
    "        attn_output = attn_output.view(bsz,q_len,-1)\n",
    "        \n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        \n",
    "        return attn_output, attn_weights\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "28437ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxx tensor([1.0000e+00, 3.1623e-01, 1.0000e-01, 3.1623e-02, 1.0000e-02, 3.1623e-03,\n",
      "        1.0000e-03, 3.1623e-04])\n",
      "yyyy torch.Size([1, 8, 1]) tensor([[[1.0000e+00],\n",
      "         [3.1623e-01],\n",
      "         [1.0000e-01],\n",
      "         [3.1623e-02],\n",
      "         [1.0000e-02],\n",
      "         [3.1623e-03],\n",
      "         [1.0000e-03],\n",
      "         [3.1623e-04]]])\n",
      "zzzzz tensor([[[1.0000e+00],\n",
      "         [3.1623e-01],\n",
      "         [1.0000e-01],\n",
      "         [3.1623e-02],\n",
      "         [1.0000e-02],\n",
      "         [3.1623e-03],\n",
      "         [1.0000e-03],\n",
      "         [3.1623e-04]],\n",
      "\n",
      "        [[1.0000e+00],\n",
      "         [3.1623e-01],\n",
      "         [1.0000e-01],\n",
      "         [3.1623e-02],\n",
      "         [1.0000e-02],\n",
      "         [3.1623e-03],\n",
      "         [1.0000e-03],\n",
      "         [3.1623e-04]]])\n",
      "position_ids_expanded torch.Size([2, 1, 10]) tensor([[[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]],\n",
      "\n",
      "        [[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]]])\n",
      "xxxxyyyuuu torch.Size([2, 8, 1]) torch.Size([2, 1, 10])\n",
      "tttt torch.Size([2, 8, 10]) tensor([[[0.0000e+00, 1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00,\n",
      "          5.0000e+00, 6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00],\n",
      "         [0.0000e+00, 3.1623e-01, 6.3246e-01, 9.4868e-01, 1.2649e+00,\n",
      "          1.5811e+00, 1.8974e+00, 2.2136e+00, 2.5298e+00, 2.8460e+00],\n",
      "         [0.0000e+00, 1.0000e-01, 2.0000e-01, 3.0000e-01, 4.0000e-01,\n",
      "          5.0000e-01, 6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01],\n",
      "         [0.0000e+00, 3.1623e-02, 6.3246e-02, 9.4868e-02, 1.2649e-01,\n",
      "          1.5811e-01, 1.8974e-01, 2.2136e-01, 2.5298e-01, 2.8460e-01],\n",
      "         [0.0000e+00, 1.0000e-02, 2.0000e-02, 3.0000e-02, 4.0000e-02,\n",
      "          5.0000e-02, 6.0000e-02, 7.0000e-02, 8.0000e-02, 9.0000e-02],\n",
      "         [0.0000e+00, 3.1623e-03, 6.3246e-03, 9.4868e-03, 1.2649e-02,\n",
      "          1.5811e-02, 1.8974e-02, 2.2136e-02, 2.5298e-02, 2.8461e-02],\n",
      "         [0.0000e+00, 1.0000e-03, 2.0000e-03, 3.0000e-03, 4.0000e-03,\n",
      "          5.0000e-03, 6.0000e-03, 7.0000e-03, 8.0000e-03, 9.0000e-03],\n",
      "         [0.0000e+00, 3.1623e-04, 6.3246e-04, 9.4868e-04, 1.2649e-03,\n",
      "          1.5811e-03, 1.8974e-03, 2.2136e-03, 2.5298e-03, 2.8461e-03]],\n",
      "\n",
      "        [[0.0000e+00, 1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00,\n",
      "          5.0000e+00, 6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00],\n",
      "         [0.0000e+00, 3.1623e-01, 6.3246e-01, 9.4868e-01, 1.2649e+00,\n",
      "          1.5811e+00, 1.8974e+00, 2.2136e+00, 2.5298e+00, 2.8460e+00],\n",
      "         [0.0000e+00, 1.0000e-01, 2.0000e-01, 3.0000e-01, 4.0000e-01,\n",
      "          5.0000e-01, 6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01],\n",
      "         [0.0000e+00, 3.1623e-02, 6.3246e-02, 9.4868e-02, 1.2649e-01,\n",
      "          1.5811e-01, 1.8974e-01, 2.2136e-01, 2.5298e-01, 2.8460e-01],\n",
      "         [0.0000e+00, 1.0000e-02, 2.0000e-02, 3.0000e-02, 4.0000e-02,\n",
      "          5.0000e-02, 6.0000e-02, 7.0000e-02, 8.0000e-02, 9.0000e-02],\n",
      "         [0.0000e+00, 3.1623e-03, 6.3246e-03, 9.4868e-03, 1.2649e-02,\n",
      "          1.5811e-02, 1.8974e-02, 2.2136e-02, 2.5298e-02, 2.8461e-02],\n",
      "         [0.0000e+00, 1.0000e-03, 2.0000e-03, 3.0000e-03, 4.0000e-03,\n",
      "          5.0000e-03, 6.0000e-03, 7.0000e-03, 8.0000e-03, 9.0000e-03],\n",
      "         [0.0000e+00, 3.1623e-04, 6.3246e-04, 9.4868e-04, 1.2649e-03,\n",
      "          1.5811e-03, 1.8974e-03, 2.2136e-03, 2.5298e-03, 2.8461e-03]]])\n",
      "q torch.Size([2, 8, 10, 16]) cos torch.Size([2, 1, 10, 16])\n",
      "Attention Output Shape: torch.Size([2, 10, 128])\n",
      "Attention Weights Shape: torch.Size([2, 8, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example configuration (assuming GemmaConfig is properly defined)\n",
    "\n",
    "class Config:\n",
    "    \n",
    "    hidden_size=128\n",
    "    num_heads=8\n",
    "    head_dim=16\n",
    "    num_key_value_heads=4\n",
    "    attention_dropout=0.1\n",
    "    max_position_embeddings=512\n",
    "    rope_theta=10000.0\n",
    "    attention_bias=False\n",
    "\n",
    "\n",
    "config = Config()\n",
    "# 初始化 GemmaAttention 层\n",
    "# 初始化 GemmaAttention 层\n",
    "attention_layer = GemmaAttention(config)\n",
    "\n",
    "# 随机生成 hidden_states，形状 [batch_size=2, seq_len=10, hidden_size=128]\n",
    "hidden_states = torch.randn(2, 10, 128)\n",
    "\n",
    "# 生成 position_ids (假设序列长度为 10，batch_size 为 2)\n",
    "position_ids = torch.arange(10).unsqueeze(0).repeat(2, 1)  # 2 个批次，序列长度为 10\n",
    "\n",
    "# 生成 attention_mask，假设0表示填充，1表示有效token\n",
    "input_sequence = torch.tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
    "                              [1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])\n",
    "attention_mask = (input_sequence != 0).unsqueeze(1).unsqueeze(2).to(torch.float)\n",
    "\n",
    "# 调用 forward 方法，传递 hidden_states、attention_mask 和 position_ids\n",
    "attn_output, attn_weights = attention_layer(hidden_states, attention_mask=attention_mask, position_ids=position_ids)\n",
    "\n",
    "# 打印输出形状\n",
    "print(\"Attention Output Shape:\", attn_output.shape)\n",
    "print(\"Attention Weights Shape:\", attn_weights.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9bd72fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaConfig:\n",
    "    def __init__(self):\n",
    "        self.hidden_size = 512  # 隐藏层大小\n",
    "        self.num_heads = 8   # 注意力头数\n",
    "        self.head_dim=32\n",
    "        self.num_key_value_heads = 4  # Key-Value头数\n",
    "        self.attention_dropout = 0.1  # Dropout率\n",
    "        self.max_position_embeddings = 1024  # 最大位置嵌入数\n",
    "        self.rope_theta = 10000  # 旋转位置编码的基数\n",
    "        self.attention_bias = False  # 是否使用偏置\n",
    "\n",
    "# 实例化配置\n",
    "config = GemmaConfig()\n",
    "# 实例化GemmaAttention\n",
    "gemma_attention = GemmaAttention(config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "69adec30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaAttention(\n",
       "  (q_proj): Linear(in_features=512, out_features=256, bias=False)\n",
       "  (k_proj): Linear(in_features=512, out_features=128, bias=False)\n",
       "  (v_proj): Linear(in_features=512, out_features=128, bias=False)\n",
       "  (o_proj): Linear(in_features=256, out_features=512, bias=False)\n",
       "  (rotary_emb): GemmaRotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d23788b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaDecoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self,config: GemmaConfig, layer_idx:int):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.self_attn = GemmaAttention(config=config,\n",
    "                                       layer_idx=layer_idx)\n",
    "        self.mlp = GemmaMLP(config)\n",
    "        \n",
    "        self.input_layernorm = GemmaRMSNorm(config.hidden_size,eps=config.rms_norm_eps)\n",
    "        \n",
    "        self.post_attention_layernorm = GemmaRMSNorm(config.hidden_size,eps=config.rms_norm_eps)\n",
    "    \n",
    "    \n",
    "    def forward(self,hidden_states: torch.Tensor,\n",
    "                     attention_mask:Optional[torch.Tensor]=None,\n",
    "                     position_ids: Optional[torch.LongTensor] = None,\n",
    "                     kv_cache: Optional[KVCache] = None\n",
    "               ) -> Tuple[torch.FloatTensor,Optional[Tuple[torch.FloatTensor,torch.FloatTensor]]]:\n",
    "        \n",
    "        residual = hidden_states\n",
    "        \n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "        \n",
    "        hidden_states, _ = self.self_attn(\n",
    "           hidden_states=hidden_states,\n",
    "           attention_mask=attention_mask,\n",
    "           position_ids=position_ids,\n",
    "           kv_cache=kv_cache\n",
    "        )\n",
    "        \n",
    "        hidden_states = residual + hidden_states\n",
    "        \n",
    "        residual = hidden_states\n",
    "        \n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        \n",
    "        hidden_states = residual + hidden_states\n",
    "        \n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3fb4a7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxx tensor([1.0000e+00, 5.6234e-01, 3.1623e-01, 1.7783e-01, 1.0000e-01, 5.6234e-02,\n",
      "        3.1623e-02, 1.7783e-02, 1.0000e-02, 5.6234e-03, 3.1623e-03, 1.7783e-03,\n",
      "        1.0000e-03, 5.6234e-04, 3.1623e-04, 1.7783e-04])\n",
      "yyyy torch.Size([1, 16, 1]) tensor([[[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]]])\n",
      "zzzzz tensor([[[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]],\n",
      "\n",
      "        [[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]]])\n",
      "position_ids_expanded torch.Size([2, 1, 10]) tensor([[[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]],\n",
      "\n",
      "        [[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]]])\n",
      "xxxxyyyuuu torch.Size([2, 16, 1]) torch.Size([2, 1, 10])\n",
      "tttt torch.Size([2, 16, 10]) tensor([[[0.0000e+00, 1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00,\n",
      "          5.0000e+00, 6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00],\n",
      "         [0.0000e+00, 5.6234e-01, 1.1247e+00, 1.6870e+00, 2.2494e+00,\n",
      "          2.8117e+00, 3.3740e+00, 3.9364e+00, 4.4987e+00, 5.0611e+00],\n",
      "         [0.0000e+00, 3.1623e-01, 6.3246e-01, 9.4868e-01, 1.2649e+00,\n",
      "          1.5811e+00, 1.8974e+00, 2.2136e+00, 2.5298e+00, 2.8460e+00],\n",
      "         [0.0000e+00, 1.7783e-01, 3.5566e-01, 5.3348e-01, 7.1131e-01,\n",
      "          8.8914e-01, 1.0670e+00, 1.2448e+00, 1.4226e+00, 1.6005e+00],\n",
      "         [0.0000e+00, 1.0000e-01, 2.0000e-01, 3.0000e-01, 4.0000e-01,\n",
      "          5.0000e-01, 6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01],\n",
      "         [0.0000e+00, 5.6234e-02, 1.1247e-01, 1.6870e-01, 2.2494e-01,\n",
      "          2.8117e-01, 3.3740e-01, 3.9364e-01, 4.4987e-01, 5.0611e-01],\n",
      "         [0.0000e+00, 3.1623e-02, 6.3246e-02, 9.4868e-02, 1.2649e-01,\n",
      "          1.5811e-01, 1.8974e-01, 2.2136e-01, 2.5298e-01, 2.8460e-01],\n",
      "         [0.0000e+00, 1.7783e-02, 3.5566e-02, 5.3348e-02, 7.1131e-02,\n",
      "          8.8914e-02, 1.0670e-01, 1.2448e-01, 1.4226e-01, 1.6005e-01],\n",
      "         [0.0000e+00, 1.0000e-02, 2.0000e-02, 3.0000e-02, 4.0000e-02,\n",
      "          5.0000e-02, 6.0000e-02, 7.0000e-02, 8.0000e-02, 9.0000e-02],\n",
      "         [0.0000e+00, 5.6234e-03, 1.1247e-02, 1.6870e-02, 2.2494e-02,\n",
      "          2.8117e-02, 3.3740e-02, 3.9364e-02, 4.4987e-02, 5.0611e-02],\n",
      "         [0.0000e+00, 3.1623e-03, 6.3246e-03, 9.4868e-03, 1.2649e-02,\n",
      "          1.5811e-02, 1.8974e-02, 2.2136e-02, 2.5298e-02, 2.8461e-02],\n",
      "         [0.0000e+00, 1.7783e-03, 3.5566e-03, 5.3348e-03, 7.1131e-03,\n",
      "          8.8914e-03, 1.0670e-02, 1.2448e-02, 1.4226e-02, 1.6005e-02],\n",
      "         [0.0000e+00, 1.0000e-03, 2.0000e-03, 3.0000e-03, 4.0000e-03,\n",
      "          5.0000e-03, 6.0000e-03, 7.0000e-03, 8.0000e-03, 9.0000e-03],\n",
      "         [0.0000e+00, 5.6234e-04, 1.1247e-03, 1.6870e-03, 2.2494e-03,\n",
      "          2.8117e-03, 3.3740e-03, 3.9364e-03, 4.4987e-03, 5.0611e-03],\n",
      "         [0.0000e+00, 3.1623e-04, 6.3246e-04, 9.4868e-04, 1.2649e-03,\n",
      "          1.5811e-03, 1.8974e-03, 2.2136e-03, 2.5298e-03, 2.8461e-03],\n",
      "         [0.0000e+00, 1.7783e-04, 3.5566e-04, 5.3348e-04, 7.1131e-04,\n",
      "          8.8914e-04, 1.0670e-03, 1.2448e-03, 1.4226e-03, 1.6005e-03]],\n",
      "\n",
      "        [[0.0000e+00, 1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00,\n",
      "          5.0000e+00, 6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00],\n",
      "         [0.0000e+00, 5.6234e-01, 1.1247e+00, 1.6870e+00, 2.2494e+00,\n",
      "          2.8117e+00, 3.3740e+00, 3.9364e+00, 4.4987e+00, 5.0611e+00],\n",
      "         [0.0000e+00, 3.1623e-01, 6.3246e-01, 9.4868e-01, 1.2649e+00,\n",
      "          1.5811e+00, 1.8974e+00, 2.2136e+00, 2.5298e+00, 2.8460e+00],\n",
      "         [0.0000e+00, 1.7783e-01, 3.5566e-01, 5.3348e-01, 7.1131e-01,\n",
      "          8.8914e-01, 1.0670e+00, 1.2448e+00, 1.4226e+00, 1.6005e+00],\n",
      "         [0.0000e+00, 1.0000e-01, 2.0000e-01, 3.0000e-01, 4.0000e-01,\n",
      "          5.0000e-01, 6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01],\n",
      "         [0.0000e+00, 5.6234e-02, 1.1247e-01, 1.6870e-01, 2.2494e-01,\n",
      "          2.8117e-01, 3.3740e-01, 3.9364e-01, 4.4987e-01, 5.0611e-01],\n",
      "         [0.0000e+00, 3.1623e-02, 6.3246e-02, 9.4868e-02, 1.2649e-01,\n",
      "          1.5811e-01, 1.8974e-01, 2.2136e-01, 2.5298e-01, 2.8460e-01],\n",
      "         [0.0000e+00, 1.7783e-02, 3.5566e-02, 5.3348e-02, 7.1131e-02,\n",
      "          8.8914e-02, 1.0670e-01, 1.2448e-01, 1.4226e-01, 1.6005e-01],\n",
      "         [0.0000e+00, 1.0000e-02, 2.0000e-02, 3.0000e-02, 4.0000e-02,\n",
      "          5.0000e-02, 6.0000e-02, 7.0000e-02, 8.0000e-02, 9.0000e-02],\n",
      "         [0.0000e+00, 5.6234e-03, 1.1247e-02, 1.6870e-02, 2.2494e-02,\n",
      "          2.8117e-02, 3.3740e-02, 3.9364e-02, 4.4987e-02, 5.0611e-02],\n",
      "         [0.0000e+00, 3.1623e-03, 6.3246e-03, 9.4868e-03, 1.2649e-02,\n",
      "          1.5811e-02, 1.8974e-02, 2.2136e-02, 2.5298e-02, 2.8461e-02],\n",
      "         [0.0000e+00, 1.7783e-03, 3.5566e-03, 5.3348e-03, 7.1131e-03,\n",
      "          8.8914e-03, 1.0670e-02, 1.2448e-02, 1.4226e-02, 1.6005e-02],\n",
      "         [0.0000e+00, 1.0000e-03, 2.0000e-03, 3.0000e-03, 4.0000e-03,\n",
      "          5.0000e-03, 6.0000e-03, 7.0000e-03, 8.0000e-03, 9.0000e-03],\n",
      "         [0.0000e+00, 5.6234e-04, 1.1247e-03, 1.6870e-03, 2.2494e-03,\n",
      "          2.8117e-03, 3.3740e-03, 3.9364e-03, 4.4987e-03, 5.0611e-03],\n",
      "         [0.0000e+00, 3.1623e-04, 6.3246e-04, 9.4868e-04, 1.2649e-03,\n",
      "          1.5811e-03, 1.8974e-03, 2.2136e-03, 2.5298e-03, 2.8461e-03],\n",
      "         [0.0000e+00, 1.7783e-04, 3.5566e-04, 5.3348e-04, 7.1131e-04,\n",
      "          8.8914e-04, 1.0670e-03, 1.2448e-03, 1.4226e-03, 1.6005e-03]]])\n",
      "q torch.Size([2, 8, 10, 32]) cos torch.Size([2, 1, 10, 32])\n",
      "Decoder Layer Output Shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "# 假设定义了 GemmaMLP, GemmaRMSNorm 等\n",
    "# 创建一个 GemmaDecoderLayer\n",
    "\n",
    "class GemmaConfig:\n",
    "    def __init__(self):\n",
    "        self.hidden_size = 512  # 隐藏层大小\n",
    "        self.intermediate_size = 512\n",
    "        self.num_heads = 8   # 注意力头数\n",
    "        self.head_dim=32\n",
    "        self.num_key_value_heads = 4  # Key-Value头数\n",
    "        self.attention_dropout = 0.1  # Dropout率\n",
    "        self.max_position_embeddings = 1024  # 最大位置嵌入数\n",
    "        self.rope_theta = 10000  # 旋转位置编码的基数\n",
    "        self.attention_bias = False  # 是否使用偏置\n",
    "        self.rms_norm_eps = 0.01\n",
    "\n",
    "# 实例化配置\n",
    "config = GemmaConfig()\n",
    "\n",
    "layer_idx = 0\n",
    "decoder_layer = GemmaDecoderLayer(config=config, layer_idx=layer_idx)\n",
    "\n",
    "# 创建一个简单的输入张量\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "hidden_states = torch.randn(batch_size, seq_len, config.hidden_size)\n",
    "\n",
    "# 创建 position_ids 和 attention_mask\n",
    "position_ids = torch.arange(seq_len).unsqueeze(0).repeat(batch_size, 1)\n",
    "attention_mask = torch.ones(batch_size, 1, seq_len, seq_len)\n",
    "\n",
    "# 前向传播\n",
    "output = decoder_layer(\n",
    "    hidden_states=hidden_states,\n",
    "    attention_mask=attention_mask,\n",
    "    position_ids=position_ids\n",
    ")\n",
    "\n",
    "# 打印输出形状\n",
    "print(\"Decoder Layer Output Shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfdd7fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7247206b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "58ed1c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaModel(nn.Module):\n",
    "    \n",
    "    def __init__(self,config: GemmaConfig):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size,config.hidden_size, self.padding_idx)\n",
    "        self.layers = nn.ModuleList(\n",
    "           [GemmaDecoderLayer(config,layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
    "        )\n",
    "        self.norm = GemmaRMSNorm(config.hidden_size,eps=config.rms_norm_eps)\n",
    "        \n",
    "    \n",
    "    def get_input_embeddings(self):\n",
    "        return self.embed_tokens\n",
    "    \n",
    "    def forward(self,attention_mask: Optional[torch.Tensor]=None,\n",
    "                     position_ids: Optional[torch.LongTensor] = None,\n",
    "                     inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "                     kv_cache: Optional[KVCache] = None):\n",
    "        \n",
    "        hidden_states = inputs_embeds\n",
    "        normalizer = torch.tensor(self.config.hidden_size**0.5,dtype=hidden_states.dtype)\n",
    "        \n",
    "        hidden_states = hidden_states * normalizer\n",
    "        \n",
    "        for decoder_layer in self.layers:\n",
    "            \n",
    "            hidden_states = decoder_layer(\n",
    "                hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                position_ids=position_ids,\n",
    "                kv_cache=kv_cache\n",
    "            )\n",
    "        \n",
    "        hidden_states = self.norm(hidden_states)\n",
    "        \n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "17d82abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxx tensor([1.0000e+00, 5.6234e-01, 3.1623e-01, 1.7783e-01, 1.0000e-01, 5.6234e-02,\n",
      "        3.1623e-02, 1.7783e-02, 1.0000e-02, 5.6234e-03, 3.1623e-03, 1.7783e-03,\n",
      "        1.0000e-03, 5.6234e-04, 3.1623e-04, 1.7783e-04])\n",
      "yyyy torch.Size([1, 16, 1]) tensor([[[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]]])\n",
      "zzzzz tensor([[[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]],\n",
      "\n",
      "        [[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]]])\n",
      "position_ids_expanded torch.Size([2, 1, 10]) tensor([[[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]],\n",
      "\n",
      "        [[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]]])\n",
      "xxxxyyyuuu torch.Size([2, 16, 1]) torch.Size([2, 1, 10])\n",
      "tttt torch.Size([2, 16, 10]) tensor([[[0.0000e+00, 1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00,\n",
      "          5.0000e+00, 6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00],\n",
      "         [0.0000e+00, 5.6234e-01, 1.1247e+00, 1.6870e+00, 2.2494e+00,\n",
      "          2.8117e+00, 3.3740e+00, 3.9364e+00, 4.4987e+00, 5.0611e+00],\n",
      "         [0.0000e+00, 3.1623e-01, 6.3246e-01, 9.4868e-01, 1.2649e+00,\n",
      "          1.5811e+00, 1.8974e+00, 2.2136e+00, 2.5298e+00, 2.8460e+00],\n",
      "         [0.0000e+00, 1.7783e-01, 3.5566e-01, 5.3348e-01, 7.1131e-01,\n",
      "          8.8914e-01, 1.0670e+00, 1.2448e+00, 1.4226e+00, 1.6005e+00],\n",
      "         [0.0000e+00, 1.0000e-01, 2.0000e-01, 3.0000e-01, 4.0000e-01,\n",
      "          5.0000e-01, 6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01],\n",
      "         [0.0000e+00, 5.6234e-02, 1.1247e-01, 1.6870e-01, 2.2494e-01,\n",
      "          2.8117e-01, 3.3740e-01, 3.9364e-01, 4.4987e-01, 5.0611e-01],\n",
      "         [0.0000e+00, 3.1623e-02, 6.3246e-02, 9.4868e-02, 1.2649e-01,\n",
      "          1.5811e-01, 1.8974e-01, 2.2136e-01, 2.5298e-01, 2.8460e-01],\n",
      "         [0.0000e+00, 1.7783e-02, 3.5566e-02, 5.3348e-02, 7.1131e-02,\n",
      "          8.8914e-02, 1.0670e-01, 1.2448e-01, 1.4226e-01, 1.6005e-01],\n",
      "         [0.0000e+00, 1.0000e-02, 2.0000e-02, 3.0000e-02, 4.0000e-02,\n",
      "          5.0000e-02, 6.0000e-02, 7.0000e-02, 8.0000e-02, 9.0000e-02],\n",
      "         [0.0000e+00, 5.6234e-03, 1.1247e-02, 1.6870e-02, 2.2494e-02,\n",
      "          2.8117e-02, 3.3740e-02, 3.9364e-02, 4.4987e-02, 5.0611e-02],\n",
      "         [0.0000e+00, 3.1623e-03, 6.3246e-03, 9.4868e-03, 1.2649e-02,\n",
      "          1.5811e-02, 1.8974e-02, 2.2136e-02, 2.5298e-02, 2.8461e-02],\n",
      "         [0.0000e+00, 1.7783e-03, 3.5566e-03, 5.3348e-03, 7.1131e-03,\n",
      "          8.8914e-03, 1.0670e-02, 1.2448e-02, 1.4226e-02, 1.6005e-02],\n",
      "         [0.0000e+00, 1.0000e-03, 2.0000e-03, 3.0000e-03, 4.0000e-03,\n",
      "          5.0000e-03, 6.0000e-03, 7.0000e-03, 8.0000e-03, 9.0000e-03],\n",
      "         [0.0000e+00, 5.6234e-04, 1.1247e-03, 1.6870e-03, 2.2494e-03,\n",
      "          2.8117e-03, 3.3740e-03, 3.9364e-03, 4.4987e-03, 5.0611e-03],\n",
      "         [0.0000e+00, 3.1623e-04, 6.3246e-04, 9.4868e-04, 1.2649e-03,\n",
      "          1.5811e-03, 1.8974e-03, 2.2136e-03, 2.5298e-03, 2.8461e-03],\n",
      "         [0.0000e+00, 1.7783e-04, 3.5566e-04, 5.3348e-04, 7.1131e-04,\n",
      "          8.8914e-04, 1.0670e-03, 1.2448e-03, 1.4226e-03, 1.6005e-03]],\n",
      "\n",
      "        [[0.0000e+00, 1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00,\n",
      "          5.0000e+00, 6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00],\n",
      "         [0.0000e+00, 5.6234e-01, 1.1247e+00, 1.6870e+00, 2.2494e+00,\n",
      "          2.8117e+00, 3.3740e+00, 3.9364e+00, 4.4987e+00, 5.0611e+00],\n",
      "         [0.0000e+00, 3.1623e-01, 6.3246e-01, 9.4868e-01, 1.2649e+00,\n",
      "          1.5811e+00, 1.8974e+00, 2.2136e+00, 2.5298e+00, 2.8460e+00],\n",
      "         [0.0000e+00, 1.7783e-01, 3.5566e-01, 5.3348e-01, 7.1131e-01,\n",
      "          8.8914e-01, 1.0670e+00, 1.2448e+00, 1.4226e+00, 1.6005e+00],\n",
      "         [0.0000e+00, 1.0000e-01, 2.0000e-01, 3.0000e-01, 4.0000e-01,\n",
      "          5.0000e-01, 6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01],\n",
      "         [0.0000e+00, 5.6234e-02, 1.1247e-01, 1.6870e-01, 2.2494e-01,\n",
      "          2.8117e-01, 3.3740e-01, 3.9364e-01, 4.4987e-01, 5.0611e-01],\n",
      "         [0.0000e+00, 3.1623e-02, 6.3246e-02, 9.4868e-02, 1.2649e-01,\n",
      "          1.5811e-01, 1.8974e-01, 2.2136e-01, 2.5298e-01, 2.8460e-01],\n",
      "         [0.0000e+00, 1.7783e-02, 3.5566e-02, 5.3348e-02, 7.1131e-02,\n",
      "          8.8914e-02, 1.0670e-01, 1.2448e-01, 1.4226e-01, 1.6005e-01],\n",
      "         [0.0000e+00, 1.0000e-02, 2.0000e-02, 3.0000e-02, 4.0000e-02,\n",
      "          5.0000e-02, 6.0000e-02, 7.0000e-02, 8.0000e-02, 9.0000e-02],\n",
      "         [0.0000e+00, 5.6234e-03, 1.1247e-02, 1.6870e-02, 2.2494e-02,\n",
      "          2.8117e-02, 3.3740e-02, 3.9364e-02, 4.4987e-02, 5.0611e-02],\n",
      "         [0.0000e+00, 3.1623e-03, 6.3246e-03, 9.4868e-03, 1.2649e-02,\n",
      "          1.5811e-02, 1.8974e-02, 2.2136e-02, 2.5298e-02, 2.8461e-02],\n",
      "         [0.0000e+00, 1.7783e-03, 3.5566e-03, 5.3348e-03, 7.1131e-03,\n",
      "          8.8914e-03, 1.0670e-02, 1.2448e-02, 1.4226e-02, 1.6005e-02],\n",
      "         [0.0000e+00, 1.0000e-03, 2.0000e-03, 3.0000e-03, 4.0000e-03,\n",
      "          5.0000e-03, 6.0000e-03, 7.0000e-03, 8.0000e-03, 9.0000e-03],\n",
      "         [0.0000e+00, 5.6234e-04, 1.1247e-03, 1.6870e-03, 2.2494e-03,\n",
      "          2.8117e-03, 3.3740e-03, 3.9364e-03, 4.4987e-03, 5.0611e-03],\n",
      "         [0.0000e+00, 3.1623e-04, 6.3246e-04, 9.4868e-04, 1.2649e-03,\n",
      "          1.5811e-03, 1.8974e-03, 2.2136e-03, 2.5298e-03, 2.8461e-03],\n",
      "         [0.0000e+00, 1.7783e-04, 3.5566e-04, 5.3348e-04, 7.1131e-04,\n",
      "          8.8914e-04, 1.0670e-03, 1.2448e-03, 1.4226e-03, 1.6005e-03]]])\n",
      "q torch.Size([2, 8, 10, 32]) cos torch.Size([2, 1, 10, 32])\n",
      "xxxx tensor([1.0000e+00, 5.6234e-01, 3.1623e-01, 1.7783e-01, 1.0000e-01, 5.6234e-02,\n",
      "        3.1623e-02, 1.7783e-02, 1.0000e-02, 5.6234e-03, 3.1623e-03, 1.7783e-03,\n",
      "        1.0000e-03, 5.6234e-04, 3.1623e-04, 1.7783e-04])\n",
      "yyyy torch.Size([1, 16, 1]) tensor([[[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]]])\n",
      "zzzzz tensor([[[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]],\n",
      "\n",
      "        [[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]]])\n",
      "position_ids_expanded torch.Size([2, 1, 10]) tensor([[[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]],\n",
      "\n",
      "        [[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]]])\n",
      "xxxxyyyuuu torch.Size([2, 16, 1]) torch.Size([2, 1, 10])\n",
      "tttt torch.Size([2, 16, 10]) tensor([[[0.0000e+00, 1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00,\n",
      "          5.0000e+00, 6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00],\n",
      "         [0.0000e+00, 5.6234e-01, 1.1247e+00, 1.6870e+00, 2.2494e+00,\n",
      "          2.8117e+00, 3.3740e+00, 3.9364e+00, 4.4987e+00, 5.0611e+00],\n",
      "         [0.0000e+00, 3.1623e-01, 6.3246e-01, 9.4868e-01, 1.2649e+00,\n",
      "          1.5811e+00, 1.8974e+00, 2.2136e+00, 2.5298e+00, 2.8460e+00],\n",
      "         [0.0000e+00, 1.7783e-01, 3.5566e-01, 5.3348e-01, 7.1131e-01,\n",
      "          8.8914e-01, 1.0670e+00, 1.2448e+00, 1.4226e+00, 1.6005e+00],\n",
      "         [0.0000e+00, 1.0000e-01, 2.0000e-01, 3.0000e-01, 4.0000e-01,\n",
      "          5.0000e-01, 6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01],\n",
      "         [0.0000e+00, 5.6234e-02, 1.1247e-01, 1.6870e-01, 2.2494e-01,\n",
      "          2.8117e-01, 3.3740e-01, 3.9364e-01, 4.4987e-01, 5.0611e-01],\n",
      "         [0.0000e+00, 3.1623e-02, 6.3246e-02, 9.4868e-02, 1.2649e-01,\n",
      "          1.5811e-01, 1.8974e-01, 2.2136e-01, 2.5298e-01, 2.8460e-01],\n",
      "         [0.0000e+00, 1.7783e-02, 3.5566e-02, 5.3348e-02, 7.1131e-02,\n",
      "          8.8914e-02, 1.0670e-01, 1.2448e-01, 1.4226e-01, 1.6005e-01],\n",
      "         [0.0000e+00, 1.0000e-02, 2.0000e-02, 3.0000e-02, 4.0000e-02,\n",
      "          5.0000e-02, 6.0000e-02, 7.0000e-02, 8.0000e-02, 9.0000e-02],\n",
      "         [0.0000e+00, 5.6234e-03, 1.1247e-02, 1.6870e-02, 2.2494e-02,\n",
      "          2.8117e-02, 3.3740e-02, 3.9364e-02, 4.4987e-02, 5.0611e-02],\n",
      "         [0.0000e+00, 3.1623e-03, 6.3246e-03, 9.4868e-03, 1.2649e-02,\n",
      "          1.5811e-02, 1.8974e-02, 2.2136e-02, 2.5298e-02, 2.8461e-02],\n",
      "         [0.0000e+00, 1.7783e-03, 3.5566e-03, 5.3348e-03, 7.1131e-03,\n",
      "          8.8914e-03, 1.0670e-02, 1.2448e-02, 1.4226e-02, 1.6005e-02],\n",
      "         [0.0000e+00, 1.0000e-03, 2.0000e-03, 3.0000e-03, 4.0000e-03,\n",
      "          5.0000e-03, 6.0000e-03, 7.0000e-03, 8.0000e-03, 9.0000e-03],\n",
      "         [0.0000e+00, 5.6234e-04, 1.1247e-03, 1.6870e-03, 2.2494e-03,\n",
      "          2.8117e-03, 3.3740e-03, 3.9364e-03, 4.4987e-03, 5.0611e-03],\n",
      "         [0.0000e+00, 3.1623e-04, 6.3246e-04, 9.4868e-04, 1.2649e-03,\n",
      "          1.5811e-03, 1.8974e-03, 2.2136e-03, 2.5298e-03, 2.8461e-03],\n",
      "         [0.0000e+00, 1.7783e-04, 3.5566e-04, 5.3348e-04, 7.1131e-04,\n",
      "          8.8914e-04, 1.0670e-03, 1.2448e-03, 1.4226e-03, 1.6005e-03]],\n",
      "\n",
      "        [[0.0000e+00, 1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00,\n",
      "          5.0000e+00, 6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00],\n",
      "         [0.0000e+00, 5.6234e-01, 1.1247e+00, 1.6870e+00, 2.2494e+00,\n",
      "          2.8117e+00, 3.3740e+00, 3.9364e+00, 4.4987e+00, 5.0611e+00],\n",
      "         [0.0000e+00, 3.1623e-01, 6.3246e-01, 9.4868e-01, 1.2649e+00,\n",
      "          1.5811e+00, 1.8974e+00, 2.2136e+00, 2.5298e+00, 2.8460e+00],\n",
      "         [0.0000e+00, 1.7783e-01, 3.5566e-01, 5.3348e-01, 7.1131e-01,\n",
      "          8.8914e-01, 1.0670e+00, 1.2448e+00, 1.4226e+00, 1.6005e+00],\n",
      "         [0.0000e+00, 1.0000e-01, 2.0000e-01, 3.0000e-01, 4.0000e-01,\n",
      "          5.0000e-01, 6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01],\n",
      "         [0.0000e+00, 5.6234e-02, 1.1247e-01, 1.6870e-01, 2.2494e-01,\n",
      "          2.8117e-01, 3.3740e-01, 3.9364e-01, 4.4987e-01, 5.0611e-01],\n",
      "         [0.0000e+00, 3.1623e-02, 6.3246e-02, 9.4868e-02, 1.2649e-01,\n",
      "          1.5811e-01, 1.8974e-01, 2.2136e-01, 2.5298e-01, 2.8460e-01],\n",
      "         [0.0000e+00, 1.7783e-02, 3.5566e-02, 5.3348e-02, 7.1131e-02,\n",
      "          8.8914e-02, 1.0670e-01, 1.2448e-01, 1.4226e-01, 1.6005e-01],\n",
      "         [0.0000e+00, 1.0000e-02, 2.0000e-02, 3.0000e-02, 4.0000e-02,\n",
      "          5.0000e-02, 6.0000e-02, 7.0000e-02, 8.0000e-02, 9.0000e-02],\n",
      "         [0.0000e+00, 5.6234e-03, 1.1247e-02, 1.6870e-02, 2.2494e-02,\n",
      "          2.8117e-02, 3.3740e-02, 3.9364e-02, 4.4987e-02, 5.0611e-02],\n",
      "         [0.0000e+00, 3.1623e-03, 6.3246e-03, 9.4868e-03, 1.2649e-02,\n",
      "          1.5811e-02, 1.8974e-02, 2.2136e-02, 2.5298e-02, 2.8461e-02],\n",
      "         [0.0000e+00, 1.7783e-03, 3.5566e-03, 5.3348e-03, 7.1131e-03,\n",
      "          8.8914e-03, 1.0670e-02, 1.2448e-02, 1.4226e-02, 1.6005e-02],\n",
      "         [0.0000e+00, 1.0000e-03, 2.0000e-03, 3.0000e-03, 4.0000e-03,\n",
      "          5.0000e-03, 6.0000e-03, 7.0000e-03, 8.0000e-03, 9.0000e-03],\n",
      "         [0.0000e+00, 5.6234e-04, 1.1247e-03, 1.6870e-03, 2.2494e-03,\n",
      "          2.8117e-03, 3.3740e-03, 3.9364e-03, 4.4987e-03, 5.0611e-03],\n",
      "         [0.0000e+00, 3.1623e-04, 6.3246e-04, 9.4868e-04, 1.2649e-03,\n",
      "          1.5811e-03, 1.8974e-03, 2.2136e-03, 2.5298e-03, 2.8461e-03],\n",
      "         [0.0000e+00, 1.7783e-04, 3.5566e-04, 5.3348e-04, 7.1131e-04,\n",
      "          8.8914e-04, 1.0670e-03, 1.2448e-03, 1.4226e-03, 1.6005e-03]]])\n",
      "q torch.Size([2, 8, 10, 32]) cos torch.Size([2, 1, 10, 32])\n",
      "xxxx tensor([1.0000e+00, 5.6234e-01, 3.1623e-01, 1.7783e-01, 1.0000e-01, 5.6234e-02,\n",
      "        3.1623e-02, 1.7783e-02, 1.0000e-02, 5.6234e-03, 3.1623e-03, 1.7783e-03,\n",
      "        1.0000e-03, 5.6234e-04, 3.1623e-04, 1.7783e-04])\n",
      "yyyy torch.Size([1, 16, 1]) tensor([[[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]]])\n",
      "zzzzz tensor([[[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]],\n",
      "\n",
      "        [[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]]])\n",
      "position_ids_expanded torch.Size([2, 1, 10]) tensor([[[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]],\n",
      "\n",
      "        [[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]]])\n",
      "xxxxyyyuuu torch.Size([2, 16, 1]) torch.Size([2, 1, 10])\n",
      "tttt torch.Size([2, 16, 10]) tensor([[[0.0000e+00, 1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00,\n",
      "          5.0000e+00, 6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00],\n",
      "         [0.0000e+00, 5.6234e-01, 1.1247e+00, 1.6870e+00, 2.2494e+00,\n",
      "          2.8117e+00, 3.3740e+00, 3.9364e+00, 4.4987e+00, 5.0611e+00],\n",
      "         [0.0000e+00, 3.1623e-01, 6.3246e-01, 9.4868e-01, 1.2649e+00,\n",
      "          1.5811e+00, 1.8974e+00, 2.2136e+00, 2.5298e+00, 2.8460e+00],\n",
      "         [0.0000e+00, 1.7783e-01, 3.5566e-01, 5.3348e-01, 7.1131e-01,\n",
      "          8.8914e-01, 1.0670e+00, 1.2448e+00, 1.4226e+00, 1.6005e+00],\n",
      "         [0.0000e+00, 1.0000e-01, 2.0000e-01, 3.0000e-01, 4.0000e-01,\n",
      "          5.0000e-01, 6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01],\n",
      "         [0.0000e+00, 5.6234e-02, 1.1247e-01, 1.6870e-01, 2.2494e-01,\n",
      "          2.8117e-01, 3.3740e-01, 3.9364e-01, 4.4987e-01, 5.0611e-01],\n",
      "         [0.0000e+00, 3.1623e-02, 6.3246e-02, 9.4868e-02, 1.2649e-01,\n",
      "          1.5811e-01, 1.8974e-01, 2.2136e-01, 2.5298e-01, 2.8460e-01],\n",
      "         [0.0000e+00, 1.7783e-02, 3.5566e-02, 5.3348e-02, 7.1131e-02,\n",
      "          8.8914e-02, 1.0670e-01, 1.2448e-01, 1.4226e-01, 1.6005e-01],\n",
      "         [0.0000e+00, 1.0000e-02, 2.0000e-02, 3.0000e-02, 4.0000e-02,\n",
      "          5.0000e-02, 6.0000e-02, 7.0000e-02, 8.0000e-02, 9.0000e-02],\n",
      "         [0.0000e+00, 5.6234e-03, 1.1247e-02, 1.6870e-02, 2.2494e-02,\n",
      "          2.8117e-02, 3.3740e-02, 3.9364e-02, 4.4987e-02, 5.0611e-02],\n",
      "         [0.0000e+00, 3.1623e-03, 6.3246e-03, 9.4868e-03, 1.2649e-02,\n",
      "          1.5811e-02, 1.8974e-02, 2.2136e-02, 2.5298e-02, 2.8461e-02],\n",
      "         [0.0000e+00, 1.7783e-03, 3.5566e-03, 5.3348e-03, 7.1131e-03,\n",
      "          8.8914e-03, 1.0670e-02, 1.2448e-02, 1.4226e-02, 1.6005e-02],\n",
      "         [0.0000e+00, 1.0000e-03, 2.0000e-03, 3.0000e-03, 4.0000e-03,\n",
      "          5.0000e-03, 6.0000e-03, 7.0000e-03, 8.0000e-03, 9.0000e-03],\n",
      "         [0.0000e+00, 5.6234e-04, 1.1247e-03, 1.6870e-03, 2.2494e-03,\n",
      "          2.8117e-03, 3.3740e-03, 3.9364e-03, 4.4987e-03, 5.0611e-03],\n",
      "         [0.0000e+00, 3.1623e-04, 6.3246e-04, 9.4868e-04, 1.2649e-03,\n",
      "          1.5811e-03, 1.8974e-03, 2.2136e-03, 2.5298e-03, 2.8461e-03],\n",
      "         [0.0000e+00, 1.7783e-04, 3.5566e-04, 5.3348e-04, 7.1131e-04,\n",
      "          8.8914e-04, 1.0670e-03, 1.2448e-03, 1.4226e-03, 1.6005e-03]],\n",
      "\n",
      "        [[0.0000e+00, 1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00,\n",
      "          5.0000e+00, 6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00],\n",
      "         [0.0000e+00, 5.6234e-01, 1.1247e+00, 1.6870e+00, 2.2494e+00,\n",
      "          2.8117e+00, 3.3740e+00, 3.9364e+00, 4.4987e+00, 5.0611e+00],\n",
      "         [0.0000e+00, 3.1623e-01, 6.3246e-01, 9.4868e-01, 1.2649e+00,\n",
      "          1.5811e+00, 1.8974e+00, 2.2136e+00, 2.5298e+00, 2.8460e+00],\n",
      "         [0.0000e+00, 1.7783e-01, 3.5566e-01, 5.3348e-01, 7.1131e-01,\n",
      "          8.8914e-01, 1.0670e+00, 1.2448e+00, 1.4226e+00, 1.6005e+00],\n",
      "         [0.0000e+00, 1.0000e-01, 2.0000e-01, 3.0000e-01, 4.0000e-01,\n",
      "          5.0000e-01, 6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01],\n",
      "         [0.0000e+00, 5.6234e-02, 1.1247e-01, 1.6870e-01, 2.2494e-01,\n",
      "          2.8117e-01, 3.3740e-01, 3.9364e-01, 4.4987e-01, 5.0611e-01],\n",
      "         [0.0000e+00, 3.1623e-02, 6.3246e-02, 9.4868e-02, 1.2649e-01,\n",
      "          1.5811e-01, 1.8974e-01, 2.2136e-01, 2.5298e-01, 2.8460e-01],\n",
      "         [0.0000e+00, 1.7783e-02, 3.5566e-02, 5.3348e-02, 7.1131e-02,\n",
      "          8.8914e-02, 1.0670e-01, 1.2448e-01, 1.4226e-01, 1.6005e-01],\n",
      "         [0.0000e+00, 1.0000e-02, 2.0000e-02, 3.0000e-02, 4.0000e-02,\n",
      "          5.0000e-02, 6.0000e-02, 7.0000e-02, 8.0000e-02, 9.0000e-02],\n",
      "         [0.0000e+00, 5.6234e-03, 1.1247e-02, 1.6870e-02, 2.2494e-02,\n",
      "          2.8117e-02, 3.3740e-02, 3.9364e-02, 4.4987e-02, 5.0611e-02],\n",
      "         [0.0000e+00, 3.1623e-03, 6.3246e-03, 9.4868e-03, 1.2649e-02,\n",
      "          1.5811e-02, 1.8974e-02, 2.2136e-02, 2.5298e-02, 2.8461e-02],\n",
      "         [0.0000e+00, 1.7783e-03, 3.5566e-03, 5.3348e-03, 7.1131e-03,\n",
      "          8.8914e-03, 1.0670e-02, 1.2448e-02, 1.4226e-02, 1.6005e-02],\n",
      "         [0.0000e+00, 1.0000e-03, 2.0000e-03, 3.0000e-03, 4.0000e-03,\n",
      "          5.0000e-03, 6.0000e-03, 7.0000e-03, 8.0000e-03, 9.0000e-03],\n",
      "         [0.0000e+00, 5.6234e-04, 1.1247e-03, 1.6870e-03, 2.2494e-03,\n",
      "          2.8117e-03, 3.3740e-03, 3.9364e-03, 4.4987e-03, 5.0611e-03],\n",
      "         [0.0000e+00, 3.1623e-04, 6.3246e-04, 9.4868e-04, 1.2649e-03,\n",
      "          1.5811e-03, 1.8974e-03, 2.2136e-03, 2.5298e-03, 2.8461e-03],\n",
      "         [0.0000e+00, 1.7783e-04, 3.5566e-04, 5.3348e-04, 7.1131e-04,\n",
      "          8.8914e-04, 1.0670e-03, 1.2448e-03, 1.4226e-03, 1.6005e-03]]])\n",
      "q torch.Size([2, 8, 10, 32]) cos torch.Size([2, 1, 10, 32])\n",
      "xxxx tensor([1.0000e+00, 5.6234e-01, 3.1623e-01, 1.7783e-01, 1.0000e-01, 5.6234e-02,\n",
      "        3.1623e-02, 1.7783e-02, 1.0000e-02, 5.6234e-03, 3.1623e-03, 1.7783e-03,\n",
      "        1.0000e-03, 5.6234e-04, 3.1623e-04, 1.7783e-04])\n",
      "yyyy torch.Size([1, 16, 1]) tensor([[[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]]])\n",
      "zzzzz tensor([[[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]],\n",
      "\n",
      "        [[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]]])\n",
      "position_ids_expanded torch.Size([2, 1, 10]) tensor([[[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]],\n",
      "\n",
      "        [[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]]])\n",
      "xxxxyyyuuu torch.Size([2, 16, 1]) torch.Size([2, 1, 10])\n",
      "tttt torch.Size([2, 16, 10]) tensor([[[0.0000e+00, 1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00,\n",
      "          5.0000e+00, 6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00],\n",
      "         [0.0000e+00, 5.6234e-01, 1.1247e+00, 1.6870e+00, 2.2494e+00,\n",
      "          2.8117e+00, 3.3740e+00, 3.9364e+00, 4.4987e+00, 5.0611e+00],\n",
      "         [0.0000e+00, 3.1623e-01, 6.3246e-01, 9.4868e-01, 1.2649e+00,\n",
      "          1.5811e+00, 1.8974e+00, 2.2136e+00, 2.5298e+00, 2.8460e+00],\n",
      "         [0.0000e+00, 1.7783e-01, 3.5566e-01, 5.3348e-01, 7.1131e-01,\n",
      "          8.8914e-01, 1.0670e+00, 1.2448e+00, 1.4226e+00, 1.6005e+00],\n",
      "         [0.0000e+00, 1.0000e-01, 2.0000e-01, 3.0000e-01, 4.0000e-01,\n",
      "          5.0000e-01, 6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01],\n",
      "         [0.0000e+00, 5.6234e-02, 1.1247e-01, 1.6870e-01, 2.2494e-01,\n",
      "          2.8117e-01, 3.3740e-01, 3.9364e-01, 4.4987e-01, 5.0611e-01],\n",
      "         [0.0000e+00, 3.1623e-02, 6.3246e-02, 9.4868e-02, 1.2649e-01,\n",
      "          1.5811e-01, 1.8974e-01, 2.2136e-01, 2.5298e-01, 2.8460e-01],\n",
      "         [0.0000e+00, 1.7783e-02, 3.5566e-02, 5.3348e-02, 7.1131e-02,\n",
      "          8.8914e-02, 1.0670e-01, 1.2448e-01, 1.4226e-01, 1.6005e-01],\n",
      "         [0.0000e+00, 1.0000e-02, 2.0000e-02, 3.0000e-02, 4.0000e-02,\n",
      "          5.0000e-02, 6.0000e-02, 7.0000e-02, 8.0000e-02, 9.0000e-02],\n",
      "         [0.0000e+00, 5.6234e-03, 1.1247e-02, 1.6870e-02, 2.2494e-02,\n",
      "          2.8117e-02, 3.3740e-02, 3.9364e-02, 4.4987e-02, 5.0611e-02],\n",
      "         [0.0000e+00, 3.1623e-03, 6.3246e-03, 9.4868e-03, 1.2649e-02,\n",
      "          1.5811e-02, 1.8974e-02, 2.2136e-02, 2.5298e-02, 2.8461e-02],\n",
      "         [0.0000e+00, 1.7783e-03, 3.5566e-03, 5.3348e-03, 7.1131e-03,\n",
      "          8.8914e-03, 1.0670e-02, 1.2448e-02, 1.4226e-02, 1.6005e-02],\n",
      "         [0.0000e+00, 1.0000e-03, 2.0000e-03, 3.0000e-03, 4.0000e-03,\n",
      "          5.0000e-03, 6.0000e-03, 7.0000e-03, 8.0000e-03, 9.0000e-03],\n",
      "         [0.0000e+00, 5.6234e-04, 1.1247e-03, 1.6870e-03, 2.2494e-03,\n",
      "          2.8117e-03, 3.3740e-03, 3.9364e-03, 4.4987e-03, 5.0611e-03],\n",
      "         [0.0000e+00, 3.1623e-04, 6.3246e-04, 9.4868e-04, 1.2649e-03,\n",
      "          1.5811e-03, 1.8974e-03, 2.2136e-03, 2.5298e-03, 2.8461e-03],\n",
      "         [0.0000e+00, 1.7783e-04, 3.5566e-04, 5.3348e-04, 7.1131e-04,\n",
      "          8.8914e-04, 1.0670e-03, 1.2448e-03, 1.4226e-03, 1.6005e-03]],\n",
      "\n",
      "        [[0.0000e+00, 1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00,\n",
      "          5.0000e+00, 6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00],\n",
      "         [0.0000e+00, 5.6234e-01, 1.1247e+00, 1.6870e+00, 2.2494e+00,\n",
      "          2.8117e+00, 3.3740e+00, 3.9364e+00, 4.4987e+00, 5.0611e+00],\n",
      "         [0.0000e+00, 3.1623e-01, 6.3246e-01, 9.4868e-01, 1.2649e+00,\n",
      "          1.5811e+00, 1.8974e+00, 2.2136e+00, 2.5298e+00, 2.8460e+00],\n",
      "         [0.0000e+00, 1.7783e-01, 3.5566e-01, 5.3348e-01, 7.1131e-01,\n",
      "          8.8914e-01, 1.0670e+00, 1.2448e+00, 1.4226e+00, 1.6005e+00],\n",
      "         [0.0000e+00, 1.0000e-01, 2.0000e-01, 3.0000e-01, 4.0000e-01,\n",
      "          5.0000e-01, 6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01],\n",
      "         [0.0000e+00, 5.6234e-02, 1.1247e-01, 1.6870e-01, 2.2494e-01,\n",
      "          2.8117e-01, 3.3740e-01, 3.9364e-01, 4.4987e-01, 5.0611e-01],\n",
      "         [0.0000e+00, 3.1623e-02, 6.3246e-02, 9.4868e-02, 1.2649e-01,\n",
      "          1.5811e-01, 1.8974e-01, 2.2136e-01, 2.5298e-01, 2.8460e-01],\n",
      "         [0.0000e+00, 1.7783e-02, 3.5566e-02, 5.3348e-02, 7.1131e-02,\n",
      "          8.8914e-02, 1.0670e-01, 1.2448e-01, 1.4226e-01, 1.6005e-01],\n",
      "         [0.0000e+00, 1.0000e-02, 2.0000e-02, 3.0000e-02, 4.0000e-02,\n",
      "          5.0000e-02, 6.0000e-02, 7.0000e-02, 8.0000e-02, 9.0000e-02],\n",
      "         [0.0000e+00, 5.6234e-03, 1.1247e-02, 1.6870e-02, 2.2494e-02,\n",
      "          2.8117e-02, 3.3740e-02, 3.9364e-02, 4.4987e-02, 5.0611e-02],\n",
      "         [0.0000e+00, 3.1623e-03, 6.3246e-03, 9.4868e-03, 1.2649e-02,\n",
      "          1.5811e-02, 1.8974e-02, 2.2136e-02, 2.5298e-02, 2.8461e-02],\n",
      "         [0.0000e+00, 1.7783e-03, 3.5566e-03, 5.3348e-03, 7.1131e-03,\n",
      "          8.8914e-03, 1.0670e-02, 1.2448e-02, 1.4226e-02, 1.6005e-02],\n",
      "         [0.0000e+00, 1.0000e-03, 2.0000e-03, 3.0000e-03, 4.0000e-03,\n",
      "          5.0000e-03, 6.0000e-03, 7.0000e-03, 8.0000e-03, 9.0000e-03],\n",
      "         [0.0000e+00, 5.6234e-04, 1.1247e-03, 1.6870e-03, 2.2494e-03,\n",
      "          2.8117e-03, 3.3740e-03, 3.9364e-03, 4.4987e-03, 5.0611e-03],\n",
      "         [0.0000e+00, 3.1623e-04, 6.3246e-04, 9.4868e-04, 1.2649e-03,\n",
      "          1.5811e-03, 1.8974e-03, 2.2136e-03, 2.5298e-03, 2.8461e-03],\n",
      "         [0.0000e+00, 1.7783e-04, 3.5566e-04, 5.3348e-04, 7.1131e-04,\n",
      "          8.8914e-04, 1.0670e-03, 1.2448e-03, 1.4226e-03, 1.6005e-03]]])\n",
      "q torch.Size([2, 8, 10, 32]) cos torch.Size([2, 1, 10, 32])\n",
      "xxxx tensor([1.0000e+00, 5.6234e-01, 3.1623e-01, 1.7783e-01, 1.0000e-01, 5.6234e-02,\n",
      "        3.1623e-02, 1.7783e-02, 1.0000e-02, 5.6234e-03, 3.1623e-03, 1.7783e-03,\n",
      "        1.0000e-03, 5.6234e-04, 3.1623e-04, 1.7783e-04])\n",
      "yyyy torch.Size([1, 16, 1]) tensor([[[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]]])\n",
      "zzzzz tensor([[[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]],\n",
      "\n",
      "        [[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]]])\n",
      "position_ids_expanded torch.Size([2, 1, 10]) tensor([[[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]],\n",
      "\n",
      "        [[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]]])\n",
      "xxxxyyyuuu torch.Size([2, 16, 1]) torch.Size([2, 1, 10])\n",
      "tttt torch.Size([2, 16, 10]) tensor([[[0.0000e+00, 1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00,\n",
      "          5.0000e+00, 6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00],\n",
      "         [0.0000e+00, 5.6234e-01, 1.1247e+00, 1.6870e+00, 2.2494e+00,\n",
      "          2.8117e+00, 3.3740e+00, 3.9364e+00, 4.4987e+00, 5.0611e+00],\n",
      "         [0.0000e+00, 3.1623e-01, 6.3246e-01, 9.4868e-01, 1.2649e+00,\n",
      "          1.5811e+00, 1.8974e+00, 2.2136e+00, 2.5298e+00, 2.8460e+00],\n",
      "         [0.0000e+00, 1.7783e-01, 3.5566e-01, 5.3348e-01, 7.1131e-01,\n",
      "          8.8914e-01, 1.0670e+00, 1.2448e+00, 1.4226e+00, 1.6005e+00],\n",
      "         [0.0000e+00, 1.0000e-01, 2.0000e-01, 3.0000e-01, 4.0000e-01,\n",
      "          5.0000e-01, 6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01],\n",
      "         [0.0000e+00, 5.6234e-02, 1.1247e-01, 1.6870e-01, 2.2494e-01,\n",
      "          2.8117e-01, 3.3740e-01, 3.9364e-01, 4.4987e-01, 5.0611e-01],\n",
      "         [0.0000e+00, 3.1623e-02, 6.3246e-02, 9.4868e-02, 1.2649e-01,\n",
      "          1.5811e-01, 1.8974e-01, 2.2136e-01, 2.5298e-01, 2.8460e-01],\n",
      "         [0.0000e+00, 1.7783e-02, 3.5566e-02, 5.3348e-02, 7.1131e-02,\n",
      "          8.8914e-02, 1.0670e-01, 1.2448e-01, 1.4226e-01, 1.6005e-01],\n",
      "         [0.0000e+00, 1.0000e-02, 2.0000e-02, 3.0000e-02, 4.0000e-02,\n",
      "          5.0000e-02, 6.0000e-02, 7.0000e-02, 8.0000e-02, 9.0000e-02],\n",
      "         [0.0000e+00, 5.6234e-03, 1.1247e-02, 1.6870e-02, 2.2494e-02,\n",
      "          2.8117e-02, 3.3740e-02, 3.9364e-02, 4.4987e-02, 5.0611e-02],\n",
      "         [0.0000e+00, 3.1623e-03, 6.3246e-03, 9.4868e-03, 1.2649e-02,\n",
      "          1.5811e-02, 1.8974e-02, 2.2136e-02, 2.5298e-02, 2.8461e-02],\n",
      "         [0.0000e+00, 1.7783e-03, 3.5566e-03, 5.3348e-03, 7.1131e-03,\n",
      "          8.8914e-03, 1.0670e-02, 1.2448e-02, 1.4226e-02, 1.6005e-02],\n",
      "         [0.0000e+00, 1.0000e-03, 2.0000e-03, 3.0000e-03, 4.0000e-03,\n",
      "          5.0000e-03, 6.0000e-03, 7.0000e-03, 8.0000e-03, 9.0000e-03],\n",
      "         [0.0000e+00, 5.6234e-04, 1.1247e-03, 1.6870e-03, 2.2494e-03,\n",
      "          2.8117e-03, 3.3740e-03, 3.9364e-03, 4.4987e-03, 5.0611e-03],\n",
      "         [0.0000e+00, 3.1623e-04, 6.3246e-04, 9.4868e-04, 1.2649e-03,\n",
      "          1.5811e-03, 1.8974e-03, 2.2136e-03, 2.5298e-03, 2.8461e-03],\n",
      "         [0.0000e+00, 1.7783e-04, 3.5566e-04, 5.3348e-04, 7.1131e-04,\n",
      "          8.8914e-04, 1.0670e-03, 1.2448e-03, 1.4226e-03, 1.6005e-03]],\n",
      "\n",
      "        [[0.0000e+00, 1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00,\n",
      "          5.0000e+00, 6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00],\n",
      "         [0.0000e+00, 5.6234e-01, 1.1247e+00, 1.6870e+00, 2.2494e+00,\n",
      "          2.8117e+00, 3.3740e+00, 3.9364e+00, 4.4987e+00, 5.0611e+00],\n",
      "         [0.0000e+00, 3.1623e-01, 6.3246e-01, 9.4868e-01, 1.2649e+00,\n",
      "          1.5811e+00, 1.8974e+00, 2.2136e+00, 2.5298e+00, 2.8460e+00],\n",
      "         [0.0000e+00, 1.7783e-01, 3.5566e-01, 5.3348e-01, 7.1131e-01,\n",
      "          8.8914e-01, 1.0670e+00, 1.2448e+00, 1.4226e+00, 1.6005e+00],\n",
      "         [0.0000e+00, 1.0000e-01, 2.0000e-01, 3.0000e-01, 4.0000e-01,\n",
      "          5.0000e-01, 6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01],\n",
      "         [0.0000e+00, 5.6234e-02, 1.1247e-01, 1.6870e-01, 2.2494e-01,\n",
      "          2.8117e-01, 3.3740e-01, 3.9364e-01, 4.4987e-01, 5.0611e-01],\n",
      "         [0.0000e+00, 3.1623e-02, 6.3246e-02, 9.4868e-02, 1.2649e-01,\n",
      "          1.5811e-01, 1.8974e-01, 2.2136e-01, 2.5298e-01, 2.8460e-01],\n",
      "         [0.0000e+00, 1.7783e-02, 3.5566e-02, 5.3348e-02, 7.1131e-02,\n",
      "          8.8914e-02, 1.0670e-01, 1.2448e-01, 1.4226e-01, 1.6005e-01],\n",
      "         [0.0000e+00, 1.0000e-02, 2.0000e-02, 3.0000e-02, 4.0000e-02,\n",
      "          5.0000e-02, 6.0000e-02, 7.0000e-02, 8.0000e-02, 9.0000e-02],\n",
      "         [0.0000e+00, 5.6234e-03, 1.1247e-02, 1.6870e-02, 2.2494e-02,\n",
      "          2.8117e-02, 3.3740e-02, 3.9364e-02, 4.4987e-02, 5.0611e-02],\n",
      "         [0.0000e+00, 3.1623e-03, 6.3246e-03, 9.4868e-03, 1.2649e-02,\n",
      "          1.5811e-02, 1.8974e-02, 2.2136e-02, 2.5298e-02, 2.8461e-02],\n",
      "         [0.0000e+00, 1.7783e-03, 3.5566e-03, 5.3348e-03, 7.1131e-03,\n",
      "          8.8914e-03, 1.0670e-02, 1.2448e-02, 1.4226e-02, 1.6005e-02],\n",
      "         [0.0000e+00, 1.0000e-03, 2.0000e-03, 3.0000e-03, 4.0000e-03,\n",
      "          5.0000e-03, 6.0000e-03, 7.0000e-03, 8.0000e-03, 9.0000e-03],\n",
      "         [0.0000e+00, 5.6234e-04, 1.1247e-03, 1.6870e-03, 2.2494e-03,\n",
      "          2.8117e-03, 3.3740e-03, 3.9364e-03, 4.4987e-03, 5.0611e-03],\n",
      "         [0.0000e+00, 3.1623e-04, 6.3246e-04, 9.4868e-04, 1.2649e-03,\n",
      "          1.5811e-03, 1.8974e-03, 2.2136e-03, 2.5298e-03, 2.8461e-03],\n",
      "         [0.0000e+00, 1.7783e-04, 3.5566e-04, 5.3348e-04, 7.1131e-04,\n",
      "          8.8914e-04, 1.0670e-03, 1.2448e-03, 1.4226e-03, 1.6005e-03]]])\n",
      "q torch.Size([2, 8, 10, 32]) cos torch.Size([2, 1, 10, 32])\n",
      "xxxx tensor([1.0000e+00, 5.6234e-01, 3.1623e-01, 1.7783e-01, 1.0000e-01, 5.6234e-02,\n",
      "        3.1623e-02, 1.7783e-02, 1.0000e-02, 5.6234e-03, 3.1623e-03, 1.7783e-03,\n",
      "        1.0000e-03, 5.6234e-04, 3.1623e-04, 1.7783e-04])\n",
      "yyyy torch.Size([1, 16, 1]) tensor([[[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]]])\n",
      "zzzzz tensor([[[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]],\n",
      "\n",
      "        [[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]]])\n",
      "position_ids_expanded torch.Size([2, 1, 10]) tensor([[[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]],\n",
      "\n",
      "        [[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]]])\n",
      "xxxxyyyuuu torch.Size([2, 16, 1]) torch.Size([2, 1, 10])\n",
      "tttt torch.Size([2, 16, 10]) tensor([[[0.0000e+00, 1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00,\n",
      "          5.0000e+00, 6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00],\n",
      "         [0.0000e+00, 5.6234e-01, 1.1247e+00, 1.6870e+00, 2.2494e+00,\n",
      "          2.8117e+00, 3.3740e+00, 3.9364e+00, 4.4987e+00, 5.0611e+00],\n",
      "         [0.0000e+00, 3.1623e-01, 6.3246e-01, 9.4868e-01, 1.2649e+00,\n",
      "          1.5811e+00, 1.8974e+00, 2.2136e+00, 2.5298e+00, 2.8460e+00],\n",
      "         [0.0000e+00, 1.7783e-01, 3.5566e-01, 5.3348e-01, 7.1131e-01,\n",
      "          8.8914e-01, 1.0670e+00, 1.2448e+00, 1.4226e+00, 1.6005e+00],\n",
      "         [0.0000e+00, 1.0000e-01, 2.0000e-01, 3.0000e-01, 4.0000e-01,\n",
      "          5.0000e-01, 6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01],\n",
      "         [0.0000e+00, 5.6234e-02, 1.1247e-01, 1.6870e-01, 2.2494e-01,\n",
      "          2.8117e-01, 3.3740e-01, 3.9364e-01, 4.4987e-01, 5.0611e-01],\n",
      "         [0.0000e+00, 3.1623e-02, 6.3246e-02, 9.4868e-02, 1.2649e-01,\n",
      "          1.5811e-01, 1.8974e-01, 2.2136e-01, 2.5298e-01, 2.8460e-01],\n",
      "         [0.0000e+00, 1.7783e-02, 3.5566e-02, 5.3348e-02, 7.1131e-02,\n",
      "          8.8914e-02, 1.0670e-01, 1.2448e-01, 1.4226e-01, 1.6005e-01],\n",
      "         [0.0000e+00, 1.0000e-02, 2.0000e-02, 3.0000e-02, 4.0000e-02,\n",
      "          5.0000e-02, 6.0000e-02, 7.0000e-02, 8.0000e-02, 9.0000e-02],\n",
      "         [0.0000e+00, 5.6234e-03, 1.1247e-02, 1.6870e-02, 2.2494e-02,\n",
      "          2.8117e-02, 3.3740e-02, 3.9364e-02, 4.4987e-02, 5.0611e-02],\n",
      "         [0.0000e+00, 3.1623e-03, 6.3246e-03, 9.4868e-03, 1.2649e-02,\n",
      "          1.5811e-02, 1.8974e-02, 2.2136e-02, 2.5298e-02, 2.8461e-02],\n",
      "         [0.0000e+00, 1.7783e-03, 3.5566e-03, 5.3348e-03, 7.1131e-03,\n",
      "          8.8914e-03, 1.0670e-02, 1.2448e-02, 1.4226e-02, 1.6005e-02],\n",
      "         [0.0000e+00, 1.0000e-03, 2.0000e-03, 3.0000e-03, 4.0000e-03,\n",
      "          5.0000e-03, 6.0000e-03, 7.0000e-03, 8.0000e-03, 9.0000e-03],\n",
      "         [0.0000e+00, 5.6234e-04, 1.1247e-03, 1.6870e-03, 2.2494e-03,\n",
      "          2.8117e-03, 3.3740e-03, 3.9364e-03, 4.4987e-03, 5.0611e-03],\n",
      "         [0.0000e+00, 3.1623e-04, 6.3246e-04, 9.4868e-04, 1.2649e-03,\n",
      "          1.5811e-03, 1.8974e-03, 2.2136e-03, 2.5298e-03, 2.8461e-03],\n",
      "         [0.0000e+00, 1.7783e-04, 3.5566e-04, 5.3348e-04, 7.1131e-04,\n",
      "          8.8914e-04, 1.0670e-03, 1.2448e-03, 1.4226e-03, 1.6005e-03]],\n",
      "\n",
      "        [[0.0000e+00, 1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00,\n",
      "          5.0000e+00, 6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00],\n",
      "         [0.0000e+00, 5.6234e-01, 1.1247e+00, 1.6870e+00, 2.2494e+00,\n",
      "          2.8117e+00, 3.3740e+00, 3.9364e+00, 4.4987e+00, 5.0611e+00],\n",
      "         [0.0000e+00, 3.1623e-01, 6.3246e-01, 9.4868e-01, 1.2649e+00,\n",
      "          1.5811e+00, 1.8974e+00, 2.2136e+00, 2.5298e+00, 2.8460e+00],\n",
      "         [0.0000e+00, 1.7783e-01, 3.5566e-01, 5.3348e-01, 7.1131e-01,\n",
      "          8.8914e-01, 1.0670e+00, 1.2448e+00, 1.4226e+00, 1.6005e+00],\n",
      "         [0.0000e+00, 1.0000e-01, 2.0000e-01, 3.0000e-01, 4.0000e-01,\n",
      "          5.0000e-01, 6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01],\n",
      "         [0.0000e+00, 5.6234e-02, 1.1247e-01, 1.6870e-01, 2.2494e-01,\n",
      "          2.8117e-01, 3.3740e-01, 3.9364e-01, 4.4987e-01, 5.0611e-01],\n",
      "         [0.0000e+00, 3.1623e-02, 6.3246e-02, 9.4868e-02, 1.2649e-01,\n",
      "          1.5811e-01, 1.8974e-01, 2.2136e-01, 2.5298e-01, 2.8460e-01],\n",
      "         [0.0000e+00, 1.7783e-02, 3.5566e-02, 5.3348e-02, 7.1131e-02,\n",
      "          8.8914e-02, 1.0670e-01, 1.2448e-01, 1.4226e-01, 1.6005e-01],\n",
      "         [0.0000e+00, 1.0000e-02, 2.0000e-02, 3.0000e-02, 4.0000e-02,\n",
      "          5.0000e-02, 6.0000e-02, 7.0000e-02, 8.0000e-02, 9.0000e-02],\n",
      "         [0.0000e+00, 5.6234e-03, 1.1247e-02, 1.6870e-02, 2.2494e-02,\n",
      "          2.8117e-02, 3.3740e-02, 3.9364e-02, 4.4987e-02, 5.0611e-02],\n",
      "         [0.0000e+00, 3.1623e-03, 6.3246e-03, 9.4868e-03, 1.2649e-02,\n",
      "          1.5811e-02, 1.8974e-02, 2.2136e-02, 2.5298e-02, 2.8461e-02],\n",
      "         [0.0000e+00, 1.7783e-03, 3.5566e-03, 5.3348e-03, 7.1131e-03,\n",
      "          8.8914e-03, 1.0670e-02, 1.2448e-02, 1.4226e-02, 1.6005e-02],\n",
      "         [0.0000e+00, 1.0000e-03, 2.0000e-03, 3.0000e-03, 4.0000e-03,\n",
      "          5.0000e-03, 6.0000e-03, 7.0000e-03, 8.0000e-03, 9.0000e-03],\n",
      "         [0.0000e+00, 5.6234e-04, 1.1247e-03, 1.6870e-03, 2.2494e-03,\n",
      "          2.8117e-03, 3.3740e-03, 3.9364e-03, 4.4987e-03, 5.0611e-03],\n",
      "         [0.0000e+00, 3.1623e-04, 6.3246e-04, 9.4868e-04, 1.2649e-03,\n",
      "          1.5811e-03, 1.8974e-03, 2.2136e-03, 2.5298e-03, 2.8461e-03],\n",
      "         [0.0000e+00, 1.7783e-04, 3.5566e-04, 5.3348e-04, 7.1131e-04,\n",
      "          8.8914e-04, 1.0670e-03, 1.2448e-03, 1.4226e-03, 1.6005e-03]]])\n",
      "q torch.Size([2, 8, 10, 32]) cos torch.Size([2, 1, 10, 32])\n",
      "Model Output Shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "# 创建模型\n",
    "\n",
    "\n",
    "class GemmaConfig:\n",
    "    def __init__(self):\n",
    "        self.hidden_size = 512  # 隐藏层大小\n",
    "        self.intermediate_size = 512\n",
    "        self.num_heads = 8   # 注意力头数\n",
    "        self.head_dim=32\n",
    "        self.num_key_value_heads = 4  # Key-Value头数\n",
    "        self.attention_dropout = 0.1  # Dropout率\n",
    "        self.max_position_embeddings = 1024  # 最大位置嵌入数\n",
    "        self.rope_theta = 10000  # 旋转位置编码的基数\n",
    "        self.attention_bias = False  # 是否使用偏置\n",
    "        self.rms_norm_eps = 0.01\n",
    "        self.intermediate_size=512  # 前馈网络的大小\n",
    "        self.num_hidden_layers=6 # 解码器层的数量\n",
    "        self.vocab_size=10000 # 假设词汇表大小\n",
    "        self.pad_token_id=0  # 填充 token 的 id\n",
    "    \n",
    "\n",
    "config = GemmaConfig()\n",
    "model = GemmaModel(config)\n",
    "\n",
    "# 假设有一个批次的输入\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "\n",
    "# 输入的 token 嵌入，假设已经通过 get_input_embeddings 获得\n",
    "input_embeds = torch.randn(batch_size, seq_len, config.hidden_size)\n",
    "\n",
    "# 创建 attention_mask 和 position_ids\n",
    "attention_mask = torch.ones(batch_size, 1, seq_len, seq_len)  # 假设没有实际的掩码\n",
    "position_ids = torch.arange(seq_len).unsqueeze(0).repeat(batch_size, 1)\n",
    "\n",
    "# 前向传播\n",
    "output = model(\n",
    "    attention_mask=attention_mask,\n",
    "    position_ids=position_ids,\n",
    "    inputs_embeds=input_embeds\n",
    ")\n",
    "\n",
    "# 打印输出的形状\n",
    "print(\"Model Output Shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b647e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13795472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb336413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b7acb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaForCausalLM(nn.Module):\n",
    "    \n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.model = GemmaModel(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size,config.vocab_size,\n",
    "                                bias=False)\n",
    "    \n",
    "    def get_input_embeddings(self):\n",
    "        return self.model.embed_tokens\n",
    "    \n",
    "    def tie_weights(self):\n",
    "        self.lm_head.weight = self.model.embed_tokens.weight\n",
    "    \n",
    "    def forward(self,\n",
    "               attention_mask: Optional[torch.Tensor]=None,\n",
    "               position_ids: Optional[torch.LongTensor]=None,\n",
    "               inputs_embeds: Optional[torch.FloatTensor]=None,\n",
    "               kv_cache: Optional[KVCache]=None,\n",
    "               )->Tuple:\n",
    "        outputs = self.model(\n",
    "           attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            kv_cache=kv_cache\n",
    "        )\n",
    "        \n",
    "        hidden_states = outputs\n",
    "        logits = self.lm_head(hidden_states)\n",
    "        logits = logits.float()\n",
    "        \n",
    "        return_data = {\n",
    "            \"logits\": logits\n",
    "        }\n",
    "        \n",
    "        if kv_cache is not None:\n",
    "            return_data['kv_cache'] = kv_cache\n",
    "        return return_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d9b64eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxx tensor([1.0000e+00, 5.6234e-01, 3.1623e-01, 1.7783e-01, 1.0000e-01, 5.6234e-02,\n",
      "        3.1623e-02, 1.7783e-02, 1.0000e-02, 5.6234e-03, 3.1623e-03, 1.7783e-03,\n",
      "        1.0000e-03, 5.6234e-04, 3.1623e-04, 1.7783e-04])\n",
      "yyyy torch.Size([1, 16, 1]) tensor([[[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]]])\n",
      "zzzzz tensor([[[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]],\n",
      "\n",
      "        [[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]]])\n",
      "position_ids_expanded torch.Size([2, 1, 10]) tensor([[[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]],\n",
      "\n",
      "        [[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]]])\n",
      "xxxxyyyuuu torch.Size([2, 16, 1]) torch.Size([2, 1, 10])\n",
      "tttt torch.Size([2, 16, 10]) tensor([[[0.0000e+00, 1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00,\n",
      "          5.0000e+00, 6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00],\n",
      "         [0.0000e+00, 5.6234e-01, 1.1247e+00, 1.6870e+00, 2.2494e+00,\n",
      "          2.8117e+00, 3.3740e+00, 3.9364e+00, 4.4987e+00, 5.0611e+00],\n",
      "         [0.0000e+00, 3.1623e-01, 6.3246e-01, 9.4868e-01, 1.2649e+00,\n",
      "          1.5811e+00, 1.8974e+00, 2.2136e+00, 2.5298e+00, 2.8460e+00],\n",
      "         [0.0000e+00, 1.7783e-01, 3.5566e-01, 5.3348e-01, 7.1131e-01,\n",
      "          8.8914e-01, 1.0670e+00, 1.2448e+00, 1.4226e+00, 1.6005e+00],\n",
      "         [0.0000e+00, 1.0000e-01, 2.0000e-01, 3.0000e-01, 4.0000e-01,\n",
      "          5.0000e-01, 6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01],\n",
      "         [0.0000e+00, 5.6234e-02, 1.1247e-01, 1.6870e-01, 2.2494e-01,\n",
      "          2.8117e-01, 3.3740e-01, 3.9364e-01, 4.4987e-01, 5.0611e-01],\n",
      "         [0.0000e+00, 3.1623e-02, 6.3246e-02, 9.4868e-02, 1.2649e-01,\n",
      "          1.5811e-01, 1.8974e-01, 2.2136e-01, 2.5298e-01, 2.8460e-01],\n",
      "         [0.0000e+00, 1.7783e-02, 3.5566e-02, 5.3348e-02, 7.1131e-02,\n",
      "          8.8914e-02, 1.0670e-01, 1.2448e-01, 1.4226e-01, 1.6005e-01],\n",
      "         [0.0000e+00, 1.0000e-02, 2.0000e-02, 3.0000e-02, 4.0000e-02,\n",
      "          5.0000e-02, 6.0000e-02, 7.0000e-02, 8.0000e-02, 9.0000e-02],\n",
      "         [0.0000e+00, 5.6234e-03, 1.1247e-02, 1.6870e-02, 2.2494e-02,\n",
      "          2.8117e-02, 3.3740e-02, 3.9364e-02, 4.4987e-02, 5.0611e-02],\n",
      "         [0.0000e+00, 3.1623e-03, 6.3246e-03, 9.4868e-03, 1.2649e-02,\n",
      "          1.5811e-02, 1.8974e-02, 2.2136e-02, 2.5298e-02, 2.8461e-02],\n",
      "         [0.0000e+00, 1.7783e-03, 3.5566e-03, 5.3348e-03, 7.1131e-03,\n",
      "          8.8914e-03, 1.0670e-02, 1.2448e-02, 1.4226e-02, 1.6005e-02],\n",
      "         [0.0000e+00, 1.0000e-03, 2.0000e-03, 3.0000e-03, 4.0000e-03,\n",
      "          5.0000e-03, 6.0000e-03, 7.0000e-03, 8.0000e-03, 9.0000e-03],\n",
      "         [0.0000e+00, 5.6234e-04, 1.1247e-03, 1.6870e-03, 2.2494e-03,\n",
      "          2.8117e-03, 3.3740e-03, 3.9364e-03, 4.4987e-03, 5.0611e-03],\n",
      "         [0.0000e+00, 3.1623e-04, 6.3246e-04, 9.4868e-04, 1.2649e-03,\n",
      "          1.5811e-03, 1.8974e-03, 2.2136e-03, 2.5298e-03, 2.8461e-03],\n",
      "         [0.0000e+00, 1.7783e-04, 3.5566e-04, 5.3348e-04, 7.1131e-04,\n",
      "          8.8914e-04, 1.0670e-03, 1.2448e-03, 1.4226e-03, 1.6005e-03]],\n",
      "\n",
      "        [[0.0000e+00, 1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00,\n",
      "          5.0000e+00, 6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00],\n",
      "         [0.0000e+00, 5.6234e-01, 1.1247e+00, 1.6870e+00, 2.2494e+00,\n",
      "          2.8117e+00, 3.3740e+00, 3.9364e+00, 4.4987e+00, 5.0611e+00],\n",
      "         [0.0000e+00, 3.1623e-01, 6.3246e-01, 9.4868e-01, 1.2649e+00,\n",
      "          1.5811e+00, 1.8974e+00, 2.2136e+00, 2.5298e+00, 2.8460e+00],\n",
      "         [0.0000e+00, 1.7783e-01, 3.5566e-01, 5.3348e-01, 7.1131e-01,\n",
      "          8.8914e-01, 1.0670e+00, 1.2448e+00, 1.4226e+00, 1.6005e+00],\n",
      "         [0.0000e+00, 1.0000e-01, 2.0000e-01, 3.0000e-01, 4.0000e-01,\n",
      "          5.0000e-01, 6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01],\n",
      "         [0.0000e+00, 5.6234e-02, 1.1247e-01, 1.6870e-01, 2.2494e-01,\n",
      "          2.8117e-01, 3.3740e-01, 3.9364e-01, 4.4987e-01, 5.0611e-01],\n",
      "         [0.0000e+00, 3.1623e-02, 6.3246e-02, 9.4868e-02, 1.2649e-01,\n",
      "          1.5811e-01, 1.8974e-01, 2.2136e-01, 2.5298e-01, 2.8460e-01],\n",
      "         [0.0000e+00, 1.7783e-02, 3.5566e-02, 5.3348e-02, 7.1131e-02,\n",
      "          8.8914e-02, 1.0670e-01, 1.2448e-01, 1.4226e-01, 1.6005e-01],\n",
      "         [0.0000e+00, 1.0000e-02, 2.0000e-02, 3.0000e-02, 4.0000e-02,\n",
      "          5.0000e-02, 6.0000e-02, 7.0000e-02, 8.0000e-02, 9.0000e-02],\n",
      "         [0.0000e+00, 5.6234e-03, 1.1247e-02, 1.6870e-02, 2.2494e-02,\n",
      "          2.8117e-02, 3.3740e-02, 3.9364e-02, 4.4987e-02, 5.0611e-02],\n",
      "         [0.0000e+00, 3.1623e-03, 6.3246e-03, 9.4868e-03, 1.2649e-02,\n",
      "          1.5811e-02, 1.8974e-02, 2.2136e-02, 2.5298e-02, 2.8461e-02],\n",
      "         [0.0000e+00, 1.7783e-03, 3.5566e-03, 5.3348e-03, 7.1131e-03,\n",
      "          8.8914e-03, 1.0670e-02, 1.2448e-02, 1.4226e-02, 1.6005e-02],\n",
      "         [0.0000e+00, 1.0000e-03, 2.0000e-03, 3.0000e-03, 4.0000e-03,\n",
      "          5.0000e-03, 6.0000e-03, 7.0000e-03, 8.0000e-03, 9.0000e-03],\n",
      "         [0.0000e+00, 5.6234e-04, 1.1247e-03, 1.6870e-03, 2.2494e-03,\n",
      "          2.8117e-03, 3.3740e-03, 3.9364e-03, 4.4987e-03, 5.0611e-03],\n",
      "         [0.0000e+00, 3.1623e-04, 6.3246e-04, 9.4868e-04, 1.2649e-03,\n",
      "          1.5811e-03, 1.8974e-03, 2.2136e-03, 2.5298e-03, 2.8461e-03],\n",
      "         [0.0000e+00, 1.7783e-04, 3.5566e-04, 5.3348e-04, 7.1131e-04,\n",
      "          8.8914e-04, 1.0670e-03, 1.2448e-03, 1.4226e-03, 1.6005e-03]]])\n",
      "q torch.Size([2, 8, 10, 32]) cos torch.Size([2, 1, 10, 32])\n",
      "xxxx tensor([1.0000e+00, 5.6234e-01, 3.1623e-01, 1.7783e-01, 1.0000e-01, 5.6234e-02,\n",
      "        3.1623e-02, 1.7783e-02, 1.0000e-02, 5.6234e-03, 3.1623e-03, 1.7783e-03,\n",
      "        1.0000e-03, 5.6234e-04, 3.1623e-04, 1.7783e-04])\n",
      "yyyy torch.Size([1, 16, 1]) tensor([[[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]]])\n",
      "zzzzz tensor([[[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]],\n",
      "\n",
      "        [[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]]])\n",
      "position_ids_expanded torch.Size([2, 1, 10]) tensor([[[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]],\n",
      "\n",
      "        [[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]]])\n",
      "xxxxyyyuuu torch.Size([2, 16, 1]) torch.Size([2, 1, 10])\n",
      "tttt torch.Size([2, 16, 10]) tensor([[[0.0000e+00, 1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00,\n",
      "          5.0000e+00, 6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00],\n",
      "         [0.0000e+00, 5.6234e-01, 1.1247e+00, 1.6870e+00, 2.2494e+00,\n",
      "          2.8117e+00, 3.3740e+00, 3.9364e+00, 4.4987e+00, 5.0611e+00],\n",
      "         [0.0000e+00, 3.1623e-01, 6.3246e-01, 9.4868e-01, 1.2649e+00,\n",
      "          1.5811e+00, 1.8974e+00, 2.2136e+00, 2.5298e+00, 2.8460e+00],\n",
      "         [0.0000e+00, 1.7783e-01, 3.5566e-01, 5.3348e-01, 7.1131e-01,\n",
      "          8.8914e-01, 1.0670e+00, 1.2448e+00, 1.4226e+00, 1.6005e+00],\n",
      "         [0.0000e+00, 1.0000e-01, 2.0000e-01, 3.0000e-01, 4.0000e-01,\n",
      "          5.0000e-01, 6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01],\n",
      "         [0.0000e+00, 5.6234e-02, 1.1247e-01, 1.6870e-01, 2.2494e-01,\n",
      "          2.8117e-01, 3.3740e-01, 3.9364e-01, 4.4987e-01, 5.0611e-01],\n",
      "         [0.0000e+00, 3.1623e-02, 6.3246e-02, 9.4868e-02, 1.2649e-01,\n",
      "          1.5811e-01, 1.8974e-01, 2.2136e-01, 2.5298e-01, 2.8460e-01],\n",
      "         [0.0000e+00, 1.7783e-02, 3.5566e-02, 5.3348e-02, 7.1131e-02,\n",
      "          8.8914e-02, 1.0670e-01, 1.2448e-01, 1.4226e-01, 1.6005e-01],\n",
      "         [0.0000e+00, 1.0000e-02, 2.0000e-02, 3.0000e-02, 4.0000e-02,\n",
      "          5.0000e-02, 6.0000e-02, 7.0000e-02, 8.0000e-02, 9.0000e-02],\n",
      "         [0.0000e+00, 5.6234e-03, 1.1247e-02, 1.6870e-02, 2.2494e-02,\n",
      "          2.8117e-02, 3.3740e-02, 3.9364e-02, 4.4987e-02, 5.0611e-02],\n",
      "         [0.0000e+00, 3.1623e-03, 6.3246e-03, 9.4868e-03, 1.2649e-02,\n",
      "          1.5811e-02, 1.8974e-02, 2.2136e-02, 2.5298e-02, 2.8461e-02],\n",
      "         [0.0000e+00, 1.7783e-03, 3.5566e-03, 5.3348e-03, 7.1131e-03,\n",
      "          8.8914e-03, 1.0670e-02, 1.2448e-02, 1.4226e-02, 1.6005e-02],\n",
      "         [0.0000e+00, 1.0000e-03, 2.0000e-03, 3.0000e-03, 4.0000e-03,\n",
      "          5.0000e-03, 6.0000e-03, 7.0000e-03, 8.0000e-03, 9.0000e-03],\n",
      "         [0.0000e+00, 5.6234e-04, 1.1247e-03, 1.6870e-03, 2.2494e-03,\n",
      "          2.8117e-03, 3.3740e-03, 3.9364e-03, 4.4987e-03, 5.0611e-03],\n",
      "         [0.0000e+00, 3.1623e-04, 6.3246e-04, 9.4868e-04, 1.2649e-03,\n",
      "          1.5811e-03, 1.8974e-03, 2.2136e-03, 2.5298e-03, 2.8461e-03],\n",
      "         [0.0000e+00, 1.7783e-04, 3.5566e-04, 5.3348e-04, 7.1131e-04,\n",
      "          8.8914e-04, 1.0670e-03, 1.2448e-03, 1.4226e-03, 1.6005e-03]],\n",
      "\n",
      "        [[0.0000e+00, 1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00,\n",
      "          5.0000e+00, 6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00],\n",
      "         [0.0000e+00, 5.6234e-01, 1.1247e+00, 1.6870e+00, 2.2494e+00,\n",
      "          2.8117e+00, 3.3740e+00, 3.9364e+00, 4.4987e+00, 5.0611e+00],\n",
      "         [0.0000e+00, 3.1623e-01, 6.3246e-01, 9.4868e-01, 1.2649e+00,\n",
      "          1.5811e+00, 1.8974e+00, 2.2136e+00, 2.5298e+00, 2.8460e+00],\n",
      "         [0.0000e+00, 1.7783e-01, 3.5566e-01, 5.3348e-01, 7.1131e-01,\n",
      "          8.8914e-01, 1.0670e+00, 1.2448e+00, 1.4226e+00, 1.6005e+00],\n",
      "         [0.0000e+00, 1.0000e-01, 2.0000e-01, 3.0000e-01, 4.0000e-01,\n",
      "          5.0000e-01, 6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01],\n",
      "         [0.0000e+00, 5.6234e-02, 1.1247e-01, 1.6870e-01, 2.2494e-01,\n",
      "          2.8117e-01, 3.3740e-01, 3.9364e-01, 4.4987e-01, 5.0611e-01],\n",
      "         [0.0000e+00, 3.1623e-02, 6.3246e-02, 9.4868e-02, 1.2649e-01,\n",
      "          1.5811e-01, 1.8974e-01, 2.2136e-01, 2.5298e-01, 2.8460e-01],\n",
      "         [0.0000e+00, 1.7783e-02, 3.5566e-02, 5.3348e-02, 7.1131e-02,\n",
      "          8.8914e-02, 1.0670e-01, 1.2448e-01, 1.4226e-01, 1.6005e-01],\n",
      "         [0.0000e+00, 1.0000e-02, 2.0000e-02, 3.0000e-02, 4.0000e-02,\n",
      "          5.0000e-02, 6.0000e-02, 7.0000e-02, 8.0000e-02, 9.0000e-02],\n",
      "         [0.0000e+00, 5.6234e-03, 1.1247e-02, 1.6870e-02, 2.2494e-02,\n",
      "          2.8117e-02, 3.3740e-02, 3.9364e-02, 4.4987e-02, 5.0611e-02],\n",
      "         [0.0000e+00, 3.1623e-03, 6.3246e-03, 9.4868e-03, 1.2649e-02,\n",
      "          1.5811e-02, 1.8974e-02, 2.2136e-02, 2.5298e-02, 2.8461e-02],\n",
      "         [0.0000e+00, 1.7783e-03, 3.5566e-03, 5.3348e-03, 7.1131e-03,\n",
      "          8.8914e-03, 1.0670e-02, 1.2448e-02, 1.4226e-02, 1.6005e-02],\n",
      "         [0.0000e+00, 1.0000e-03, 2.0000e-03, 3.0000e-03, 4.0000e-03,\n",
      "          5.0000e-03, 6.0000e-03, 7.0000e-03, 8.0000e-03, 9.0000e-03],\n",
      "         [0.0000e+00, 5.6234e-04, 1.1247e-03, 1.6870e-03, 2.2494e-03,\n",
      "          2.8117e-03, 3.3740e-03, 3.9364e-03, 4.4987e-03, 5.0611e-03],\n",
      "         [0.0000e+00, 3.1623e-04, 6.3246e-04, 9.4868e-04, 1.2649e-03,\n",
      "          1.5811e-03, 1.8974e-03, 2.2136e-03, 2.5298e-03, 2.8461e-03],\n",
      "         [0.0000e+00, 1.7783e-04, 3.5566e-04, 5.3348e-04, 7.1131e-04,\n",
      "          8.8914e-04, 1.0670e-03, 1.2448e-03, 1.4226e-03, 1.6005e-03]]])\n",
      "q torch.Size([2, 8, 10, 32]) cos torch.Size([2, 1, 10, 32])\n",
      "xxxx tensor([1.0000e+00, 5.6234e-01, 3.1623e-01, 1.7783e-01, 1.0000e-01, 5.6234e-02,\n",
      "        3.1623e-02, 1.7783e-02, 1.0000e-02, 5.6234e-03, 3.1623e-03, 1.7783e-03,\n",
      "        1.0000e-03, 5.6234e-04, 3.1623e-04, 1.7783e-04])\n",
      "yyyy torch.Size([1, 16, 1]) tensor([[[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]]])\n",
      "zzzzz tensor([[[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]],\n",
      "\n",
      "        [[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]]])\n",
      "position_ids_expanded torch.Size([2, 1, 10]) tensor([[[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]],\n",
      "\n",
      "        [[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]]])\n",
      "xxxxyyyuuu torch.Size([2, 16, 1]) torch.Size([2, 1, 10])\n",
      "tttt torch.Size([2, 16, 10]) tensor([[[0.0000e+00, 1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00,\n",
      "          5.0000e+00, 6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00],\n",
      "         [0.0000e+00, 5.6234e-01, 1.1247e+00, 1.6870e+00, 2.2494e+00,\n",
      "          2.8117e+00, 3.3740e+00, 3.9364e+00, 4.4987e+00, 5.0611e+00],\n",
      "         [0.0000e+00, 3.1623e-01, 6.3246e-01, 9.4868e-01, 1.2649e+00,\n",
      "          1.5811e+00, 1.8974e+00, 2.2136e+00, 2.5298e+00, 2.8460e+00],\n",
      "         [0.0000e+00, 1.7783e-01, 3.5566e-01, 5.3348e-01, 7.1131e-01,\n",
      "          8.8914e-01, 1.0670e+00, 1.2448e+00, 1.4226e+00, 1.6005e+00],\n",
      "         [0.0000e+00, 1.0000e-01, 2.0000e-01, 3.0000e-01, 4.0000e-01,\n",
      "          5.0000e-01, 6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01],\n",
      "         [0.0000e+00, 5.6234e-02, 1.1247e-01, 1.6870e-01, 2.2494e-01,\n",
      "          2.8117e-01, 3.3740e-01, 3.9364e-01, 4.4987e-01, 5.0611e-01],\n",
      "         [0.0000e+00, 3.1623e-02, 6.3246e-02, 9.4868e-02, 1.2649e-01,\n",
      "          1.5811e-01, 1.8974e-01, 2.2136e-01, 2.5298e-01, 2.8460e-01],\n",
      "         [0.0000e+00, 1.7783e-02, 3.5566e-02, 5.3348e-02, 7.1131e-02,\n",
      "          8.8914e-02, 1.0670e-01, 1.2448e-01, 1.4226e-01, 1.6005e-01],\n",
      "         [0.0000e+00, 1.0000e-02, 2.0000e-02, 3.0000e-02, 4.0000e-02,\n",
      "          5.0000e-02, 6.0000e-02, 7.0000e-02, 8.0000e-02, 9.0000e-02],\n",
      "         [0.0000e+00, 5.6234e-03, 1.1247e-02, 1.6870e-02, 2.2494e-02,\n",
      "          2.8117e-02, 3.3740e-02, 3.9364e-02, 4.4987e-02, 5.0611e-02],\n",
      "         [0.0000e+00, 3.1623e-03, 6.3246e-03, 9.4868e-03, 1.2649e-02,\n",
      "          1.5811e-02, 1.8974e-02, 2.2136e-02, 2.5298e-02, 2.8461e-02],\n",
      "         [0.0000e+00, 1.7783e-03, 3.5566e-03, 5.3348e-03, 7.1131e-03,\n",
      "          8.8914e-03, 1.0670e-02, 1.2448e-02, 1.4226e-02, 1.6005e-02],\n",
      "         [0.0000e+00, 1.0000e-03, 2.0000e-03, 3.0000e-03, 4.0000e-03,\n",
      "          5.0000e-03, 6.0000e-03, 7.0000e-03, 8.0000e-03, 9.0000e-03],\n",
      "         [0.0000e+00, 5.6234e-04, 1.1247e-03, 1.6870e-03, 2.2494e-03,\n",
      "          2.8117e-03, 3.3740e-03, 3.9364e-03, 4.4987e-03, 5.0611e-03],\n",
      "         [0.0000e+00, 3.1623e-04, 6.3246e-04, 9.4868e-04, 1.2649e-03,\n",
      "          1.5811e-03, 1.8974e-03, 2.2136e-03, 2.5298e-03, 2.8461e-03],\n",
      "         [0.0000e+00, 1.7783e-04, 3.5566e-04, 5.3348e-04, 7.1131e-04,\n",
      "          8.8914e-04, 1.0670e-03, 1.2448e-03, 1.4226e-03, 1.6005e-03]],\n",
      "\n",
      "        [[0.0000e+00, 1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00,\n",
      "          5.0000e+00, 6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00],\n",
      "         [0.0000e+00, 5.6234e-01, 1.1247e+00, 1.6870e+00, 2.2494e+00,\n",
      "          2.8117e+00, 3.3740e+00, 3.9364e+00, 4.4987e+00, 5.0611e+00],\n",
      "         [0.0000e+00, 3.1623e-01, 6.3246e-01, 9.4868e-01, 1.2649e+00,\n",
      "          1.5811e+00, 1.8974e+00, 2.2136e+00, 2.5298e+00, 2.8460e+00],\n",
      "         [0.0000e+00, 1.7783e-01, 3.5566e-01, 5.3348e-01, 7.1131e-01,\n",
      "          8.8914e-01, 1.0670e+00, 1.2448e+00, 1.4226e+00, 1.6005e+00],\n",
      "         [0.0000e+00, 1.0000e-01, 2.0000e-01, 3.0000e-01, 4.0000e-01,\n",
      "          5.0000e-01, 6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01],\n",
      "         [0.0000e+00, 5.6234e-02, 1.1247e-01, 1.6870e-01, 2.2494e-01,\n",
      "          2.8117e-01, 3.3740e-01, 3.9364e-01, 4.4987e-01, 5.0611e-01],\n",
      "         [0.0000e+00, 3.1623e-02, 6.3246e-02, 9.4868e-02, 1.2649e-01,\n",
      "          1.5811e-01, 1.8974e-01, 2.2136e-01, 2.5298e-01, 2.8460e-01],\n",
      "         [0.0000e+00, 1.7783e-02, 3.5566e-02, 5.3348e-02, 7.1131e-02,\n",
      "          8.8914e-02, 1.0670e-01, 1.2448e-01, 1.4226e-01, 1.6005e-01],\n",
      "         [0.0000e+00, 1.0000e-02, 2.0000e-02, 3.0000e-02, 4.0000e-02,\n",
      "          5.0000e-02, 6.0000e-02, 7.0000e-02, 8.0000e-02, 9.0000e-02],\n",
      "         [0.0000e+00, 5.6234e-03, 1.1247e-02, 1.6870e-02, 2.2494e-02,\n",
      "          2.8117e-02, 3.3740e-02, 3.9364e-02, 4.4987e-02, 5.0611e-02],\n",
      "         [0.0000e+00, 3.1623e-03, 6.3246e-03, 9.4868e-03, 1.2649e-02,\n",
      "          1.5811e-02, 1.8974e-02, 2.2136e-02, 2.5298e-02, 2.8461e-02],\n",
      "         [0.0000e+00, 1.7783e-03, 3.5566e-03, 5.3348e-03, 7.1131e-03,\n",
      "          8.8914e-03, 1.0670e-02, 1.2448e-02, 1.4226e-02, 1.6005e-02],\n",
      "         [0.0000e+00, 1.0000e-03, 2.0000e-03, 3.0000e-03, 4.0000e-03,\n",
      "          5.0000e-03, 6.0000e-03, 7.0000e-03, 8.0000e-03, 9.0000e-03],\n",
      "         [0.0000e+00, 5.6234e-04, 1.1247e-03, 1.6870e-03, 2.2494e-03,\n",
      "          2.8117e-03, 3.3740e-03, 3.9364e-03, 4.4987e-03, 5.0611e-03],\n",
      "         [0.0000e+00, 3.1623e-04, 6.3246e-04, 9.4868e-04, 1.2649e-03,\n",
      "          1.5811e-03, 1.8974e-03, 2.2136e-03, 2.5298e-03, 2.8461e-03],\n",
      "         [0.0000e+00, 1.7783e-04, 3.5566e-04, 5.3348e-04, 7.1131e-04,\n",
      "          8.8914e-04, 1.0670e-03, 1.2448e-03, 1.4226e-03, 1.6005e-03]]])\n",
      "q torch.Size([2, 8, 10, 32]) cos torch.Size([2, 1, 10, 32])\n",
      "xxxx tensor([1.0000e+00, 5.6234e-01, 3.1623e-01, 1.7783e-01, 1.0000e-01, 5.6234e-02,\n",
      "        3.1623e-02, 1.7783e-02, 1.0000e-02, 5.6234e-03, 3.1623e-03, 1.7783e-03,\n",
      "        1.0000e-03, 5.6234e-04, 3.1623e-04, 1.7783e-04])\n",
      "yyyy torch.Size([1, 16, 1]) tensor([[[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]]])\n",
      "zzzzz tensor([[[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]],\n",
      "\n",
      "        [[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]]])\n",
      "position_ids_expanded torch.Size([2, 1, 10]) tensor([[[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]],\n",
      "\n",
      "        [[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]]])\n",
      "xxxxyyyuuu torch.Size([2, 16, 1]) torch.Size([2, 1, 10])\n",
      "tttt torch.Size([2, 16, 10]) tensor([[[0.0000e+00, 1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00,\n",
      "          5.0000e+00, 6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00],\n",
      "         [0.0000e+00, 5.6234e-01, 1.1247e+00, 1.6870e+00, 2.2494e+00,\n",
      "          2.8117e+00, 3.3740e+00, 3.9364e+00, 4.4987e+00, 5.0611e+00],\n",
      "         [0.0000e+00, 3.1623e-01, 6.3246e-01, 9.4868e-01, 1.2649e+00,\n",
      "          1.5811e+00, 1.8974e+00, 2.2136e+00, 2.5298e+00, 2.8460e+00],\n",
      "         [0.0000e+00, 1.7783e-01, 3.5566e-01, 5.3348e-01, 7.1131e-01,\n",
      "          8.8914e-01, 1.0670e+00, 1.2448e+00, 1.4226e+00, 1.6005e+00],\n",
      "         [0.0000e+00, 1.0000e-01, 2.0000e-01, 3.0000e-01, 4.0000e-01,\n",
      "          5.0000e-01, 6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01],\n",
      "         [0.0000e+00, 5.6234e-02, 1.1247e-01, 1.6870e-01, 2.2494e-01,\n",
      "          2.8117e-01, 3.3740e-01, 3.9364e-01, 4.4987e-01, 5.0611e-01],\n",
      "         [0.0000e+00, 3.1623e-02, 6.3246e-02, 9.4868e-02, 1.2649e-01,\n",
      "          1.5811e-01, 1.8974e-01, 2.2136e-01, 2.5298e-01, 2.8460e-01],\n",
      "         [0.0000e+00, 1.7783e-02, 3.5566e-02, 5.3348e-02, 7.1131e-02,\n",
      "          8.8914e-02, 1.0670e-01, 1.2448e-01, 1.4226e-01, 1.6005e-01],\n",
      "         [0.0000e+00, 1.0000e-02, 2.0000e-02, 3.0000e-02, 4.0000e-02,\n",
      "          5.0000e-02, 6.0000e-02, 7.0000e-02, 8.0000e-02, 9.0000e-02],\n",
      "         [0.0000e+00, 5.6234e-03, 1.1247e-02, 1.6870e-02, 2.2494e-02,\n",
      "          2.8117e-02, 3.3740e-02, 3.9364e-02, 4.4987e-02, 5.0611e-02],\n",
      "         [0.0000e+00, 3.1623e-03, 6.3246e-03, 9.4868e-03, 1.2649e-02,\n",
      "          1.5811e-02, 1.8974e-02, 2.2136e-02, 2.5298e-02, 2.8461e-02],\n",
      "         [0.0000e+00, 1.7783e-03, 3.5566e-03, 5.3348e-03, 7.1131e-03,\n",
      "          8.8914e-03, 1.0670e-02, 1.2448e-02, 1.4226e-02, 1.6005e-02],\n",
      "         [0.0000e+00, 1.0000e-03, 2.0000e-03, 3.0000e-03, 4.0000e-03,\n",
      "          5.0000e-03, 6.0000e-03, 7.0000e-03, 8.0000e-03, 9.0000e-03],\n",
      "         [0.0000e+00, 5.6234e-04, 1.1247e-03, 1.6870e-03, 2.2494e-03,\n",
      "          2.8117e-03, 3.3740e-03, 3.9364e-03, 4.4987e-03, 5.0611e-03],\n",
      "         [0.0000e+00, 3.1623e-04, 6.3246e-04, 9.4868e-04, 1.2649e-03,\n",
      "          1.5811e-03, 1.8974e-03, 2.2136e-03, 2.5298e-03, 2.8461e-03],\n",
      "         [0.0000e+00, 1.7783e-04, 3.5566e-04, 5.3348e-04, 7.1131e-04,\n",
      "          8.8914e-04, 1.0670e-03, 1.2448e-03, 1.4226e-03, 1.6005e-03]],\n",
      "\n",
      "        [[0.0000e+00, 1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00,\n",
      "          5.0000e+00, 6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00],\n",
      "         [0.0000e+00, 5.6234e-01, 1.1247e+00, 1.6870e+00, 2.2494e+00,\n",
      "          2.8117e+00, 3.3740e+00, 3.9364e+00, 4.4987e+00, 5.0611e+00],\n",
      "         [0.0000e+00, 3.1623e-01, 6.3246e-01, 9.4868e-01, 1.2649e+00,\n",
      "          1.5811e+00, 1.8974e+00, 2.2136e+00, 2.5298e+00, 2.8460e+00],\n",
      "         [0.0000e+00, 1.7783e-01, 3.5566e-01, 5.3348e-01, 7.1131e-01,\n",
      "          8.8914e-01, 1.0670e+00, 1.2448e+00, 1.4226e+00, 1.6005e+00],\n",
      "         [0.0000e+00, 1.0000e-01, 2.0000e-01, 3.0000e-01, 4.0000e-01,\n",
      "          5.0000e-01, 6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01],\n",
      "         [0.0000e+00, 5.6234e-02, 1.1247e-01, 1.6870e-01, 2.2494e-01,\n",
      "          2.8117e-01, 3.3740e-01, 3.9364e-01, 4.4987e-01, 5.0611e-01],\n",
      "         [0.0000e+00, 3.1623e-02, 6.3246e-02, 9.4868e-02, 1.2649e-01,\n",
      "          1.5811e-01, 1.8974e-01, 2.2136e-01, 2.5298e-01, 2.8460e-01],\n",
      "         [0.0000e+00, 1.7783e-02, 3.5566e-02, 5.3348e-02, 7.1131e-02,\n",
      "          8.8914e-02, 1.0670e-01, 1.2448e-01, 1.4226e-01, 1.6005e-01],\n",
      "         [0.0000e+00, 1.0000e-02, 2.0000e-02, 3.0000e-02, 4.0000e-02,\n",
      "          5.0000e-02, 6.0000e-02, 7.0000e-02, 8.0000e-02, 9.0000e-02],\n",
      "         [0.0000e+00, 5.6234e-03, 1.1247e-02, 1.6870e-02, 2.2494e-02,\n",
      "          2.8117e-02, 3.3740e-02, 3.9364e-02, 4.4987e-02, 5.0611e-02],\n",
      "         [0.0000e+00, 3.1623e-03, 6.3246e-03, 9.4868e-03, 1.2649e-02,\n",
      "          1.5811e-02, 1.8974e-02, 2.2136e-02, 2.5298e-02, 2.8461e-02],\n",
      "         [0.0000e+00, 1.7783e-03, 3.5566e-03, 5.3348e-03, 7.1131e-03,\n",
      "          8.8914e-03, 1.0670e-02, 1.2448e-02, 1.4226e-02, 1.6005e-02],\n",
      "         [0.0000e+00, 1.0000e-03, 2.0000e-03, 3.0000e-03, 4.0000e-03,\n",
      "          5.0000e-03, 6.0000e-03, 7.0000e-03, 8.0000e-03, 9.0000e-03],\n",
      "         [0.0000e+00, 5.6234e-04, 1.1247e-03, 1.6870e-03, 2.2494e-03,\n",
      "          2.8117e-03, 3.3740e-03, 3.9364e-03, 4.4987e-03, 5.0611e-03],\n",
      "         [0.0000e+00, 3.1623e-04, 6.3246e-04, 9.4868e-04, 1.2649e-03,\n",
      "          1.5811e-03, 1.8974e-03, 2.2136e-03, 2.5298e-03, 2.8461e-03],\n",
      "         [0.0000e+00, 1.7783e-04, 3.5566e-04, 5.3348e-04, 7.1131e-04,\n",
      "          8.8914e-04, 1.0670e-03, 1.2448e-03, 1.4226e-03, 1.6005e-03]]])\n",
      "q torch.Size([2, 8, 10, 32]) cos torch.Size([2, 1, 10, 32])\n",
      "xxxx tensor([1.0000e+00, 5.6234e-01, 3.1623e-01, 1.7783e-01, 1.0000e-01, 5.6234e-02,\n",
      "        3.1623e-02, 1.7783e-02, 1.0000e-02, 5.6234e-03, 3.1623e-03, 1.7783e-03,\n",
      "        1.0000e-03, 5.6234e-04, 3.1623e-04, 1.7783e-04])\n",
      "yyyy torch.Size([1, 16, 1]) tensor([[[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]]])\n",
      "zzzzz tensor([[[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]],\n",
      "\n",
      "        [[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]]])\n",
      "position_ids_expanded torch.Size([2, 1, 10]) tensor([[[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]],\n",
      "\n",
      "        [[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]]])\n",
      "xxxxyyyuuu torch.Size([2, 16, 1]) torch.Size([2, 1, 10])\n",
      "tttt torch.Size([2, 16, 10]) tensor([[[0.0000e+00, 1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00,\n",
      "          5.0000e+00, 6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00],\n",
      "         [0.0000e+00, 5.6234e-01, 1.1247e+00, 1.6870e+00, 2.2494e+00,\n",
      "          2.8117e+00, 3.3740e+00, 3.9364e+00, 4.4987e+00, 5.0611e+00],\n",
      "         [0.0000e+00, 3.1623e-01, 6.3246e-01, 9.4868e-01, 1.2649e+00,\n",
      "          1.5811e+00, 1.8974e+00, 2.2136e+00, 2.5298e+00, 2.8460e+00],\n",
      "         [0.0000e+00, 1.7783e-01, 3.5566e-01, 5.3348e-01, 7.1131e-01,\n",
      "          8.8914e-01, 1.0670e+00, 1.2448e+00, 1.4226e+00, 1.6005e+00],\n",
      "         [0.0000e+00, 1.0000e-01, 2.0000e-01, 3.0000e-01, 4.0000e-01,\n",
      "          5.0000e-01, 6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01],\n",
      "         [0.0000e+00, 5.6234e-02, 1.1247e-01, 1.6870e-01, 2.2494e-01,\n",
      "          2.8117e-01, 3.3740e-01, 3.9364e-01, 4.4987e-01, 5.0611e-01],\n",
      "         [0.0000e+00, 3.1623e-02, 6.3246e-02, 9.4868e-02, 1.2649e-01,\n",
      "          1.5811e-01, 1.8974e-01, 2.2136e-01, 2.5298e-01, 2.8460e-01],\n",
      "         [0.0000e+00, 1.7783e-02, 3.5566e-02, 5.3348e-02, 7.1131e-02,\n",
      "          8.8914e-02, 1.0670e-01, 1.2448e-01, 1.4226e-01, 1.6005e-01],\n",
      "         [0.0000e+00, 1.0000e-02, 2.0000e-02, 3.0000e-02, 4.0000e-02,\n",
      "          5.0000e-02, 6.0000e-02, 7.0000e-02, 8.0000e-02, 9.0000e-02],\n",
      "         [0.0000e+00, 5.6234e-03, 1.1247e-02, 1.6870e-02, 2.2494e-02,\n",
      "          2.8117e-02, 3.3740e-02, 3.9364e-02, 4.4987e-02, 5.0611e-02],\n",
      "         [0.0000e+00, 3.1623e-03, 6.3246e-03, 9.4868e-03, 1.2649e-02,\n",
      "          1.5811e-02, 1.8974e-02, 2.2136e-02, 2.5298e-02, 2.8461e-02],\n",
      "         [0.0000e+00, 1.7783e-03, 3.5566e-03, 5.3348e-03, 7.1131e-03,\n",
      "          8.8914e-03, 1.0670e-02, 1.2448e-02, 1.4226e-02, 1.6005e-02],\n",
      "         [0.0000e+00, 1.0000e-03, 2.0000e-03, 3.0000e-03, 4.0000e-03,\n",
      "          5.0000e-03, 6.0000e-03, 7.0000e-03, 8.0000e-03, 9.0000e-03],\n",
      "         [0.0000e+00, 5.6234e-04, 1.1247e-03, 1.6870e-03, 2.2494e-03,\n",
      "          2.8117e-03, 3.3740e-03, 3.9364e-03, 4.4987e-03, 5.0611e-03],\n",
      "         [0.0000e+00, 3.1623e-04, 6.3246e-04, 9.4868e-04, 1.2649e-03,\n",
      "          1.5811e-03, 1.8974e-03, 2.2136e-03, 2.5298e-03, 2.8461e-03],\n",
      "         [0.0000e+00, 1.7783e-04, 3.5566e-04, 5.3348e-04, 7.1131e-04,\n",
      "          8.8914e-04, 1.0670e-03, 1.2448e-03, 1.4226e-03, 1.6005e-03]],\n",
      "\n",
      "        [[0.0000e+00, 1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00,\n",
      "          5.0000e+00, 6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00],\n",
      "         [0.0000e+00, 5.6234e-01, 1.1247e+00, 1.6870e+00, 2.2494e+00,\n",
      "          2.8117e+00, 3.3740e+00, 3.9364e+00, 4.4987e+00, 5.0611e+00],\n",
      "         [0.0000e+00, 3.1623e-01, 6.3246e-01, 9.4868e-01, 1.2649e+00,\n",
      "          1.5811e+00, 1.8974e+00, 2.2136e+00, 2.5298e+00, 2.8460e+00],\n",
      "         [0.0000e+00, 1.7783e-01, 3.5566e-01, 5.3348e-01, 7.1131e-01,\n",
      "          8.8914e-01, 1.0670e+00, 1.2448e+00, 1.4226e+00, 1.6005e+00],\n",
      "         [0.0000e+00, 1.0000e-01, 2.0000e-01, 3.0000e-01, 4.0000e-01,\n",
      "          5.0000e-01, 6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01],\n",
      "         [0.0000e+00, 5.6234e-02, 1.1247e-01, 1.6870e-01, 2.2494e-01,\n",
      "          2.8117e-01, 3.3740e-01, 3.9364e-01, 4.4987e-01, 5.0611e-01],\n",
      "         [0.0000e+00, 3.1623e-02, 6.3246e-02, 9.4868e-02, 1.2649e-01,\n",
      "          1.5811e-01, 1.8974e-01, 2.2136e-01, 2.5298e-01, 2.8460e-01],\n",
      "         [0.0000e+00, 1.7783e-02, 3.5566e-02, 5.3348e-02, 7.1131e-02,\n",
      "          8.8914e-02, 1.0670e-01, 1.2448e-01, 1.4226e-01, 1.6005e-01],\n",
      "         [0.0000e+00, 1.0000e-02, 2.0000e-02, 3.0000e-02, 4.0000e-02,\n",
      "          5.0000e-02, 6.0000e-02, 7.0000e-02, 8.0000e-02, 9.0000e-02],\n",
      "         [0.0000e+00, 5.6234e-03, 1.1247e-02, 1.6870e-02, 2.2494e-02,\n",
      "          2.8117e-02, 3.3740e-02, 3.9364e-02, 4.4987e-02, 5.0611e-02],\n",
      "         [0.0000e+00, 3.1623e-03, 6.3246e-03, 9.4868e-03, 1.2649e-02,\n",
      "          1.5811e-02, 1.8974e-02, 2.2136e-02, 2.5298e-02, 2.8461e-02],\n",
      "         [0.0000e+00, 1.7783e-03, 3.5566e-03, 5.3348e-03, 7.1131e-03,\n",
      "          8.8914e-03, 1.0670e-02, 1.2448e-02, 1.4226e-02, 1.6005e-02],\n",
      "         [0.0000e+00, 1.0000e-03, 2.0000e-03, 3.0000e-03, 4.0000e-03,\n",
      "          5.0000e-03, 6.0000e-03, 7.0000e-03, 8.0000e-03, 9.0000e-03],\n",
      "         [0.0000e+00, 5.6234e-04, 1.1247e-03, 1.6870e-03, 2.2494e-03,\n",
      "          2.8117e-03, 3.3740e-03, 3.9364e-03, 4.4987e-03, 5.0611e-03],\n",
      "         [0.0000e+00, 3.1623e-04, 6.3246e-04, 9.4868e-04, 1.2649e-03,\n",
      "          1.5811e-03, 1.8974e-03, 2.2136e-03, 2.5298e-03, 2.8461e-03],\n",
      "         [0.0000e+00, 1.7783e-04, 3.5566e-04, 5.3348e-04, 7.1131e-04,\n",
      "          8.8914e-04, 1.0670e-03, 1.2448e-03, 1.4226e-03, 1.6005e-03]]])\n",
      "q torch.Size([2, 8, 10, 32]) cos torch.Size([2, 1, 10, 32])\n",
      "xxxx tensor([1.0000e+00, 5.6234e-01, 3.1623e-01, 1.7783e-01, 1.0000e-01, 5.6234e-02,\n",
      "        3.1623e-02, 1.7783e-02, 1.0000e-02, 5.6234e-03, 3.1623e-03, 1.7783e-03,\n",
      "        1.0000e-03, 5.6234e-04, 3.1623e-04, 1.7783e-04])\n",
      "yyyy torch.Size([1, 16, 1]) tensor([[[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]]])\n",
      "zzzzz tensor([[[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]],\n",
      "\n",
      "        [[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]]])\n",
      "position_ids_expanded torch.Size([2, 1, 10]) tensor([[[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]],\n",
      "\n",
      "        [[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]]])\n",
      "xxxxyyyuuu torch.Size([2, 16, 1]) torch.Size([2, 1, 10])\n",
      "tttt torch.Size([2, 16, 10]) tensor([[[0.0000e+00, 1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00,\n",
      "          5.0000e+00, 6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00],\n",
      "         [0.0000e+00, 5.6234e-01, 1.1247e+00, 1.6870e+00, 2.2494e+00,\n",
      "          2.8117e+00, 3.3740e+00, 3.9364e+00, 4.4987e+00, 5.0611e+00],\n",
      "         [0.0000e+00, 3.1623e-01, 6.3246e-01, 9.4868e-01, 1.2649e+00,\n",
      "          1.5811e+00, 1.8974e+00, 2.2136e+00, 2.5298e+00, 2.8460e+00],\n",
      "         [0.0000e+00, 1.7783e-01, 3.5566e-01, 5.3348e-01, 7.1131e-01,\n",
      "          8.8914e-01, 1.0670e+00, 1.2448e+00, 1.4226e+00, 1.6005e+00],\n",
      "         [0.0000e+00, 1.0000e-01, 2.0000e-01, 3.0000e-01, 4.0000e-01,\n",
      "          5.0000e-01, 6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01],\n",
      "         [0.0000e+00, 5.6234e-02, 1.1247e-01, 1.6870e-01, 2.2494e-01,\n",
      "          2.8117e-01, 3.3740e-01, 3.9364e-01, 4.4987e-01, 5.0611e-01],\n",
      "         [0.0000e+00, 3.1623e-02, 6.3246e-02, 9.4868e-02, 1.2649e-01,\n",
      "          1.5811e-01, 1.8974e-01, 2.2136e-01, 2.5298e-01, 2.8460e-01],\n",
      "         [0.0000e+00, 1.7783e-02, 3.5566e-02, 5.3348e-02, 7.1131e-02,\n",
      "          8.8914e-02, 1.0670e-01, 1.2448e-01, 1.4226e-01, 1.6005e-01],\n",
      "         [0.0000e+00, 1.0000e-02, 2.0000e-02, 3.0000e-02, 4.0000e-02,\n",
      "          5.0000e-02, 6.0000e-02, 7.0000e-02, 8.0000e-02, 9.0000e-02],\n",
      "         [0.0000e+00, 5.6234e-03, 1.1247e-02, 1.6870e-02, 2.2494e-02,\n",
      "          2.8117e-02, 3.3740e-02, 3.9364e-02, 4.4987e-02, 5.0611e-02],\n",
      "         [0.0000e+00, 3.1623e-03, 6.3246e-03, 9.4868e-03, 1.2649e-02,\n",
      "          1.5811e-02, 1.8974e-02, 2.2136e-02, 2.5298e-02, 2.8461e-02],\n",
      "         [0.0000e+00, 1.7783e-03, 3.5566e-03, 5.3348e-03, 7.1131e-03,\n",
      "          8.8914e-03, 1.0670e-02, 1.2448e-02, 1.4226e-02, 1.6005e-02],\n",
      "         [0.0000e+00, 1.0000e-03, 2.0000e-03, 3.0000e-03, 4.0000e-03,\n",
      "          5.0000e-03, 6.0000e-03, 7.0000e-03, 8.0000e-03, 9.0000e-03],\n",
      "         [0.0000e+00, 5.6234e-04, 1.1247e-03, 1.6870e-03, 2.2494e-03,\n",
      "          2.8117e-03, 3.3740e-03, 3.9364e-03, 4.4987e-03, 5.0611e-03],\n",
      "         [0.0000e+00, 3.1623e-04, 6.3246e-04, 9.4868e-04, 1.2649e-03,\n",
      "          1.5811e-03, 1.8974e-03, 2.2136e-03, 2.5298e-03, 2.8461e-03],\n",
      "         [0.0000e+00, 1.7783e-04, 3.5566e-04, 5.3348e-04, 7.1131e-04,\n",
      "          8.8914e-04, 1.0670e-03, 1.2448e-03, 1.4226e-03, 1.6005e-03]],\n",
      "\n",
      "        [[0.0000e+00, 1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00,\n",
      "          5.0000e+00, 6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00],\n",
      "         [0.0000e+00, 5.6234e-01, 1.1247e+00, 1.6870e+00, 2.2494e+00,\n",
      "          2.8117e+00, 3.3740e+00, 3.9364e+00, 4.4987e+00, 5.0611e+00],\n",
      "         [0.0000e+00, 3.1623e-01, 6.3246e-01, 9.4868e-01, 1.2649e+00,\n",
      "          1.5811e+00, 1.8974e+00, 2.2136e+00, 2.5298e+00, 2.8460e+00],\n",
      "         [0.0000e+00, 1.7783e-01, 3.5566e-01, 5.3348e-01, 7.1131e-01,\n",
      "          8.8914e-01, 1.0670e+00, 1.2448e+00, 1.4226e+00, 1.6005e+00],\n",
      "         [0.0000e+00, 1.0000e-01, 2.0000e-01, 3.0000e-01, 4.0000e-01,\n",
      "          5.0000e-01, 6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01],\n",
      "         [0.0000e+00, 5.6234e-02, 1.1247e-01, 1.6870e-01, 2.2494e-01,\n",
      "          2.8117e-01, 3.3740e-01, 3.9364e-01, 4.4987e-01, 5.0611e-01],\n",
      "         [0.0000e+00, 3.1623e-02, 6.3246e-02, 9.4868e-02, 1.2649e-01,\n",
      "          1.5811e-01, 1.8974e-01, 2.2136e-01, 2.5298e-01, 2.8460e-01],\n",
      "         [0.0000e+00, 1.7783e-02, 3.5566e-02, 5.3348e-02, 7.1131e-02,\n",
      "          8.8914e-02, 1.0670e-01, 1.2448e-01, 1.4226e-01, 1.6005e-01],\n",
      "         [0.0000e+00, 1.0000e-02, 2.0000e-02, 3.0000e-02, 4.0000e-02,\n",
      "          5.0000e-02, 6.0000e-02, 7.0000e-02, 8.0000e-02, 9.0000e-02],\n",
      "         [0.0000e+00, 5.6234e-03, 1.1247e-02, 1.6870e-02, 2.2494e-02,\n",
      "          2.8117e-02, 3.3740e-02, 3.9364e-02, 4.4987e-02, 5.0611e-02],\n",
      "         [0.0000e+00, 3.1623e-03, 6.3246e-03, 9.4868e-03, 1.2649e-02,\n",
      "          1.5811e-02, 1.8974e-02, 2.2136e-02, 2.5298e-02, 2.8461e-02],\n",
      "         [0.0000e+00, 1.7783e-03, 3.5566e-03, 5.3348e-03, 7.1131e-03,\n",
      "          8.8914e-03, 1.0670e-02, 1.2448e-02, 1.4226e-02, 1.6005e-02],\n",
      "         [0.0000e+00, 1.0000e-03, 2.0000e-03, 3.0000e-03, 4.0000e-03,\n",
      "          5.0000e-03, 6.0000e-03, 7.0000e-03, 8.0000e-03, 9.0000e-03],\n",
      "         [0.0000e+00, 5.6234e-04, 1.1247e-03, 1.6870e-03, 2.2494e-03,\n",
      "          2.8117e-03, 3.3740e-03, 3.9364e-03, 4.4987e-03, 5.0611e-03],\n",
      "         [0.0000e+00, 3.1623e-04, 6.3246e-04, 9.4868e-04, 1.2649e-03,\n",
      "          1.5811e-03, 1.8974e-03, 2.2136e-03, 2.5298e-03, 2.8461e-03],\n",
      "         [0.0000e+00, 1.7783e-04, 3.5566e-04, 5.3348e-04, 7.1131e-04,\n",
      "          8.8914e-04, 1.0670e-03, 1.2448e-03, 1.4226e-03, 1.6005e-03]]])\n",
      "q torch.Size([2, 8, 10, 32]) cos torch.Size([2, 1, 10, 32])\n",
      "Logits Shape: torch.Size([2, 10, 10000])\n",
      "Logits Example: tensor([[[ 0.0402,  0.5073, -0.0091,  ..., -0.1201, -0.1881, -0.9839],\n",
      "         [-0.0757, -0.3546,  0.0485,  ..., -0.1291,  0.0396,  0.2593],\n",
      "         [-0.1247, -1.0304,  0.2621,  ..., -0.9324, -0.3075,  0.7636],\n",
      "         ...,\n",
      "         [ 0.2738, -0.7528,  0.0149,  ...,  1.3034,  0.1124,  0.0385],\n",
      "         [-0.8829,  0.3995,  0.5442,  ...,  0.5162,  0.1932,  0.2671],\n",
      "         [ 0.3288, -0.3701, -0.9524,  ..., -0.9292, -0.7014, -0.3161]],\n",
      "\n",
      "        [[ 0.4380,  0.2175, -0.7718,  ..., -0.3604,  0.5078,  0.5257],\n",
      "         [ 0.2718,  0.4158, -0.0076,  ..., -0.6966, -0.5476, -0.8451],\n",
      "         [-0.8873, -0.2133,  0.5715,  ...,  0.2689,  0.9288, -0.6752],\n",
      "         ...,\n",
      "         [-0.5380,  0.3584,  0.3147,  ...,  0.1056, -0.2171, -0.1984],\n",
      "         [ 0.4230, -0.0525,  0.1496,  ..., -0.5189,  0.4234, -0.0646],\n",
      "         [-0.6366, -0.5036,  0.3482,  ..., -0.5450,  1.0759, -0.1288]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "\n",
    "class GemmaConfig:\n",
    "    def __init__(self):\n",
    "        self.hidden_size = 512  # 隐藏层大小\n",
    "        self.intermediate_size = 512\n",
    "        self.num_heads = 8   # 注意力头数\n",
    "        self.head_dim=32\n",
    "        self.num_key_value_heads = 4  # Key-Value头数\n",
    "        self.attention_dropout = 0.1  # Dropout率\n",
    "        self.max_position_embeddings = 1024  # 最大位置嵌入数\n",
    "        self.rope_theta = 10000  # 旋转位置编码的基数\n",
    "        self.attention_bias = False  # 是否使用偏置\n",
    "        self.rms_norm_eps = 0.01\n",
    "        self.intermediate_size=512  # 前馈网络的大小\n",
    "        self.num_hidden_layers=6 # 解码器层的数量\n",
    "        self.vocab_size=10000 # 假设词汇表大小\n",
    "        self.pad_token_id=0  # 填充 token 的 id\n",
    "    \n",
    "\n",
    "config = GemmaConfig()\n",
    "\n",
    "model = GemmaForCausalLM(config)\n",
    "\n",
    "# Example input tensors\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "\n",
    "# Simulating token IDs (for input embeddings)\n",
    "input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))\n",
    "\n",
    "# Generate attention mask and position IDs\n",
    "attention_mask = torch.ones(batch_size, 1, seq_len, seq_len)  # No padding mask\n",
    "position_ids = torch.arange(seq_len).unsqueeze(0).repeat(batch_size, 1)\n",
    "\n",
    "# Pass inputs through the model\n",
    "outputs = model(\n",
    "    attention_mask=attention_mask,\n",
    "    position_ids=position_ids,\n",
    "    inputs_embeds=model.get_input_embeddings()(input_ids),  # Getting the input embeddings from the model\n",
    ")\n",
    "\n",
    "# Output\n",
    "print(\"Logits Shape:\", outputs[\"logits\"].shape)  # This will have shape (batch_size, seq_len, vocab_size)\n",
    "print(\"Logits Example:\", outputs[\"logits\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14391f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b292c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78de220d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d7ca153",
   "metadata": {},
   "outputs": [],
   "source": [
    " class PaliGemmaMultiModalProjector(nn.Module):\n",
    "        \n",
    "        def __init__(self,config:PaliGemmaConfig):\n",
    "            super().__init__()\n",
    "            self.linear = nn.Linear(config.vision_config.hidden_size,\n",
    "                                   config.vision_config.projection_dim, \n",
    "                                    bias=True)\n",
    "        \n",
    "        def forward(self,image_features):\n",
    "            hidden_states = self.linear(image_features)\n",
    "            return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e2149484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projected Features Shape: torch.Size([2, 10, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple config-like class for this example\n",
    "class VisionConfig:\n",
    "    def __init__(self):\n",
    "        self.hidden_size = 512  # Size of the input image features\n",
    "        self.projection_dim = 256  # The target projection dimension\n",
    "\n",
    "class PaliGemmaConfig:\n",
    "    def __init__(self):\n",
    "        self.vision_config = VisionConfig()  # Include the vision config\n",
    "\n",
    "# Create the projector model\n",
    "config = PaliGemmaConfig()\n",
    "model = PaliGemmaMultiModalProjector(config)\n",
    "\n",
    "# Simulate image features (e.g., from a Vision Transformer output)\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "image_features = torch.randn(batch_size, seq_len, config.vision_config.hidden_size)\n",
    "\n",
    "# Forward pass through the model\n",
    "projected_features = model(image_features)\n",
    "\n",
    "# Output\n",
    "print(\"Projected Features Shape:\", projected_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b83e42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e368366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8edc1768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision_transformer_1 import VisionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "56e1283f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaliGemmaForConditionalGeneration(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self,config: PaliGemmaConfig):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.vision_tower = VisionModel(config.vision_config)\n",
    "        self.multi_modal_projector = PaliGemmaMultiModalProjector(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        \n",
    "        self.language_model = GemmaForCausalLM(config.text_config)\n",
    "        self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def tie_weights(self):\n",
    "        return self.language_model.tie_weights()\n",
    "    \n",
    "    \n",
    "    def _merge_input_ids_with_image_features(\n",
    "       self,image_features: torch.Tensor,\n",
    "        inputs_embeds: torch.Tensor,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        kv_cache: Optional[KVCache]=None\n",
    "    ):\n",
    "        _,_,embed_dim = image_features.shape\n",
    "        batch_size,sequence_length = input_ids.shape\n",
    "        dtype,device = inputs_embeds.dtype, inputs_embeds.device\n",
    "        \n",
    "        \n",
    "        scaled_image_features = image_features / (self.config.hidden_size**0.5)\n",
    "        \n",
    "        final_embedding = torch.zeros(batch_size,sequence_length,\n",
    "                                     embed_dim,dtype=inputs_embeds.dtype,\n",
    "                                     device=inputs_embeds.device)\n",
    "        \n",
    "        \n",
    "        text_mask = (input_ids != self.config.image_token_index) & (input_ids != self.pad_token_id)\n",
    "        \n",
    "        \n",
    "        image_mask = input_ids == self.config.image_token_index\n",
    "        \n",
    "        \n",
    "        pad_mask = input_ids == self.pad_token_id\n",
    "        \n",
    "        text_mask_expanded = text_mask.unsqueeze(-1).expand(-1,-1,embed_dim)\n",
    "        \n",
    "        pad_mask_expanded = pad_mask.unsqueeze(-1).expand(-1,-1,embed_dim)\n",
    "        \n",
    "        image_mask_expanded = image_mask.unsqueeze(-1).expand(-1,-1,embed_dim)\n",
    "        \n",
    "        \n",
    "        final_embedding = torch.where(text_mask_expanded,inputs_embeds,\n",
    "                                     final_embedding)\n",
    "        \n",
    "        final_embedding = final_embedding.masked_scatter(\n",
    "             image_mask_expanded,scaled_image_features\n",
    "        )\n",
    "        \n",
    "        final_embedding = torch.where(\n",
    "           pad_mask_expanded,torch.zeros_like(final_embedding),final_embedding\n",
    "        )\n",
    "        \n",
    "        dtype,device = inputs_embeds.dtype,inputs_embeds.device\n",
    "        min_dtype = torch.finfo(dtype).min\n",
    "        \n",
    "        \n",
    "        q_len = inputs_embeds.shape[1]\n",
    "        \n",
    "        \n",
    "        if kv_cache is None or kv_cache.num_items() == 0:\n",
    "            \n",
    "            causual_mask = torch.full(\n",
    "                (batch_size,q_len,q_len),fill_value=0,\n",
    "                dtype=dtype,\n",
    "                device=device\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            assert q_len == 1\n",
    "            kv_len = kv_cache.num_items() + q_len\n",
    "            \n",
    "            causual_mask = troch.full(\n",
    "              (batch_size,q_len,kv_len),fill_value=0,\n",
    "                dtype=dtype,\n",
    "                device=device\n",
    "            )\n",
    "        \n",
    "        \n",
    "        causual_mask = causual_mask.unsqueeze(1)\n",
    "        \n",
    "        if kv_cache is not None and kv_cache.num_items() > 0:\n",
    "            position_ids = attention_mask.cumsum(-1)[:,-1]\n",
    "            if position_ids.dim() == 1:\n",
    "                position_ids = position_ids.unsqueeze(0)\n",
    "        else:\n",
    "            position_ids = (attention_mask.cumsum(-1)).masked_fill_((attention_mask==0),1).to(device)\n",
    "        \n",
    "        \n",
    "        return final_embedding,causual_mask,position_ids\n",
    "\n",
    "    \n",
    "    \n",
    "    def forward(self,input_ids:torch.LongTensor=None,\n",
    "                     pixel_values: torch.FloatTensor = None,\n",
    "                     attention_mask: Optional[torch.Tensor] = None,\n",
    "                     kv_cache: Optional[KVCache] = None,\n",
    "               ) -> Tuple:\n",
    "        \n",
    "        assert torch.all(attention_mask == 1), \"the input can not be padded\"\n",
    "\n",
    "        inputs_embeds = self.language_model.get_input_embeddings()(input_ids)\n",
    "        \n",
    "        selected_image_feature = self.vision_tower(pixel_values.to(inputs_embeds.dtype))\n",
    "        \n",
    "        \n",
    "        image_features = self.multi_modal_projector(selected_image_feature)\n",
    "        \n",
    "        \n",
    "        inputs_embeds,attention_mask,position_ids = self._merge_input_ids_with_image_features(image_features,\n",
    "                                                                                             inputs_embeds,\n",
    "                                                                                             input_ids,\n",
    "                                                                                             attention_mask,\n",
    "                                                                                             kv_cache)\n",
    "        \n",
    "        \n",
    "        \n",
    "        outputs = self.language_model(\n",
    "           attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            kv_cache=kv_cache\n",
    "        )\n",
    "        return outputs\n",
    "    \n",
    "    def save_pretrained(self,save_directory:str):\n",
    "        \n",
    "        os.makedirs(save_directory,exist_ok=True)\n",
    "        \n",
    "        config_dict = {\n",
    "            'vision_condif': self.config.vision_config.__dict__,\n",
    "            'text_config': self.config.text_config.__dict__,\n",
    "            'ignore_index': self.config.ignore_index,\n",
    "            'image_token_index': self.config.image_token_index,\n",
    "            'vocab_size':self.config.vocab_size,\n",
    "            'projection_dim':self.config.projection_dim,\n",
    "            'hidden_size':self.config.hidden_size,\n",
    "            'pad_token_id':self.config.pad_token_id\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(save_directory,'config.json'),'w') as f:\n",
    "            json.dump(config_dict,f)\n",
    "        \n",
    "        \n",
    "        model_state = self.state_dict()\n",
    "        torch.save(model_state,os.path.join(save_directory,'pytorch_model.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9e896157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxx tensor([1.0000e+00, 5.6234e-01, 3.1623e-01, 1.7783e-01, 1.0000e-01, 5.6234e-02,\n",
      "        3.1623e-02, 1.7783e-02, 1.0000e-02, 5.6234e-03, 3.1623e-03, 1.7783e-03,\n",
      "        1.0000e-03, 5.6234e-04, 3.1623e-04, 1.7783e-04])\n",
      "yyyy torch.Size([1, 16, 1]) tensor([[[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]]])\n",
      "zzzzz tensor([[[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]],\n",
      "\n",
      "        [[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]]])\n",
      "position_ids_expanded torch.Size([2, 1, 10]) tensor([[[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]],\n",
      "\n",
      "        [[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]]])\n",
      "xxxxyyyuuu torch.Size([2, 16, 1]) torch.Size([2, 1, 10])\n",
      "tttt torch.Size([2, 16, 10]) tensor([[[1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00, 5.0000e+00,\n",
      "          6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00, 1.0000e+01],\n",
      "         [5.6234e-01, 1.1247e+00, 1.6870e+00, 2.2494e+00, 2.8117e+00,\n",
      "          3.3740e+00, 3.9364e+00, 4.4987e+00, 5.0611e+00, 5.6234e+00],\n",
      "         [3.1623e-01, 6.3246e-01, 9.4868e-01, 1.2649e+00, 1.5811e+00,\n",
      "          1.8974e+00, 2.2136e+00, 2.5298e+00, 2.8460e+00, 3.1623e+00],\n",
      "         [1.7783e-01, 3.5566e-01, 5.3348e-01, 7.1131e-01, 8.8914e-01,\n",
      "          1.0670e+00, 1.2448e+00, 1.4226e+00, 1.6005e+00, 1.7783e+00],\n",
      "         [1.0000e-01, 2.0000e-01, 3.0000e-01, 4.0000e-01, 5.0000e-01,\n",
      "          6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01, 1.0000e+00],\n",
      "         [5.6234e-02, 1.1247e-01, 1.6870e-01, 2.2494e-01, 2.8117e-01,\n",
      "          3.3740e-01, 3.9364e-01, 4.4987e-01, 5.0611e-01, 5.6234e-01],\n",
      "         [3.1623e-02, 6.3246e-02, 9.4868e-02, 1.2649e-01, 1.5811e-01,\n",
      "          1.8974e-01, 2.2136e-01, 2.5298e-01, 2.8460e-01, 3.1623e-01],\n",
      "         [1.7783e-02, 3.5566e-02, 5.3348e-02, 7.1131e-02, 8.8914e-02,\n",
      "          1.0670e-01, 1.2448e-01, 1.4226e-01, 1.6005e-01, 1.7783e-01],\n",
      "         [1.0000e-02, 2.0000e-02, 3.0000e-02, 4.0000e-02, 5.0000e-02,\n",
      "          6.0000e-02, 7.0000e-02, 8.0000e-02, 9.0000e-02, 1.0000e-01],\n",
      "         [5.6234e-03, 1.1247e-02, 1.6870e-02, 2.2494e-02, 2.8117e-02,\n",
      "          3.3740e-02, 3.9364e-02, 4.4987e-02, 5.0611e-02, 5.6234e-02],\n",
      "         [3.1623e-03, 6.3246e-03, 9.4868e-03, 1.2649e-02, 1.5811e-02,\n",
      "          1.8974e-02, 2.2136e-02, 2.5298e-02, 2.8461e-02, 3.1623e-02],\n",
      "         [1.7783e-03, 3.5566e-03, 5.3348e-03, 7.1131e-03, 8.8914e-03,\n",
      "          1.0670e-02, 1.2448e-02, 1.4226e-02, 1.6005e-02, 1.7783e-02],\n",
      "         [1.0000e-03, 2.0000e-03, 3.0000e-03, 4.0000e-03, 5.0000e-03,\n",
      "          6.0000e-03, 7.0000e-03, 8.0000e-03, 9.0000e-03, 1.0000e-02],\n",
      "         [5.6234e-04, 1.1247e-03, 1.6870e-03, 2.2494e-03, 2.8117e-03,\n",
      "          3.3740e-03, 3.9364e-03, 4.4987e-03, 5.0611e-03, 5.6234e-03],\n",
      "         [3.1623e-04, 6.3246e-04, 9.4868e-04, 1.2649e-03, 1.5811e-03,\n",
      "          1.8974e-03, 2.2136e-03, 2.5298e-03, 2.8461e-03, 3.1623e-03],\n",
      "         [1.7783e-04, 3.5566e-04, 5.3348e-04, 7.1131e-04, 8.8914e-04,\n",
      "          1.0670e-03, 1.2448e-03, 1.4226e-03, 1.6005e-03, 1.7783e-03]],\n",
      "\n",
      "        [[1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00, 5.0000e+00,\n",
      "          6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00, 1.0000e+01],\n",
      "         [5.6234e-01, 1.1247e+00, 1.6870e+00, 2.2494e+00, 2.8117e+00,\n",
      "          3.3740e+00, 3.9364e+00, 4.4987e+00, 5.0611e+00, 5.6234e+00],\n",
      "         [3.1623e-01, 6.3246e-01, 9.4868e-01, 1.2649e+00, 1.5811e+00,\n",
      "          1.8974e+00, 2.2136e+00, 2.5298e+00, 2.8460e+00, 3.1623e+00],\n",
      "         [1.7783e-01, 3.5566e-01, 5.3348e-01, 7.1131e-01, 8.8914e-01,\n",
      "          1.0670e+00, 1.2448e+00, 1.4226e+00, 1.6005e+00, 1.7783e+00],\n",
      "         [1.0000e-01, 2.0000e-01, 3.0000e-01, 4.0000e-01, 5.0000e-01,\n",
      "          6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01, 1.0000e+00],\n",
      "         [5.6234e-02, 1.1247e-01, 1.6870e-01, 2.2494e-01, 2.8117e-01,\n",
      "          3.3740e-01, 3.9364e-01, 4.4987e-01, 5.0611e-01, 5.6234e-01],\n",
      "         [3.1623e-02, 6.3246e-02, 9.4868e-02, 1.2649e-01, 1.5811e-01,\n",
      "          1.8974e-01, 2.2136e-01, 2.5298e-01, 2.8460e-01, 3.1623e-01],\n",
      "         [1.7783e-02, 3.5566e-02, 5.3348e-02, 7.1131e-02, 8.8914e-02,\n",
      "          1.0670e-01, 1.2448e-01, 1.4226e-01, 1.6005e-01, 1.7783e-01],\n",
      "         [1.0000e-02, 2.0000e-02, 3.0000e-02, 4.0000e-02, 5.0000e-02,\n",
      "          6.0000e-02, 7.0000e-02, 8.0000e-02, 9.0000e-02, 1.0000e-01],\n",
      "         [5.6234e-03, 1.1247e-02, 1.6870e-02, 2.2494e-02, 2.8117e-02,\n",
      "          3.3740e-02, 3.9364e-02, 4.4987e-02, 5.0611e-02, 5.6234e-02],\n",
      "         [3.1623e-03, 6.3246e-03, 9.4868e-03, 1.2649e-02, 1.5811e-02,\n",
      "          1.8974e-02, 2.2136e-02, 2.5298e-02, 2.8461e-02, 3.1623e-02],\n",
      "         [1.7783e-03, 3.5566e-03, 5.3348e-03, 7.1131e-03, 8.8914e-03,\n",
      "          1.0670e-02, 1.2448e-02, 1.4226e-02, 1.6005e-02, 1.7783e-02],\n",
      "         [1.0000e-03, 2.0000e-03, 3.0000e-03, 4.0000e-03, 5.0000e-03,\n",
      "          6.0000e-03, 7.0000e-03, 8.0000e-03, 9.0000e-03, 1.0000e-02],\n",
      "         [5.6234e-04, 1.1247e-03, 1.6870e-03, 2.2494e-03, 2.8117e-03,\n",
      "          3.3740e-03, 3.9364e-03, 4.4987e-03, 5.0611e-03, 5.6234e-03],\n",
      "         [3.1623e-04, 6.3246e-04, 9.4868e-04, 1.2649e-03, 1.5811e-03,\n",
      "          1.8974e-03, 2.2136e-03, 2.5298e-03, 2.8461e-03, 3.1623e-03],\n",
      "         [1.7783e-04, 3.5566e-04, 5.3348e-04, 7.1131e-04, 8.8914e-04,\n",
      "          1.0670e-03, 1.2448e-03, 1.4226e-03, 1.6005e-03, 1.7783e-03]]])\n",
      "q torch.Size([2, 8, 10, 32]) cos torch.Size([2, 1, 10, 32])\n",
      "xxxx tensor([1.0000e+00, 5.6234e-01, 3.1623e-01, 1.7783e-01, 1.0000e-01, 5.6234e-02,\n",
      "        3.1623e-02, 1.7783e-02, 1.0000e-02, 5.6234e-03, 3.1623e-03, 1.7783e-03,\n",
      "        1.0000e-03, 5.6234e-04, 3.1623e-04, 1.7783e-04])\n",
      "yyyy torch.Size([1, 16, 1]) tensor([[[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]]])\n",
      "zzzzz tensor([[[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]],\n",
      "\n",
      "        [[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]]])\n",
      "position_ids_expanded torch.Size([2, 1, 10]) tensor([[[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]],\n",
      "\n",
      "        [[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]]])\n",
      "xxxxyyyuuu torch.Size([2, 16, 1]) torch.Size([2, 1, 10])\n",
      "tttt torch.Size([2, 16, 10]) tensor([[[1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00, 5.0000e+00,\n",
      "          6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00, 1.0000e+01],\n",
      "         [5.6234e-01, 1.1247e+00, 1.6870e+00, 2.2494e+00, 2.8117e+00,\n",
      "          3.3740e+00, 3.9364e+00, 4.4987e+00, 5.0611e+00, 5.6234e+00],\n",
      "         [3.1623e-01, 6.3246e-01, 9.4868e-01, 1.2649e+00, 1.5811e+00,\n",
      "          1.8974e+00, 2.2136e+00, 2.5298e+00, 2.8460e+00, 3.1623e+00],\n",
      "         [1.7783e-01, 3.5566e-01, 5.3348e-01, 7.1131e-01, 8.8914e-01,\n",
      "          1.0670e+00, 1.2448e+00, 1.4226e+00, 1.6005e+00, 1.7783e+00],\n",
      "         [1.0000e-01, 2.0000e-01, 3.0000e-01, 4.0000e-01, 5.0000e-01,\n",
      "          6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01, 1.0000e+00],\n",
      "         [5.6234e-02, 1.1247e-01, 1.6870e-01, 2.2494e-01, 2.8117e-01,\n",
      "          3.3740e-01, 3.9364e-01, 4.4987e-01, 5.0611e-01, 5.6234e-01],\n",
      "         [3.1623e-02, 6.3246e-02, 9.4868e-02, 1.2649e-01, 1.5811e-01,\n",
      "          1.8974e-01, 2.2136e-01, 2.5298e-01, 2.8460e-01, 3.1623e-01],\n",
      "         [1.7783e-02, 3.5566e-02, 5.3348e-02, 7.1131e-02, 8.8914e-02,\n",
      "          1.0670e-01, 1.2448e-01, 1.4226e-01, 1.6005e-01, 1.7783e-01],\n",
      "         [1.0000e-02, 2.0000e-02, 3.0000e-02, 4.0000e-02, 5.0000e-02,\n",
      "          6.0000e-02, 7.0000e-02, 8.0000e-02, 9.0000e-02, 1.0000e-01],\n",
      "         [5.6234e-03, 1.1247e-02, 1.6870e-02, 2.2494e-02, 2.8117e-02,\n",
      "          3.3740e-02, 3.9364e-02, 4.4987e-02, 5.0611e-02, 5.6234e-02],\n",
      "         [3.1623e-03, 6.3246e-03, 9.4868e-03, 1.2649e-02, 1.5811e-02,\n",
      "          1.8974e-02, 2.2136e-02, 2.5298e-02, 2.8461e-02, 3.1623e-02],\n",
      "         [1.7783e-03, 3.5566e-03, 5.3348e-03, 7.1131e-03, 8.8914e-03,\n",
      "          1.0670e-02, 1.2448e-02, 1.4226e-02, 1.6005e-02, 1.7783e-02],\n",
      "         [1.0000e-03, 2.0000e-03, 3.0000e-03, 4.0000e-03, 5.0000e-03,\n",
      "          6.0000e-03, 7.0000e-03, 8.0000e-03, 9.0000e-03, 1.0000e-02],\n",
      "         [5.6234e-04, 1.1247e-03, 1.6870e-03, 2.2494e-03, 2.8117e-03,\n",
      "          3.3740e-03, 3.9364e-03, 4.4987e-03, 5.0611e-03, 5.6234e-03],\n",
      "         [3.1623e-04, 6.3246e-04, 9.4868e-04, 1.2649e-03, 1.5811e-03,\n",
      "          1.8974e-03, 2.2136e-03, 2.5298e-03, 2.8461e-03, 3.1623e-03],\n",
      "         [1.7783e-04, 3.5566e-04, 5.3348e-04, 7.1131e-04, 8.8914e-04,\n",
      "          1.0670e-03, 1.2448e-03, 1.4226e-03, 1.6005e-03, 1.7783e-03]],\n",
      "\n",
      "        [[1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00, 5.0000e+00,\n",
      "          6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00, 1.0000e+01],\n",
      "         [5.6234e-01, 1.1247e+00, 1.6870e+00, 2.2494e+00, 2.8117e+00,\n",
      "          3.3740e+00, 3.9364e+00, 4.4987e+00, 5.0611e+00, 5.6234e+00],\n",
      "         [3.1623e-01, 6.3246e-01, 9.4868e-01, 1.2649e+00, 1.5811e+00,\n",
      "          1.8974e+00, 2.2136e+00, 2.5298e+00, 2.8460e+00, 3.1623e+00],\n",
      "         [1.7783e-01, 3.5566e-01, 5.3348e-01, 7.1131e-01, 8.8914e-01,\n",
      "          1.0670e+00, 1.2448e+00, 1.4226e+00, 1.6005e+00, 1.7783e+00],\n",
      "         [1.0000e-01, 2.0000e-01, 3.0000e-01, 4.0000e-01, 5.0000e-01,\n",
      "          6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01, 1.0000e+00],\n",
      "         [5.6234e-02, 1.1247e-01, 1.6870e-01, 2.2494e-01, 2.8117e-01,\n",
      "          3.3740e-01, 3.9364e-01, 4.4987e-01, 5.0611e-01, 5.6234e-01],\n",
      "         [3.1623e-02, 6.3246e-02, 9.4868e-02, 1.2649e-01, 1.5811e-01,\n",
      "          1.8974e-01, 2.2136e-01, 2.5298e-01, 2.8460e-01, 3.1623e-01],\n",
      "         [1.7783e-02, 3.5566e-02, 5.3348e-02, 7.1131e-02, 8.8914e-02,\n",
      "          1.0670e-01, 1.2448e-01, 1.4226e-01, 1.6005e-01, 1.7783e-01],\n",
      "         [1.0000e-02, 2.0000e-02, 3.0000e-02, 4.0000e-02, 5.0000e-02,\n",
      "          6.0000e-02, 7.0000e-02, 8.0000e-02, 9.0000e-02, 1.0000e-01],\n",
      "         [5.6234e-03, 1.1247e-02, 1.6870e-02, 2.2494e-02, 2.8117e-02,\n",
      "          3.3740e-02, 3.9364e-02, 4.4987e-02, 5.0611e-02, 5.6234e-02],\n",
      "         [3.1623e-03, 6.3246e-03, 9.4868e-03, 1.2649e-02, 1.5811e-02,\n",
      "          1.8974e-02, 2.2136e-02, 2.5298e-02, 2.8461e-02, 3.1623e-02],\n",
      "         [1.7783e-03, 3.5566e-03, 5.3348e-03, 7.1131e-03, 8.8914e-03,\n",
      "          1.0670e-02, 1.2448e-02, 1.4226e-02, 1.6005e-02, 1.7783e-02],\n",
      "         [1.0000e-03, 2.0000e-03, 3.0000e-03, 4.0000e-03, 5.0000e-03,\n",
      "          6.0000e-03, 7.0000e-03, 8.0000e-03, 9.0000e-03, 1.0000e-02],\n",
      "         [5.6234e-04, 1.1247e-03, 1.6870e-03, 2.2494e-03, 2.8117e-03,\n",
      "          3.3740e-03, 3.9364e-03, 4.4987e-03, 5.0611e-03, 5.6234e-03],\n",
      "         [3.1623e-04, 6.3246e-04, 9.4868e-04, 1.2649e-03, 1.5811e-03,\n",
      "          1.8974e-03, 2.2136e-03, 2.5298e-03, 2.8461e-03, 3.1623e-03],\n",
      "         [1.7783e-04, 3.5566e-04, 5.3348e-04, 7.1131e-04, 8.8914e-04,\n",
      "          1.0670e-03, 1.2448e-03, 1.4226e-03, 1.6005e-03, 1.7783e-03]]])\n",
      "q torch.Size([2, 8, 10, 32]) cos torch.Size([2, 1, 10, 32])\n",
      "xxxx tensor([1.0000e+00, 5.6234e-01, 3.1623e-01, 1.7783e-01, 1.0000e-01, 5.6234e-02,\n",
      "        3.1623e-02, 1.7783e-02, 1.0000e-02, 5.6234e-03, 3.1623e-03, 1.7783e-03,\n",
      "        1.0000e-03, 5.6234e-04, 3.1623e-04, 1.7783e-04])\n",
      "yyyy torch.Size([1, 16, 1]) tensor([[[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]]])\n",
      "zzzzz tensor([[[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]],\n",
      "\n",
      "        [[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]]])\n",
      "position_ids_expanded torch.Size([2, 1, 10]) tensor([[[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]],\n",
      "\n",
      "        [[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]]])\n",
      "xxxxyyyuuu torch.Size([2, 16, 1]) torch.Size([2, 1, 10])\n",
      "tttt torch.Size([2, 16, 10]) tensor([[[1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00, 5.0000e+00,\n",
      "          6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00, 1.0000e+01],\n",
      "         [5.6234e-01, 1.1247e+00, 1.6870e+00, 2.2494e+00, 2.8117e+00,\n",
      "          3.3740e+00, 3.9364e+00, 4.4987e+00, 5.0611e+00, 5.6234e+00],\n",
      "         [3.1623e-01, 6.3246e-01, 9.4868e-01, 1.2649e+00, 1.5811e+00,\n",
      "          1.8974e+00, 2.2136e+00, 2.5298e+00, 2.8460e+00, 3.1623e+00],\n",
      "         [1.7783e-01, 3.5566e-01, 5.3348e-01, 7.1131e-01, 8.8914e-01,\n",
      "          1.0670e+00, 1.2448e+00, 1.4226e+00, 1.6005e+00, 1.7783e+00],\n",
      "         [1.0000e-01, 2.0000e-01, 3.0000e-01, 4.0000e-01, 5.0000e-01,\n",
      "          6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01, 1.0000e+00],\n",
      "         [5.6234e-02, 1.1247e-01, 1.6870e-01, 2.2494e-01, 2.8117e-01,\n",
      "          3.3740e-01, 3.9364e-01, 4.4987e-01, 5.0611e-01, 5.6234e-01],\n",
      "         [3.1623e-02, 6.3246e-02, 9.4868e-02, 1.2649e-01, 1.5811e-01,\n",
      "          1.8974e-01, 2.2136e-01, 2.5298e-01, 2.8460e-01, 3.1623e-01],\n",
      "         [1.7783e-02, 3.5566e-02, 5.3348e-02, 7.1131e-02, 8.8914e-02,\n",
      "          1.0670e-01, 1.2448e-01, 1.4226e-01, 1.6005e-01, 1.7783e-01],\n",
      "         [1.0000e-02, 2.0000e-02, 3.0000e-02, 4.0000e-02, 5.0000e-02,\n",
      "          6.0000e-02, 7.0000e-02, 8.0000e-02, 9.0000e-02, 1.0000e-01],\n",
      "         [5.6234e-03, 1.1247e-02, 1.6870e-02, 2.2494e-02, 2.8117e-02,\n",
      "          3.3740e-02, 3.9364e-02, 4.4987e-02, 5.0611e-02, 5.6234e-02],\n",
      "         [3.1623e-03, 6.3246e-03, 9.4868e-03, 1.2649e-02, 1.5811e-02,\n",
      "          1.8974e-02, 2.2136e-02, 2.5298e-02, 2.8461e-02, 3.1623e-02],\n",
      "         [1.7783e-03, 3.5566e-03, 5.3348e-03, 7.1131e-03, 8.8914e-03,\n",
      "          1.0670e-02, 1.2448e-02, 1.4226e-02, 1.6005e-02, 1.7783e-02],\n",
      "         [1.0000e-03, 2.0000e-03, 3.0000e-03, 4.0000e-03, 5.0000e-03,\n",
      "          6.0000e-03, 7.0000e-03, 8.0000e-03, 9.0000e-03, 1.0000e-02],\n",
      "         [5.6234e-04, 1.1247e-03, 1.6870e-03, 2.2494e-03, 2.8117e-03,\n",
      "          3.3740e-03, 3.9364e-03, 4.4987e-03, 5.0611e-03, 5.6234e-03],\n",
      "         [3.1623e-04, 6.3246e-04, 9.4868e-04, 1.2649e-03, 1.5811e-03,\n",
      "          1.8974e-03, 2.2136e-03, 2.5298e-03, 2.8461e-03, 3.1623e-03],\n",
      "         [1.7783e-04, 3.5566e-04, 5.3348e-04, 7.1131e-04, 8.8914e-04,\n",
      "          1.0670e-03, 1.2448e-03, 1.4226e-03, 1.6005e-03, 1.7783e-03]],\n",
      "\n",
      "        [[1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00, 5.0000e+00,\n",
      "          6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00, 1.0000e+01],\n",
      "         [5.6234e-01, 1.1247e+00, 1.6870e+00, 2.2494e+00, 2.8117e+00,\n",
      "          3.3740e+00, 3.9364e+00, 4.4987e+00, 5.0611e+00, 5.6234e+00],\n",
      "         [3.1623e-01, 6.3246e-01, 9.4868e-01, 1.2649e+00, 1.5811e+00,\n",
      "          1.8974e+00, 2.2136e+00, 2.5298e+00, 2.8460e+00, 3.1623e+00],\n",
      "         [1.7783e-01, 3.5566e-01, 5.3348e-01, 7.1131e-01, 8.8914e-01,\n",
      "          1.0670e+00, 1.2448e+00, 1.4226e+00, 1.6005e+00, 1.7783e+00],\n",
      "         [1.0000e-01, 2.0000e-01, 3.0000e-01, 4.0000e-01, 5.0000e-01,\n",
      "          6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01, 1.0000e+00],\n",
      "         [5.6234e-02, 1.1247e-01, 1.6870e-01, 2.2494e-01, 2.8117e-01,\n",
      "          3.3740e-01, 3.9364e-01, 4.4987e-01, 5.0611e-01, 5.6234e-01],\n",
      "         [3.1623e-02, 6.3246e-02, 9.4868e-02, 1.2649e-01, 1.5811e-01,\n",
      "          1.8974e-01, 2.2136e-01, 2.5298e-01, 2.8460e-01, 3.1623e-01],\n",
      "         [1.7783e-02, 3.5566e-02, 5.3348e-02, 7.1131e-02, 8.8914e-02,\n",
      "          1.0670e-01, 1.2448e-01, 1.4226e-01, 1.6005e-01, 1.7783e-01],\n",
      "         [1.0000e-02, 2.0000e-02, 3.0000e-02, 4.0000e-02, 5.0000e-02,\n",
      "          6.0000e-02, 7.0000e-02, 8.0000e-02, 9.0000e-02, 1.0000e-01],\n",
      "         [5.6234e-03, 1.1247e-02, 1.6870e-02, 2.2494e-02, 2.8117e-02,\n",
      "          3.3740e-02, 3.9364e-02, 4.4987e-02, 5.0611e-02, 5.6234e-02],\n",
      "         [3.1623e-03, 6.3246e-03, 9.4868e-03, 1.2649e-02, 1.5811e-02,\n",
      "          1.8974e-02, 2.2136e-02, 2.5298e-02, 2.8461e-02, 3.1623e-02],\n",
      "         [1.7783e-03, 3.5566e-03, 5.3348e-03, 7.1131e-03, 8.8914e-03,\n",
      "          1.0670e-02, 1.2448e-02, 1.4226e-02, 1.6005e-02, 1.7783e-02],\n",
      "         [1.0000e-03, 2.0000e-03, 3.0000e-03, 4.0000e-03, 5.0000e-03,\n",
      "          6.0000e-03, 7.0000e-03, 8.0000e-03, 9.0000e-03, 1.0000e-02],\n",
      "         [5.6234e-04, 1.1247e-03, 1.6870e-03, 2.2494e-03, 2.8117e-03,\n",
      "          3.3740e-03, 3.9364e-03, 4.4987e-03, 5.0611e-03, 5.6234e-03],\n",
      "         [3.1623e-04, 6.3246e-04, 9.4868e-04, 1.2649e-03, 1.5811e-03,\n",
      "          1.8974e-03, 2.2136e-03, 2.5298e-03, 2.8461e-03, 3.1623e-03],\n",
      "         [1.7783e-04, 3.5566e-04, 5.3348e-04, 7.1131e-04, 8.8914e-04,\n",
      "          1.0670e-03, 1.2448e-03, 1.4226e-03, 1.6005e-03, 1.7783e-03]]])\n",
      "q torch.Size([2, 8, 10, 32]) cos torch.Size([2, 1, 10, 32])\n",
      "xxxx tensor([1.0000e+00, 5.6234e-01, 3.1623e-01, 1.7783e-01, 1.0000e-01, 5.6234e-02,\n",
      "        3.1623e-02, 1.7783e-02, 1.0000e-02, 5.6234e-03, 3.1623e-03, 1.7783e-03,\n",
      "        1.0000e-03, 5.6234e-04, 3.1623e-04, 1.7783e-04])\n",
      "yyyy torch.Size([1, 16, 1]) tensor([[[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]]])\n",
      "zzzzz tensor([[[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]],\n",
      "\n",
      "        [[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]]])\n",
      "position_ids_expanded torch.Size([2, 1, 10]) tensor([[[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]],\n",
      "\n",
      "        [[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]]])\n",
      "xxxxyyyuuu torch.Size([2, 16, 1]) torch.Size([2, 1, 10])\n",
      "tttt torch.Size([2, 16, 10]) tensor([[[1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00, 5.0000e+00,\n",
      "          6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00, 1.0000e+01],\n",
      "         [5.6234e-01, 1.1247e+00, 1.6870e+00, 2.2494e+00, 2.8117e+00,\n",
      "          3.3740e+00, 3.9364e+00, 4.4987e+00, 5.0611e+00, 5.6234e+00],\n",
      "         [3.1623e-01, 6.3246e-01, 9.4868e-01, 1.2649e+00, 1.5811e+00,\n",
      "          1.8974e+00, 2.2136e+00, 2.5298e+00, 2.8460e+00, 3.1623e+00],\n",
      "         [1.7783e-01, 3.5566e-01, 5.3348e-01, 7.1131e-01, 8.8914e-01,\n",
      "          1.0670e+00, 1.2448e+00, 1.4226e+00, 1.6005e+00, 1.7783e+00],\n",
      "         [1.0000e-01, 2.0000e-01, 3.0000e-01, 4.0000e-01, 5.0000e-01,\n",
      "          6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01, 1.0000e+00],\n",
      "         [5.6234e-02, 1.1247e-01, 1.6870e-01, 2.2494e-01, 2.8117e-01,\n",
      "          3.3740e-01, 3.9364e-01, 4.4987e-01, 5.0611e-01, 5.6234e-01],\n",
      "         [3.1623e-02, 6.3246e-02, 9.4868e-02, 1.2649e-01, 1.5811e-01,\n",
      "          1.8974e-01, 2.2136e-01, 2.5298e-01, 2.8460e-01, 3.1623e-01],\n",
      "         [1.7783e-02, 3.5566e-02, 5.3348e-02, 7.1131e-02, 8.8914e-02,\n",
      "          1.0670e-01, 1.2448e-01, 1.4226e-01, 1.6005e-01, 1.7783e-01],\n",
      "         [1.0000e-02, 2.0000e-02, 3.0000e-02, 4.0000e-02, 5.0000e-02,\n",
      "          6.0000e-02, 7.0000e-02, 8.0000e-02, 9.0000e-02, 1.0000e-01],\n",
      "         [5.6234e-03, 1.1247e-02, 1.6870e-02, 2.2494e-02, 2.8117e-02,\n",
      "          3.3740e-02, 3.9364e-02, 4.4987e-02, 5.0611e-02, 5.6234e-02],\n",
      "         [3.1623e-03, 6.3246e-03, 9.4868e-03, 1.2649e-02, 1.5811e-02,\n",
      "          1.8974e-02, 2.2136e-02, 2.5298e-02, 2.8461e-02, 3.1623e-02],\n",
      "         [1.7783e-03, 3.5566e-03, 5.3348e-03, 7.1131e-03, 8.8914e-03,\n",
      "          1.0670e-02, 1.2448e-02, 1.4226e-02, 1.6005e-02, 1.7783e-02],\n",
      "         [1.0000e-03, 2.0000e-03, 3.0000e-03, 4.0000e-03, 5.0000e-03,\n",
      "          6.0000e-03, 7.0000e-03, 8.0000e-03, 9.0000e-03, 1.0000e-02],\n",
      "         [5.6234e-04, 1.1247e-03, 1.6870e-03, 2.2494e-03, 2.8117e-03,\n",
      "          3.3740e-03, 3.9364e-03, 4.4987e-03, 5.0611e-03, 5.6234e-03],\n",
      "         [3.1623e-04, 6.3246e-04, 9.4868e-04, 1.2649e-03, 1.5811e-03,\n",
      "          1.8974e-03, 2.2136e-03, 2.5298e-03, 2.8461e-03, 3.1623e-03],\n",
      "         [1.7783e-04, 3.5566e-04, 5.3348e-04, 7.1131e-04, 8.8914e-04,\n",
      "          1.0670e-03, 1.2448e-03, 1.4226e-03, 1.6005e-03, 1.7783e-03]],\n",
      "\n",
      "        [[1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00, 5.0000e+00,\n",
      "          6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00, 1.0000e+01],\n",
      "         [5.6234e-01, 1.1247e+00, 1.6870e+00, 2.2494e+00, 2.8117e+00,\n",
      "          3.3740e+00, 3.9364e+00, 4.4987e+00, 5.0611e+00, 5.6234e+00],\n",
      "         [3.1623e-01, 6.3246e-01, 9.4868e-01, 1.2649e+00, 1.5811e+00,\n",
      "          1.8974e+00, 2.2136e+00, 2.5298e+00, 2.8460e+00, 3.1623e+00],\n",
      "         [1.7783e-01, 3.5566e-01, 5.3348e-01, 7.1131e-01, 8.8914e-01,\n",
      "          1.0670e+00, 1.2448e+00, 1.4226e+00, 1.6005e+00, 1.7783e+00],\n",
      "         [1.0000e-01, 2.0000e-01, 3.0000e-01, 4.0000e-01, 5.0000e-01,\n",
      "          6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01, 1.0000e+00],\n",
      "         [5.6234e-02, 1.1247e-01, 1.6870e-01, 2.2494e-01, 2.8117e-01,\n",
      "          3.3740e-01, 3.9364e-01, 4.4987e-01, 5.0611e-01, 5.6234e-01],\n",
      "         [3.1623e-02, 6.3246e-02, 9.4868e-02, 1.2649e-01, 1.5811e-01,\n",
      "          1.8974e-01, 2.2136e-01, 2.5298e-01, 2.8460e-01, 3.1623e-01],\n",
      "         [1.7783e-02, 3.5566e-02, 5.3348e-02, 7.1131e-02, 8.8914e-02,\n",
      "          1.0670e-01, 1.2448e-01, 1.4226e-01, 1.6005e-01, 1.7783e-01],\n",
      "         [1.0000e-02, 2.0000e-02, 3.0000e-02, 4.0000e-02, 5.0000e-02,\n",
      "          6.0000e-02, 7.0000e-02, 8.0000e-02, 9.0000e-02, 1.0000e-01],\n",
      "         [5.6234e-03, 1.1247e-02, 1.6870e-02, 2.2494e-02, 2.8117e-02,\n",
      "          3.3740e-02, 3.9364e-02, 4.4987e-02, 5.0611e-02, 5.6234e-02],\n",
      "         [3.1623e-03, 6.3246e-03, 9.4868e-03, 1.2649e-02, 1.5811e-02,\n",
      "          1.8974e-02, 2.2136e-02, 2.5298e-02, 2.8461e-02, 3.1623e-02],\n",
      "         [1.7783e-03, 3.5566e-03, 5.3348e-03, 7.1131e-03, 8.8914e-03,\n",
      "          1.0670e-02, 1.2448e-02, 1.4226e-02, 1.6005e-02, 1.7783e-02],\n",
      "         [1.0000e-03, 2.0000e-03, 3.0000e-03, 4.0000e-03, 5.0000e-03,\n",
      "          6.0000e-03, 7.0000e-03, 8.0000e-03, 9.0000e-03, 1.0000e-02],\n",
      "         [5.6234e-04, 1.1247e-03, 1.6870e-03, 2.2494e-03, 2.8117e-03,\n",
      "          3.3740e-03, 3.9364e-03, 4.4987e-03, 5.0611e-03, 5.6234e-03],\n",
      "         [3.1623e-04, 6.3246e-04, 9.4868e-04, 1.2649e-03, 1.5811e-03,\n",
      "          1.8974e-03, 2.2136e-03, 2.5298e-03, 2.8461e-03, 3.1623e-03],\n",
      "         [1.7783e-04, 3.5566e-04, 5.3348e-04, 7.1131e-04, 8.8914e-04,\n",
      "          1.0670e-03, 1.2448e-03, 1.4226e-03, 1.6005e-03, 1.7783e-03]]])\n",
      "q torch.Size([2, 8, 10, 32]) cos torch.Size([2, 1, 10, 32])\n",
      "xxxx tensor([1.0000e+00, 5.6234e-01, 3.1623e-01, 1.7783e-01, 1.0000e-01, 5.6234e-02,\n",
      "        3.1623e-02, 1.7783e-02, 1.0000e-02, 5.6234e-03, 3.1623e-03, 1.7783e-03,\n",
      "        1.0000e-03, 5.6234e-04, 3.1623e-04, 1.7783e-04])\n",
      "yyyy torch.Size([1, 16, 1]) tensor([[[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]]])\n",
      "zzzzz tensor([[[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]],\n",
      "\n",
      "        [[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]]])\n",
      "position_ids_expanded torch.Size([2, 1, 10]) tensor([[[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]],\n",
      "\n",
      "        [[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]]])\n",
      "xxxxyyyuuu torch.Size([2, 16, 1]) torch.Size([2, 1, 10])\n",
      "tttt torch.Size([2, 16, 10]) tensor([[[1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00, 5.0000e+00,\n",
      "          6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00, 1.0000e+01],\n",
      "         [5.6234e-01, 1.1247e+00, 1.6870e+00, 2.2494e+00, 2.8117e+00,\n",
      "          3.3740e+00, 3.9364e+00, 4.4987e+00, 5.0611e+00, 5.6234e+00],\n",
      "         [3.1623e-01, 6.3246e-01, 9.4868e-01, 1.2649e+00, 1.5811e+00,\n",
      "          1.8974e+00, 2.2136e+00, 2.5298e+00, 2.8460e+00, 3.1623e+00],\n",
      "         [1.7783e-01, 3.5566e-01, 5.3348e-01, 7.1131e-01, 8.8914e-01,\n",
      "          1.0670e+00, 1.2448e+00, 1.4226e+00, 1.6005e+00, 1.7783e+00],\n",
      "         [1.0000e-01, 2.0000e-01, 3.0000e-01, 4.0000e-01, 5.0000e-01,\n",
      "          6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01, 1.0000e+00],\n",
      "         [5.6234e-02, 1.1247e-01, 1.6870e-01, 2.2494e-01, 2.8117e-01,\n",
      "          3.3740e-01, 3.9364e-01, 4.4987e-01, 5.0611e-01, 5.6234e-01],\n",
      "         [3.1623e-02, 6.3246e-02, 9.4868e-02, 1.2649e-01, 1.5811e-01,\n",
      "          1.8974e-01, 2.2136e-01, 2.5298e-01, 2.8460e-01, 3.1623e-01],\n",
      "         [1.7783e-02, 3.5566e-02, 5.3348e-02, 7.1131e-02, 8.8914e-02,\n",
      "          1.0670e-01, 1.2448e-01, 1.4226e-01, 1.6005e-01, 1.7783e-01],\n",
      "         [1.0000e-02, 2.0000e-02, 3.0000e-02, 4.0000e-02, 5.0000e-02,\n",
      "          6.0000e-02, 7.0000e-02, 8.0000e-02, 9.0000e-02, 1.0000e-01],\n",
      "         [5.6234e-03, 1.1247e-02, 1.6870e-02, 2.2494e-02, 2.8117e-02,\n",
      "          3.3740e-02, 3.9364e-02, 4.4987e-02, 5.0611e-02, 5.6234e-02],\n",
      "         [3.1623e-03, 6.3246e-03, 9.4868e-03, 1.2649e-02, 1.5811e-02,\n",
      "          1.8974e-02, 2.2136e-02, 2.5298e-02, 2.8461e-02, 3.1623e-02],\n",
      "         [1.7783e-03, 3.5566e-03, 5.3348e-03, 7.1131e-03, 8.8914e-03,\n",
      "          1.0670e-02, 1.2448e-02, 1.4226e-02, 1.6005e-02, 1.7783e-02],\n",
      "         [1.0000e-03, 2.0000e-03, 3.0000e-03, 4.0000e-03, 5.0000e-03,\n",
      "          6.0000e-03, 7.0000e-03, 8.0000e-03, 9.0000e-03, 1.0000e-02],\n",
      "         [5.6234e-04, 1.1247e-03, 1.6870e-03, 2.2494e-03, 2.8117e-03,\n",
      "          3.3740e-03, 3.9364e-03, 4.4987e-03, 5.0611e-03, 5.6234e-03],\n",
      "         [3.1623e-04, 6.3246e-04, 9.4868e-04, 1.2649e-03, 1.5811e-03,\n",
      "          1.8974e-03, 2.2136e-03, 2.5298e-03, 2.8461e-03, 3.1623e-03],\n",
      "         [1.7783e-04, 3.5566e-04, 5.3348e-04, 7.1131e-04, 8.8914e-04,\n",
      "          1.0670e-03, 1.2448e-03, 1.4226e-03, 1.6005e-03, 1.7783e-03]],\n",
      "\n",
      "        [[1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00, 5.0000e+00,\n",
      "          6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00, 1.0000e+01],\n",
      "         [5.6234e-01, 1.1247e+00, 1.6870e+00, 2.2494e+00, 2.8117e+00,\n",
      "          3.3740e+00, 3.9364e+00, 4.4987e+00, 5.0611e+00, 5.6234e+00],\n",
      "         [3.1623e-01, 6.3246e-01, 9.4868e-01, 1.2649e+00, 1.5811e+00,\n",
      "          1.8974e+00, 2.2136e+00, 2.5298e+00, 2.8460e+00, 3.1623e+00],\n",
      "         [1.7783e-01, 3.5566e-01, 5.3348e-01, 7.1131e-01, 8.8914e-01,\n",
      "          1.0670e+00, 1.2448e+00, 1.4226e+00, 1.6005e+00, 1.7783e+00],\n",
      "         [1.0000e-01, 2.0000e-01, 3.0000e-01, 4.0000e-01, 5.0000e-01,\n",
      "          6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01, 1.0000e+00],\n",
      "         [5.6234e-02, 1.1247e-01, 1.6870e-01, 2.2494e-01, 2.8117e-01,\n",
      "          3.3740e-01, 3.9364e-01, 4.4987e-01, 5.0611e-01, 5.6234e-01],\n",
      "         [3.1623e-02, 6.3246e-02, 9.4868e-02, 1.2649e-01, 1.5811e-01,\n",
      "          1.8974e-01, 2.2136e-01, 2.5298e-01, 2.8460e-01, 3.1623e-01],\n",
      "         [1.7783e-02, 3.5566e-02, 5.3348e-02, 7.1131e-02, 8.8914e-02,\n",
      "          1.0670e-01, 1.2448e-01, 1.4226e-01, 1.6005e-01, 1.7783e-01],\n",
      "         [1.0000e-02, 2.0000e-02, 3.0000e-02, 4.0000e-02, 5.0000e-02,\n",
      "          6.0000e-02, 7.0000e-02, 8.0000e-02, 9.0000e-02, 1.0000e-01],\n",
      "         [5.6234e-03, 1.1247e-02, 1.6870e-02, 2.2494e-02, 2.8117e-02,\n",
      "          3.3740e-02, 3.9364e-02, 4.4987e-02, 5.0611e-02, 5.6234e-02],\n",
      "         [3.1623e-03, 6.3246e-03, 9.4868e-03, 1.2649e-02, 1.5811e-02,\n",
      "          1.8974e-02, 2.2136e-02, 2.5298e-02, 2.8461e-02, 3.1623e-02],\n",
      "         [1.7783e-03, 3.5566e-03, 5.3348e-03, 7.1131e-03, 8.8914e-03,\n",
      "          1.0670e-02, 1.2448e-02, 1.4226e-02, 1.6005e-02, 1.7783e-02],\n",
      "         [1.0000e-03, 2.0000e-03, 3.0000e-03, 4.0000e-03, 5.0000e-03,\n",
      "          6.0000e-03, 7.0000e-03, 8.0000e-03, 9.0000e-03, 1.0000e-02],\n",
      "         [5.6234e-04, 1.1247e-03, 1.6870e-03, 2.2494e-03, 2.8117e-03,\n",
      "          3.3740e-03, 3.9364e-03, 4.4987e-03, 5.0611e-03, 5.6234e-03],\n",
      "         [3.1623e-04, 6.3246e-04, 9.4868e-04, 1.2649e-03, 1.5811e-03,\n",
      "          1.8974e-03, 2.2136e-03, 2.5298e-03, 2.8461e-03, 3.1623e-03],\n",
      "         [1.7783e-04, 3.5566e-04, 5.3348e-04, 7.1131e-04, 8.8914e-04,\n",
      "          1.0670e-03, 1.2448e-03, 1.4226e-03, 1.6005e-03, 1.7783e-03]]])\n",
      "q torch.Size([2, 8, 10, 32]) cos torch.Size([2, 1, 10, 32])\n",
      "xxxx tensor([1.0000e+00, 5.6234e-01, 3.1623e-01, 1.7783e-01, 1.0000e-01, 5.6234e-02,\n",
      "        3.1623e-02, 1.7783e-02, 1.0000e-02, 5.6234e-03, 3.1623e-03, 1.7783e-03,\n",
      "        1.0000e-03, 5.6234e-04, 3.1623e-04, 1.7783e-04])\n",
      "yyyy torch.Size([1, 16, 1]) tensor([[[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]]])\n",
      "zzzzz tensor([[[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]],\n",
      "\n",
      "        [[1.0000e+00],\n",
      "         [5.6234e-01],\n",
      "         [3.1623e-01],\n",
      "         [1.7783e-01],\n",
      "         [1.0000e-01],\n",
      "         [5.6234e-02],\n",
      "         [3.1623e-02],\n",
      "         [1.7783e-02],\n",
      "         [1.0000e-02],\n",
      "         [5.6234e-03],\n",
      "         [3.1623e-03],\n",
      "         [1.7783e-03],\n",
      "         [1.0000e-03],\n",
      "         [5.6234e-04],\n",
      "         [3.1623e-04],\n",
      "         [1.7783e-04]]])\n",
      "position_ids_expanded torch.Size([2, 1, 10]) tensor([[[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]],\n",
      "\n",
      "        [[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]]])\n",
      "xxxxyyyuuu torch.Size([2, 16, 1]) torch.Size([2, 1, 10])\n",
      "tttt torch.Size([2, 16, 10]) tensor([[[1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00, 5.0000e+00,\n",
      "          6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00, 1.0000e+01],\n",
      "         [5.6234e-01, 1.1247e+00, 1.6870e+00, 2.2494e+00, 2.8117e+00,\n",
      "          3.3740e+00, 3.9364e+00, 4.4987e+00, 5.0611e+00, 5.6234e+00],\n",
      "         [3.1623e-01, 6.3246e-01, 9.4868e-01, 1.2649e+00, 1.5811e+00,\n",
      "          1.8974e+00, 2.2136e+00, 2.5298e+00, 2.8460e+00, 3.1623e+00],\n",
      "         [1.7783e-01, 3.5566e-01, 5.3348e-01, 7.1131e-01, 8.8914e-01,\n",
      "          1.0670e+00, 1.2448e+00, 1.4226e+00, 1.6005e+00, 1.7783e+00],\n",
      "         [1.0000e-01, 2.0000e-01, 3.0000e-01, 4.0000e-01, 5.0000e-01,\n",
      "          6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01, 1.0000e+00],\n",
      "         [5.6234e-02, 1.1247e-01, 1.6870e-01, 2.2494e-01, 2.8117e-01,\n",
      "          3.3740e-01, 3.9364e-01, 4.4987e-01, 5.0611e-01, 5.6234e-01],\n",
      "         [3.1623e-02, 6.3246e-02, 9.4868e-02, 1.2649e-01, 1.5811e-01,\n",
      "          1.8974e-01, 2.2136e-01, 2.5298e-01, 2.8460e-01, 3.1623e-01],\n",
      "         [1.7783e-02, 3.5566e-02, 5.3348e-02, 7.1131e-02, 8.8914e-02,\n",
      "          1.0670e-01, 1.2448e-01, 1.4226e-01, 1.6005e-01, 1.7783e-01],\n",
      "         [1.0000e-02, 2.0000e-02, 3.0000e-02, 4.0000e-02, 5.0000e-02,\n",
      "          6.0000e-02, 7.0000e-02, 8.0000e-02, 9.0000e-02, 1.0000e-01],\n",
      "         [5.6234e-03, 1.1247e-02, 1.6870e-02, 2.2494e-02, 2.8117e-02,\n",
      "          3.3740e-02, 3.9364e-02, 4.4987e-02, 5.0611e-02, 5.6234e-02],\n",
      "         [3.1623e-03, 6.3246e-03, 9.4868e-03, 1.2649e-02, 1.5811e-02,\n",
      "          1.8974e-02, 2.2136e-02, 2.5298e-02, 2.8461e-02, 3.1623e-02],\n",
      "         [1.7783e-03, 3.5566e-03, 5.3348e-03, 7.1131e-03, 8.8914e-03,\n",
      "          1.0670e-02, 1.2448e-02, 1.4226e-02, 1.6005e-02, 1.7783e-02],\n",
      "         [1.0000e-03, 2.0000e-03, 3.0000e-03, 4.0000e-03, 5.0000e-03,\n",
      "          6.0000e-03, 7.0000e-03, 8.0000e-03, 9.0000e-03, 1.0000e-02],\n",
      "         [5.6234e-04, 1.1247e-03, 1.6870e-03, 2.2494e-03, 2.8117e-03,\n",
      "          3.3740e-03, 3.9364e-03, 4.4987e-03, 5.0611e-03, 5.6234e-03],\n",
      "         [3.1623e-04, 6.3246e-04, 9.4868e-04, 1.2649e-03, 1.5811e-03,\n",
      "          1.8974e-03, 2.2136e-03, 2.5298e-03, 2.8461e-03, 3.1623e-03],\n",
      "         [1.7783e-04, 3.5566e-04, 5.3348e-04, 7.1131e-04, 8.8914e-04,\n",
      "          1.0670e-03, 1.2448e-03, 1.4226e-03, 1.6005e-03, 1.7783e-03]],\n",
      "\n",
      "        [[1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00, 5.0000e+00,\n",
      "          6.0000e+00, 7.0000e+00, 8.0000e+00, 9.0000e+00, 1.0000e+01],\n",
      "         [5.6234e-01, 1.1247e+00, 1.6870e+00, 2.2494e+00, 2.8117e+00,\n",
      "          3.3740e+00, 3.9364e+00, 4.4987e+00, 5.0611e+00, 5.6234e+00],\n",
      "         [3.1623e-01, 6.3246e-01, 9.4868e-01, 1.2649e+00, 1.5811e+00,\n",
      "          1.8974e+00, 2.2136e+00, 2.5298e+00, 2.8460e+00, 3.1623e+00],\n",
      "         [1.7783e-01, 3.5566e-01, 5.3348e-01, 7.1131e-01, 8.8914e-01,\n",
      "          1.0670e+00, 1.2448e+00, 1.4226e+00, 1.6005e+00, 1.7783e+00],\n",
      "         [1.0000e-01, 2.0000e-01, 3.0000e-01, 4.0000e-01, 5.0000e-01,\n",
      "          6.0000e-01, 7.0000e-01, 8.0000e-01, 9.0000e-01, 1.0000e+00],\n",
      "         [5.6234e-02, 1.1247e-01, 1.6870e-01, 2.2494e-01, 2.8117e-01,\n",
      "          3.3740e-01, 3.9364e-01, 4.4987e-01, 5.0611e-01, 5.6234e-01],\n",
      "         [3.1623e-02, 6.3246e-02, 9.4868e-02, 1.2649e-01, 1.5811e-01,\n",
      "          1.8974e-01, 2.2136e-01, 2.5298e-01, 2.8460e-01, 3.1623e-01],\n",
      "         [1.7783e-02, 3.5566e-02, 5.3348e-02, 7.1131e-02, 8.8914e-02,\n",
      "          1.0670e-01, 1.2448e-01, 1.4226e-01, 1.6005e-01, 1.7783e-01],\n",
      "         [1.0000e-02, 2.0000e-02, 3.0000e-02, 4.0000e-02, 5.0000e-02,\n",
      "          6.0000e-02, 7.0000e-02, 8.0000e-02, 9.0000e-02, 1.0000e-01],\n",
      "         [5.6234e-03, 1.1247e-02, 1.6870e-02, 2.2494e-02, 2.8117e-02,\n",
      "          3.3740e-02, 3.9364e-02, 4.4987e-02, 5.0611e-02, 5.6234e-02],\n",
      "         [3.1623e-03, 6.3246e-03, 9.4868e-03, 1.2649e-02, 1.5811e-02,\n",
      "          1.8974e-02, 2.2136e-02, 2.5298e-02, 2.8461e-02, 3.1623e-02],\n",
      "         [1.7783e-03, 3.5566e-03, 5.3348e-03, 7.1131e-03, 8.8914e-03,\n",
      "          1.0670e-02, 1.2448e-02, 1.4226e-02, 1.6005e-02, 1.7783e-02],\n",
      "         [1.0000e-03, 2.0000e-03, 3.0000e-03, 4.0000e-03, 5.0000e-03,\n",
      "          6.0000e-03, 7.0000e-03, 8.0000e-03, 9.0000e-03, 1.0000e-02],\n",
      "         [5.6234e-04, 1.1247e-03, 1.6870e-03, 2.2494e-03, 2.8117e-03,\n",
      "          3.3740e-03, 3.9364e-03, 4.4987e-03, 5.0611e-03, 5.6234e-03],\n",
      "         [3.1623e-04, 6.3246e-04, 9.4868e-04, 1.2649e-03, 1.5811e-03,\n",
      "          1.8974e-03, 2.2136e-03, 2.5298e-03, 2.8461e-03, 3.1623e-03],\n",
      "         [1.7783e-04, 3.5566e-04, 5.3348e-04, 7.1131e-04, 8.8914e-04,\n",
      "          1.0670e-03, 1.2448e-03, 1.4226e-03, 1.6005e-03, 1.7783e-03]]])\n",
      "q torch.Size([2, 8, 10, 32]) cos torch.Size([2, 1, 10, 32])\n",
      "torch.Size([2, 10, 10000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming you have a valid config class and inputs\n",
    "class GemmaConfig:\n",
    "    def __init__(self):\n",
    "        self.hidden_size = 512  # 隐藏层大小\n",
    "        self.intermediate_size = 512\n",
    "        self.num_heads = 8   # 注意力头数\n",
    "        self.head_dim=32\n",
    "        self.num_key_value_heads = 4  # Key-Value头数\n",
    "        self.attention_dropout = 0.1  # Dropout率\n",
    "        self.max_position_embeddings = 1024  # 最大位置嵌入数\n",
    "        self.rope_theta = 10000  # 旋转位置编码的基数\n",
    "        self.attention_bias = False  # 是否使用偏置\n",
    "        self.rms_norm_eps = 0.01\n",
    "        self.intermediate_size=512  # 前馈网络的大小\n",
    "        self.num_hidden_layers=6 # 解码器层的数量\n",
    "        self.vocab_size=10000 # 假设词汇表大小\n",
    "        self.pad_token_id=0  # 填充 token 的 id\n",
    "    \n",
    "\n",
    "text_config = GemmaConfig()\n",
    "\n",
    "# Define a simple config-like class for this example\n",
    "class VisionConfig:\n",
    "    def __init__(self):\n",
    "        self.hidden_size = 512  # Size of the input image features\n",
    "        self.projection_dim = 512  # The target projection dimension\n",
    "        self.image_size = 224\n",
    "        self.patch_size = 16\n",
    "        self.num_channels = 3\n",
    "        self.num_hidden_layers = 2\n",
    "        self.num_attention_heads = 2\n",
    "        self.attention_dropout = 0.9\n",
    "        self.layer_norm_eps = 0.000001\n",
    "        self.intermediate_size = 512\n",
    "        \n",
    "        \n",
    "class PaliGemmaConfig:\n",
    "    def __init__(self):\n",
    "        self.vision_config = VisionConfig()  # Include the vision config\n",
    "        self.text_config = text_config\n",
    "        self.vocab_size = 1200\n",
    "        self.pad_token_id=0  # 填充 token 的 id\n",
    "        self.hidden_size = 512\n",
    "        self.image_token_index = 0\n",
    "\n",
    "        \n",
    "# Initialize the configuration for the model\n",
    "config = PaliGemmaConfig()\n",
    "\n",
    "# Initialize the model\n",
    "model = PaliGemmaForConditionalGeneration(config)\n",
    "\n",
    "# Example input (tokenized text and pixel values for images)\n",
    "input_ids = torch.randint(0, config.vocab_size, (2, 10))  # Batch size of 2, sequence length 10\n",
    "pixel_values = torch.randn(2, 3, 224, 224)  # Batch size 2, 3 channels, 224x224 images\n",
    "attention_mask = torch.ones(2, 10)  # Mask for the input text (all tokens are attended)\n",
    "\n",
    "# Forward pass\n",
    "output = model(input_ids=input_ids, pixel_values=pixel_values, attention_mask=attention_mask)\n",
    "\n",
    "# The output should contain logits for conditional text generation\n",
    "print(output[\"logits\"].shape)  # Example output shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bce1972",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mem0",
   "language": "python",
   "name": "mem0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
