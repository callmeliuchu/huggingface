{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63e1b368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "282de0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /Users/liuchu/.cache/huggingface/modules/datasets_modules/datasets/kde4/243129fb2398d5b0b4f7f6831ab27ad84774b7ce374cf10f60f6e1ff331648ac (last modified on Tue Dec 31 15:44:07 2024) since it couldn't be found locally at kde4, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "raw_dataset = load_dataset('kde4',lang1='en',lang2='zh_CN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18a9d59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = raw_dataset['train'].train_test_split(train_size=0.9,seed=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1886436f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 125699\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 13967\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ec08f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Username:', 'zh_CN': '用户名 ：'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset['train'][7886]['translation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b9f24ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d51ec4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'Helsinki-NLP/opus-mt-en-zh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62361f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuchu/opt/anaconda3/envs/mem0/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfb4fde3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianTokenizer(name_or_path='Helsinki-NLP/opus-mt-en-zh', vocab_size=65001, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t65000: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88b0b37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sentence = split_dataset['train'][3]['translation']['en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5d2ea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_sentence = split_dataset['train'][3]['translation']['zh_CN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f3dc2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(en_sentence,text_target=zh_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d48c59ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [26, 13932, 49644, 36, 17, 3778, 12179, 13, 39382, 1857, 15, 13, 816, 269, 6, 84, 32, 3, 471, 35, 3, 1963, 27139, 131, 26953, 7866, 3778, 6, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [453, 18437, 9470, 1401, 22, 17, 8, 35797, 3793, 673, 3300, 4993, 12, 32891, 19543, 3278, 10, 11560, 35797, 67, 1963, 2926, 1333, 131, 228, 18437, 9470, 1401, 8, 35797, 5051, 8, 10, 0]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d726d058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10984875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁STRING▁()▁函数返回给定数字的字符串值。▁此函数与▁NUM2STRING▁函数相同▁。</s>'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(tokenizer.convert_ids_to_tokens(inputs['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b402d0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### 手动实现transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bdfbc61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6447d033",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim,hidden_dim,output_dim):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear1 = nn.Linear(input_dim,hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim,output_dim)\n",
    "        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c2b6c76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1800cb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((4,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "39e6ee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = FeedForward(5,7,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a749a2e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b4178e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6de5aac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(input_dim)\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.ln(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3bf63c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3836, -1.3330,  0.3668, -0.4173],\n",
       "        [ 0.9285, -0.2232,  0.8387, -1.5441],\n",
       "        [ 0.8393,  0.0920,  0.7285, -1.6599],\n",
       "        [ 1.5067, -0.0065, -0.2004, -1.2999],\n",
       "        [-0.7406, -0.4775,  1.7229, -0.5048]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(5,4)\n",
    "ln = LayerNorm(4)\n",
    "ln(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "72ca7ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim,hidden_dim):\n",
    "        super().__init__()\n",
    "        self.qw = nn.Linear(input_dim,hidden_dim)\n",
    "        self.kw = nn.Linear(input_dim,hidden_dim)\n",
    "        self.vw = nn.Linear(input_dim,hidden_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        ## B,T,C\n",
    "        B,T,C = x.shape\n",
    "        q = self.qw(x)\n",
    "        k = self.kw(x)\n",
    "        v = self.vw(x)\n",
    "        print(q.shape,k.shape,k.T.shape)\n",
    "        att = q @ k.permute(0,2,1)\n",
    "#         att = att.masked_fill(mask, value)\n",
    "        att = F.softmax(att,dim=-1)\n",
    "        v = att @ v\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3d0a0ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "36f4a81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "att = Attention(4,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c76d7891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 6]) torch.Size([5, 3, 6]) torch.Size([6, 3, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 6])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "c14f3c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim,head_size,hidden_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.qw = nn.Linear(input_dim,head_size * hidden_size)\n",
    "        self.kw = nn.Linear(input_dim,head_size * hidden_size)\n",
    "        self.vw = nn.Linear(input_dim,head_size * hidden_size)\n",
    "    \n",
    "    def qkv(self,x):\n",
    "        B,T,C = x.shape\n",
    "        q = self.qw(x).reshape(B,T,self.head_size,self.hidden_size).permute(0,2,1,3) # B,head_size,T,hidden_size\n",
    "        k = self.kw(x).reshape(B,T,self.head_size,self.hidden_size).permute(0,2,1,3)\n",
    "        v = self.vw(x).reshape(B,T,self.head_size,self.hidden_size).permute(0,2,1,3)\n",
    "        return q,k,v\n",
    "    \n",
    "    def forward(self,q,k,v):\n",
    "        #### q ===> B,head_size,T,hidden_size\n",
    "        B,head_size,T,hidden_size = q.shape\n",
    "        att = q @ k.permute(0,1,3,2) # B,head_size,T,T\n",
    "        att = F.softmax(att,dim=-1)\n",
    "        v = att @ v  # B,head_size,T,hidden_size\n",
    "#         v = v.permute(0,2,1,3) # B,T,head_size,hidden_size\n",
    "#         v = v.reshape(B,T,self.head_size * self.hidden_size)\n",
    "        return v       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "820ad105",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5,4,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "57a93d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "att = MultiHeadAttention(3,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "1bdb1002",
   "metadata": {},
   "outputs": [],
   "source": [
    "q,k,v = att.qkv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "157a11f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 2, 4, 3]), torch.Size([5, 2, 4, 3]), torch.Size([5, 2, 4, 3]))"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape,k.shape,v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "758c71fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 4, 3])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att(q,k,v).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "a31a3fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim,head_size,hidden_dim):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(input_dim,head_size,hidden_dim)\n",
    "        self.ln1 = LayerNorm(hidden_dim)\n",
    "        self.fd = FeedForward(hidden_dim,hidden_dim,hidden_dim)\n",
    "        self.ln2 = LayerNorm(hidden_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        q,k,v = self.mha.qkv(x)\n",
    "        x = q + self.mha(q,k,v)\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.fd(x)\n",
    "        x = self.ln2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "9226efc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "block = EncoderBlock(4,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "d3893364",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "9dfc4fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "18fafd88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 3, 2])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "b40c8988",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim,head_size,hidden_dim):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(input_dim,head_size,hidden_dim)\n",
    "        self.ln1 = LayerNorm(hidden_dim)\n",
    "        self.fd = FeedForward(input_dim,hidden_dim,input_dim)\n",
    "        self.ln2 = LayerNorm(hidden_dim)\n",
    "        self.mha2 = MultiHeadAttention(hidden_dim,head_size,hidden_dim)\n",
    "        self.fd2 = FeedForward(hidden_dim,hidden_dim,hidden_dim)\n",
    "        self.ln3 = LayerNorm(hidden_dim)\n",
    "        \n",
    "    def forward(self,x,k,v):\n",
    "        x,k0,v0 = self.mha.qkv(x) ### 需要masked\n",
    "        x = x + self.mha(x,k,v)\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.mha2(x,k,v) ### cross attention\n",
    "        x = self.ln2(x)\n",
    "        x = x + self.fd2(x)\n",
    "        x = self.ln3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "0de6e170",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoderblock = DecoderBlock(4,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "8ac6dad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 4])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "40ef981b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 3, 2])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoderblock(x,v,v).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8085383",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41189173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cf4eed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8c1e1a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self,n,input_dim,head_size,hidden_dim,input_vocab_size,output_vocab_size):\n",
    "        super().__init__()\n",
    "        self.encoder_blocks = nn.ModuleList(\n",
    "           [EncoderBlock(input_dim,head_size,hidden_dim) for _ in range(n)]\n",
    "        )\n",
    "        self.decoder_blocks = nn.ModuleList(\n",
    "           [DecoderBlock(input_dim,head_size,hidden_dim)   for _ in range(n)]\n",
    "        )\n",
    "        self.input_embeddings = nn.Embedding(input_vocab_size,input_dim)\n",
    "        self.output_embeddings = nn.Embedding(output_vocab_size,input_dim)\n",
    "        self.output_linear = nn.Linear(head_size * hidden_dim,output_vocab_size)\n",
    "        \n",
    "    \n",
    "    def forward(self,x,y):\n",
    "        #### x ==> B,T\n",
    "        x = self.input_embeddings(x) ### B,T,C\n",
    "        x = self.encoder_blocks(x)  ### B,head_size,T,hidden_size\n",
    "        y = self.input_embeddings(y) ### B,T,C\n",
    "        y = self.decoder_blocks(y,x,x) ### B,head_size,T,hidden_size\n",
    "        B,head_size,T,hidden_size = y.shape\n",
    "        y = y.permute(0,2,1,3) ## B,T,head_size,hidden_size\n",
    "        y = y.reshape(B,T,-1)\n",
    "        logits = self.output_linear(y) # B,T,output_vocab_size\n",
    "    \n",
    "    def predict(self,x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b4d501",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mem0",
   "language": "python",
   "name": "mem0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
