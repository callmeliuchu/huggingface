{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81841ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c94a9fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /Users/liuchu/.cache/huggingface/modules/datasets_modules/datasets/kde4/243129fb2398d5b0b4f7f6831ab27ad84774b7ce374cf10f60f6e1ff331648ac (last modified on Tue Dec 31 15:44:07 2024) since it couldn't be found locally at kde4, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "raw_dataset = load_dataset('kde4',lang1='en',lang2='zh_CN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbe7aaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = raw_dataset['train'].train_test_split(train_size=0.9,seed=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24bc9461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 125699\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 13967\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1ffcccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Username:', 'zh_CN': '用户名 ：'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset['train'][7886]['translation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9abcab57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cebd4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'Helsinki-NLP/opus-mt-en-zh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b348663",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuchu/opt/anaconda3/envs/mem0/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0d03ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianTokenizer(name_or_path='Helsinki-NLP/opus-mt-en-zh', vocab_size=65001, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t65000: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53ea00d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sentence = split_dataset['train'][3]['translation']['en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5719602",
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_sentence = split_dataset['train'][3]['translation']['zh_CN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c5f9414",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(en_sentence,text_target=zh_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e9206ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [26, 13932, 49644, 36, 17, 3778, 12179, 13, 39382, 1857, 15, 13, 816, 269, 6, 84, 32, 3, 471, 35, 3, 1963, 27139, 131, 26953, 7866, 3778, 6, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [453, 18437, 9470, 1401, 22, 17, 8, 35797, 3793, 673, 3300, 4993, 12, 32891, 19543, 3278, 10, 11560, 35797, 67, 1963, 2926, 1333, 131, 228, 18437, 9470, 1401, 8, 35797, 5051, 8, 10, 0]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ee5e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c1609f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁STRING▁()▁函数返回给定数字的字符串值。▁此函数与▁NUM2STRING▁函数相同▁。</s>'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(tokenizer.convert_ids_to_tokens(inputs['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "17c7deae",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### 手动实现transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "16ce5a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "533f960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim,hidden_dim,output_dim):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear1 = nn.Linear(input_dim,hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim,output_dim)\n",
    "        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3b57f6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bb13ed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((4,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f931e4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = FeedForward(5,7,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ece1869b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "557ee179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "43af3335",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(input_dim)\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.ln(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a2184d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3836, -1.3330,  0.3668, -0.4173],\n",
       "        [ 0.9285, -0.2232,  0.8387, -1.5441],\n",
       "        [ 0.8393,  0.0920,  0.7285, -1.6599],\n",
       "        [ 1.5067, -0.0065, -0.2004, -1.2999],\n",
       "        [-0.7406, -0.4775,  1.7229, -0.5048]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(5,4)\n",
    "ln = LayerNorm(4)\n",
    "ln(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ff605ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim,hidden_dim):\n",
    "        super().__init__()\n",
    "        self.qw = nn.Linear(input_dim,hidden_dim)\n",
    "        self.kw = nn.Linear(input_dim,hidden_dim)\n",
    "        self.vw = nn.Linear(input_dim,hidden_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        ## B,T,C\n",
    "        B,T,C = x.shape\n",
    "        q = self.qw(x)\n",
    "        k = self.kw(x)\n",
    "        v = self.vw(x)\n",
    "        print(q.shape,k.shape,k.T.shape)\n",
    "        att = q @ k.permute(0,2,1)\n",
    "#         att = att.masked_fill(mask, value)\n",
    "        att = F.softmax(att,dim=-1)\n",
    "        v = att @ v\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d077974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d920fb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "att = Attention(4,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ee8e2087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 6]) torch.Size([5, 3, 6]) torch.Size([6, 3, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 6])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "b38dcea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim,head_size,hidden_size):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self,q,k,v):\n",
    "        #### q ===> B,head_size,T,hidden_size\n",
    "        B,head_size,T,hidden_size = q.shape\n",
    "        att = q @ k.permute(0,1,3,2) # B,head_size,T,T\n",
    "        att = F.softmax(att,dim=-1)\n",
    "        v = att @ v  # B,head_size,T,hidden_size\n",
    "#         v = v.permute(0,2,1,3) # B,T,head_size,hidden_size\n",
    "#         v = v.reshape(B,T,self.head_size * self.hidden_size)\n",
    "        return v       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "e09ed0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5,4,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "528dde47",
   "metadata": {},
   "outputs": [],
   "source": [
    "att = MultiHeadAttention(3,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "447241c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "q,k,v = att.qkv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "0acd0ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 2, 4, 3]), torch.Size([5, 2, 4, 3]), torch.Size([5, 2, 4, 3]))"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape,k.shape,v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "dab55d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 4, 3])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att(q,k,v).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "b67e34c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim,head_size,hidden_dim):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(input_dim,head_size,hidden_dim)\n",
    "        self.ln1 = LayerNorm(hidden_dim)\n",
    "        self.fd = FeedForward(hidden_dim,hidden_dim,hidden_dim)\n",
    "        self.ln2 = LayerNorm(hidden_dim)\n",
    "    \n",
    "    def forward(self,q,k,v):\n",
    "        x = q + self.mha(q,k,v)\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.fd(x)\n",
    "        x = self.ln2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "77cb8227",
   "metadata": {},
   "outputs": [],
   "source": [
    "block = EncoderBlock(4,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "2429802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "7e86e64a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[261], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mem0/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mem0/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[258], line 11\u001b[0m, in \u001b[0;36mEncoderBlock.forward\u001b[0;34m(self, q, k, v)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,q,k,v):\n\u001b[0;32m---> 11\u001b[0m     x \u001b[38;5;241m=\u001b[39m q \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmha\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(x)\n\u001b[1;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfd(x)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mem0/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mem0/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[248], line 8\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, q, k, v)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,q,k,v):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m#### q ===> B,head_size,T,hidden_size\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     B,head_size,T,hidden_size \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m      9\u001b[0m     att \u001b[38;5;241m=\u001b[39m q \u001b[38;5;241m@\u001b[39m k\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# B,head_size,T,T\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     att \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(att,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 3)"
     ]
    }
   ],
   "source": [
    "v = block(x,x,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "0e7f3dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 3, 2])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "36be857f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim,head_size,hidden_dim):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(input_dim,head_size,hidden_dim)\n",
    "        self.ln1 = LayerNorm(hidden_dim)\n",
    "        self.fd = FeedForward(input_dim,hidden_dim,input_dim)\n",
    "        self.ln2 = LayerNorm(hidden_dim)\n",
    "        self.mha2 = MultiHeadAttention(hidden_dim,head_size,hidden_dim)\n",
    "        self.fd2 = FeedForward(hidden_dim,hidden_dim,hidden_dim)\n",
    "        self.ln3 = LayerNorm(hidden_dim)\n",
    "        \n",
    "    def forward(self,x,k,v):\n",
    "#         x,k0,v0 = self.mha.qkv(x) ### 需要masked\n",
    "        x = x + self.mha(x,k,v)\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.mha2(x,k,v) ### cross attention\n",
    "        x = self.ln2(x)\n",
    "        x = x + self.fd2(x)\n",
    "        x = self.ln3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "da7d217d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoderblock = DecoderBlock(4,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "5666526e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 4])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "b347b205",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[265], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdecoderblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mem0/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mem0/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[262], line 15\u001b[0m, in \u001b[0;36mDecoderBlock.forward\u001b[0;34m(self, x, k, v)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x,k,v):\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#         x,k0,v0 = self.mha.qkv(x) ### 需要masked\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m         x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmha\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(x)\n\u001b[1;32m     17\u001b[0m         x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmha2(x,k,v) \u001b[38;5;66;03m### cross attention\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mem0/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mem0/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[248], line 8\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, q, k, v)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,q,k,v):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m#### q ===> B,head_size,T,hidden_size\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     B,head_size,T,hidden_size \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m      9\u001b[0m     att \u001b[38;5;241m=\u001b[39m q \u001b[38;5;241m@\u001b[39m k\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# B,head_size,T,T\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     att \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(att,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 3)"
     ]
    }
   ],
   "source": [
    "decoderblock(x,v,v).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e857977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bf6aea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afef0abf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "df475d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self,n,input_dim,head_size,hidden_dim,input_vocab_size,output_vocab_size):\n",
    "        super().__init__()\n",
    "        self.encoder_blocks = nn.ModuleList(\n",
    "           [EncoderBlock(input_dim,head_size,hidden_dim) for _ in range(n)]\n",
    "        )\n",
    "        self.decoder_blocks = nn.ModuleList(\n",
    "           [DecoderBlock(input_dim,head_size,hidden_dim)   for _ in range(n)]\n",
    "        )\n",
    "        self.input_embeddings = nn.Embedding(input_vocab_size,input_dim)\n",
    "        self.output_embeddings = nn.Embedding(output_vocab_size,input_dim)\n",
    "        self.output_linear = nn.Linear(head_size * hidden_dim,output_vocab_size)\n",
    "        \n",
    "        self.head_size = head_size\n",
    "        self.hidden_size = hidden_dim\n",
    "        self.qw = nn.Linear(input_dim,head_size * hidden_dim)\n",
    "        self.kw = nn.Linear(input_dim,head_size * hidden_dim)\n",
    "        self.vw = nn.Linear(input_dim,head_size * hidden_dim)\n",
    "    \n",
    "    def qkv(self,x):\n",
    "        B,T,C = x.shape\n",
    "        q = self.qw(x).reshape(B,T,self.head_size,self.hidden_size).permute(0,2,1,3) # B,head_size,T,hidden_size\n",
    "        k = self.kw(x).reshape(B,T,self.head_size,self.hidden_size).permute(0,2,1,3)\n",
    "        v = self.vw(x).reshape(B,T,self.head_size,self.hidden_size).permute(0,2,1,3)\n",
    "        return q,k,v\n",
    "        \n",
    "    \n",
    "    def forward(self,x,y):\n",
    "        #### x ==> B,T\n",
    "        x = self.input_embeddings(x) ### B,T,C\n",
    "        x,k,v = self.qkv(x)\n",
    "        for block in self.encoder_blocks:\n",
    "            x = block(x,k,v)  ### B,head_size,T,hidden_size\n",
    "        y = self.input_embeddings(y) ### B,T,C\n",
    "        y,k,v = self.qkv(y)\n",
    "        for block in self.decoder_blocks:\n",
    "            y = block(y,x,x) ### B,head_size,T,hidden_size\n",
    "        B,head_size,T,hidden_size = y.shape\n",
    "        y = y.permute(0,2,1,3) ## B,T,head_size,hidden_size\n",
    "        y = y.reshape(B,T,-1)\n",
    "        logits = self.output_linear(y) # B,T,output_vocab_size\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "a9f0bb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "input_dim = 4\n",
    "head_size = 2\n",
    "hidden_dim = input_dim // head_size\n",
    "input_vocab_size = 10\n",
    "output_vocab_size = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "246e7fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(n,input_dim,head_size,hidden_dim,input_vocab_size,output_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "ef28c2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.LongTensor([\n",
    "    [0,1,3],\n",
    "    [0,2,3]\n",
    "])\n",
    "y = torch.LongTensor([\n",
    "    [1,3,4,5],\n",
    "    [2,3,4,6]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "addf30d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 15])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer(x,y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482455fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bbb588",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mem0",
   "language": "python",
   "name": "mem0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
