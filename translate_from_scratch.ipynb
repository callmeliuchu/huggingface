{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf5d9858",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b384240e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /Users/liuchu/.cache/huggingface/modules/datasets_modules/datasets/kde4/243129fb2398d5b0b4f7f6831ab27ad84774b7ce374cf10f60f6e1ff331648ac (last modified on Tue Dec 31 15:44:07 2024) since it couldn't be found locally at kde4, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "raw_dataset = load_dataset('kde4',lang1='en',lang2='zh_CN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17f31b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = raw_dataset['train'].train_test_split(train_size=0.9,seed=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e7c8eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 125699\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 13967\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b65ef14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Username:', 'zh_CN': '用户名 ：'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset['train'][7886]['translation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b59ee815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d0b951c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'Helsinki-NLP/opus-mt-en-zh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8814d0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuchu/opt/anaconda3/envs/mem0/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb516565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianTokenizer(name_or_path='Helsinki-NLP/opus-mt-en-zh', vocab_size=65001, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t65000: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0485f34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sentence = split_dataset['train'][3]['translation']['en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5fab0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_sentence = split_dataset['train'][3]['translation']['zh_CN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c72063b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(en_sentence,text_target=zh_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f883654d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [26, 13932, 49644, 36, 17, 3778, 12179, 13, 39382, 1857, 15, 13, 816, 269, 6, 84, 32, 3, 471, 35, 3, 1963, 27139, 131, 26953, 7866, 3778, 6, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [453, 18437, 9470, 1401, 22, 17, 8, 35797, 3793, 673, 3300, 4993, 12, 32891, 19543, 3278, 10, 11560, 35797, 67, 1963, 2926, 1333, 131, 228, 18437, 9470, 1401, 8, 35797, 5051, 8, 10, 0]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9813dd9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d90102a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁STRING▁()▁函数返回给定数字的字符串值。▁此函数与▁NUM2STRING▁函数相同▁。</s>'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(tokenizer.convert_ids_to_tokens(inputs['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "30a37fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### 手动实现transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ce46cccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2627b5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim,hidden_dim,output_dim):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear1 = nn.Linear(input_dim,hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim,output_dim)\n",
    "        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c6dd23bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cb3091cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((4,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b7c82e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = FeedForward(5,7,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1152851d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3fc955d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4d5a90c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(input_dim)\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.ln(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "829a6004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3836, -1.3330,  0.3668, -0.4173],\n",
       "        [ 0.9285, -0.2232,  0.8387, -1.5441],\n",
       "        [ 0.8393,  0.0920,  0.7285, -1.6599],\n",
       "        [ 1.5067, -0.0065, -0.2004, -1.2999],\n",
       "        [-0.7406, -0.4775,  1.7229, -0.5048]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(5,4)\n",
    "ln = LayerNorm(4)\n",
    "ln(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1e0c25e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim,hidden_dim):\n",
    "        super().__init__()\n",
    "        self.qw = nn.Linear(input_dim,hidden_dim)\n",
    "        self.kw = nn.Linear(input_dim,hidden_dim)\n",
    "        self.vw = nn.Linear(input_dim,hidden_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        ## B,T,C\n",
    "        B,T,C = x.shape\n",
    "        q = self.qw(x)\n",
    "        k = self.kw(x)\n",
    "        v = self.vw(x)\n",
    "        print(q.shape,k.shape,k.T.shape)\n",
    "        att = q @ k.permute(0,2,1)\n",
    "#         att = att.masked_fill(mask, value)\n",
    "        att = F.softmax(att,dim=-1)\n",
    "        v = att @ v\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7bb421f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d2d4bd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "att = Attention(4,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "48bbf1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 6]) torch.Size([5, 3, 6]) torch.Size([6, 3, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 6])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "id": "23a51408",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim,head_size,hidden_size):\n",
    "        super().__init__()        \n",
    "        self.head_size = head_size\n",
    "        self.hidden_size = hidden_dim\n",
    "        self.qw = nn.Linear(input_dim,head_size * hidden_dim)\n",
    "        self.kw = nn.Linear(input_dim,head_size * hidden_dim)\n",
    "        self.vw = nn.Linear(input_dim,head_size * hidden_dim)\n",
    "        \n",
    "    def forward(self,q,k,v,masked=False):\n",
    "        #### q ==> B,T,C\n",
    "        q = self.qw(q)\n",
    "        k = self.kw(k)\n",
    "        v = self.vw(v)\n",
    "        #### q ===> B,head_size,T,hidden_size\n",
    "        B,T,C = q.shape\n",
    "        q = q.reshape(B,T,self.head_size,self.hidden_size).permute(0,2,1,3)\n",
    "        B,T,C = k.shape\n",
    "        k = k.reshape(B,T,self.head_size,self.hidden_size).permute(0,2,1,3)\n",
    "        B,T,C = v.shape\n",
    "        v = v.reshape(B,T,self.head_size,self.hidden_size).permute(0,2,1,3)\n",
    "        B,head_size,T,hidden_size = q.shape\n",
    "        att = q @ k.permute(0,1,3,2) # B,head_size,T,T\n",
    "        if masked:\n",
    "            _,_,m,n = att.shape\n",
    "            mask = torch.ones(m,n)\n",
    "            mask = torch.tril(mask)\n",
    "            att = att.masked_fill(mask==0,float('-inf'))\n",
    "        att = F.softmax(att,dim=-1)\n",
    "        v = att @ v  # B,head_size,T,hidden_size\n",
    "        v = v.permute(0,2,1,3) # B,T,head_size,hidden_size\n",
    "        v = v.reshape(B,T,self.head_size * self.hidden_size)\n",
    "        return v       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "id": "a3f8a197",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5,4,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "id": "4ec8dbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "att1 = MultiHeadAttention(3,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc938d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "id": "039f0b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "att = torch.randn(1,1,2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "id": "75adb49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.2754, -1.5449,  0.9508, -0.7407],\n",
       "          [ 0.6911,  0.5579, -0.8990, -1.2444]]]])"
      ]
     },
     "execution_count": 764,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "id": "d912815a",
   "metadata": {},
   "outputs": [],
   "source": [
    " _,_,m,n = att.shape\n",
    "mask = torch.ones(m,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "id": "76b93428",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.tril(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "id": "d67822f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True],\n",
       "        [False, False,  True,  True]])"
      ]
     },
     "execution_count": 767,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "id": "a33b1a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "att = att.masked_fill(mask==0,float('-inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "id": "0997bd0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.5333, 0.4667, 0.0000, 0.0000]]]])"
      ]
     },
     "execution_count": 769,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(att,dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "id": "8d7f25d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[770], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43matt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Tensor' object is not callable"
     ]
    }
   ],
   "source": [
    "att(x,x,x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "id": "d20f6130",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim,head_size,hidden_dim):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(input_dim,head_size,hidden_dim)\n",
    "        self.ln1 = LayerNorm(input_dim)\n",
    "        self.fd = FeedForward(input_dim,hidden_dim,input_dim)\n",
    "        self.ln2 = LayerNorm(input_dim)\n",
    "    \n",
    "    def forward(self,q,k,v):\n",
    "        x = q + self.mha(q,k,v)\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.fd(x)\n",
    "        x = self.ln2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "id": "4c4c7c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "block = EncoderBlock(4,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "id": "700a9dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee38b35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ad4271",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "id": "8dc510c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim,head_size,hidden_dim):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(input_dim,head_size,hidden_dim)\n",
    "        self.ln1 = LayerNorm(input_dim)\n",
    "        self.fd = FeedForward(input_dim,hidden_dim,input_dim)\n",
    "        self.ln2 = LayerNorm(input_dim)\n",
    "        self.mha2 = MultiHeadAttention(input_dim,head_size,hidden_dim)\n",
    "        self.fd2 = FeedForward(input_dim,hidden_dim,input_dim)\n",
    "        self.ln3 = LayerNorm(input_dim)\n",
    "        \n",
    "    def forward(self,x,k,v):\n",
    "#         x,k0,v0 = self.mha.qkv(x) ### 需要masked\n",
    "        x = x + self.mha(x,k,v)\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.mha2(x,k,v) ### cross attention\n",
    "        x = self.ln2(x)\n",
    "        x = x + self.fd2(x)\n",
    "        x = self.ln3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d5272e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023ccea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb46eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b91eb66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9306fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab14816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "id": "bf1352c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self,n,input_dim,head_size,hidden_dim,input_vocab_size,output_vocab_size):\n",
    "        super().__init__()\n",
    "        self.encoder_blocks = nn.ModuleList(\n",
    "           [EncoderBlock(input_dim,head_size,hidden_dim) for _ in range(n)]\n",
    "        )\n",
    "        self.decoder_blocks = nn.ModuleList(\n",
    "           [DecoderBlock(input_dim,head_size,hidden_dim)   for _ in range(n)]\n",
    "        )\n",
    "        self.input_embeddings = nn.Embedding(input_vocab_size,input_dim)\n",
    "        self.output_embeddings = nn.Embedding(output_vocab_size,input_dim)\n",
    "        self.output_linear = nn.Linear(head_size * hidden_dim,output_vocab_size)\n",
    "        self.input_pos_embedding = nn.Embedding(1024,input_dim)\n",
    "        self.output_pos_embedding = nn.Embedding(1024,input_dim)\n",
    "        \n",
    "\n",
    "    def forward(self,x,y):\n",
    "        #### x ==> B,T\n",
    "        B,T = x.shape\n",
    "        x = self.input_embeddings(x) ### B,T,C\n",
    "        x_pos = self.input_pos_embedding(torch.arange(T))\n",
    "        x = x + x_pos\n",
    "        for block in self.encoder_blocks:\n",
    "            x = block(x,x,x)  ### B,head_size,T,hidden_size\n",
    "        B,T = y.shape\n",
    "        y = self.output_embeddings(y) ### B,T,C\n",
    "        y_pos = self.output_pos_embedding(torch.arange(T))\n",
    "        y = y + y_pos\n",
    "        for block in self.decoder_blocks:\n",
    "            y = block(y,x,x) ### B,head_size,T,hidden_size\n",
    "        logits = self.output_linear(y) # B,T,output_vocab_size\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "id": "87b61687",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "input_dim = 4\n",
    "head_size = 2\n",
    "hidden_dim = input_dim // head_size\n",
    "input_vocab_size = 10\n",
    "output_vocab_size = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "id": "9143d7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(n,input_dim,head_size,hidden_dim,input_vocab_size,output_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "id": "80fd7651",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.LongTensor([\n",
    "    [0,1,3],\n",
    "    [0,2,3]\n",
    "])\n",
    "y = torch.LongTensor([\n",
    "    [1,3,4,5],\n",
    "    [2,3,4,6]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "id": "039b4f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 15])"
      ]
     },
     "execution_count": 899,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer(x,y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "id": "b5bd4b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    {\"input\":\"In the heart of the city, there is a beautiful park where people can enjoy nature and relax after a long day of work.\",\"output\":\"在城市的中心，有一座美丽的公园，人们可以在那里享受自然，在漫长的工作日后放松身心。\"},\n",
    "    {\"input\":\"The rapid development of technology has brought about significant changes in our daily lives, making it more convenient for us to communicate and access information.\",\"output\":\"科技的快速发展给我们的日常生活带来了巨大的变化，使我们能够更方便地进行沟通和获取信息。\"},\n",
    "    {\"input\":\"Despite the challenges we face, we should always maintain an optimistic attitude towards life, believing that every problem has a solution.\",\"output\":\"尽管我们面临挑战，但我们应该始终保持对生活的乐观态度，相信每个问题都有解决方案。\"},\n",
    "    {\"input\":\"With the increasing popularity of online shopping, more and more people are choosing to purchase goods through the internet, which not only saves time but also offers a wider range of choices.\",\"output\":\"随着网上购物的日益普及，越来越多的人选择通过互联网购买商品，这不仅节省了时间，还提供了更广泛的选择。\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "id": "e6fd84b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \n",
    "    def __init__(self,sentences):\n",
    "        self.vocab_set = set()\n",
    "        for sentence in sentences:\n",
    "            self.vocab_set.update(sentence)\n",
    "        self.vocab_set = list(self.vocab_set)\n",
    "        self.vocab_set = ['<pad>','<bos>','<eos>'] + self.vocab_set\n",
    "        self.token2id = {c:i for i,c in enumerate(self.vocab_set)}\n",
    "        self.id2token = {i:c for c,i in self.token2id.items()}\n",
    "    \n",
    "    def convert_token_to_id(self,tokens):\n",
    "        return [self.token2id.get(t,'') for t in tokens]\n",
    "    \n",
    "    def convert_id_to_token(self,ids):\n",
    "        return [self.id2token.get(i,-1) for i in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "id": "979fdbc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In the heart of the city, there is a beautiful park where people can enjoy nature and relax after a long day of work.',\n",
       " 'The rapid development of technology has brought about significant changes in our daily lives, making it more convenient for us to communicate and access information.',\n",
       " 'Despite the challenges we face, we should always maintain an optimistic attitude towards life, believing that every problem has a solution.',\n",
       " 'With the increasing popularity of online shopping, more and more people are choosing to purchase goods through the internet, which not only saves time but also offers a wider range of choices.']"
      ]
     },
     "execution_count": 902,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[d['input'] for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "id": "8a675055",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokenizer = Tokenizer([d['input'] for d in dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "id": "32a3a493",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tokenizer = Tokenizer([d['output'] for d in dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "id": "83319b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', '<bos>', '<eos>', '每', '后', '更']"
      ]
     },
     "execution_count": 905,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tokenizer.convert_id_to_token([0,1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "id": "a759600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(sentences,tokenizer,max_length,is_output=False):\n",
    "    res = []\n",
    "    for sentence in sentences:\n",
    "        arr = tokenizer.convert_token_to_id(sentence)\n",
    "        if is_output:\n",
    "            arr = tokenizer.convert_token_to_id(['<bos>']) + arr + tokenizer.convert_token_to_id(['<eos>'])\n",
    "        if len(arr) > max_length:\n",
    "            arr = arr[:max_length]\n",
    "        else:\n",
    "            arr = arr + tokenizer.convert_token_to_id(['<pad>']) * (max_length - len(arr))\n",
    "        res.append(arr)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "id": "7f0ac8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [d['input'] for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "id": "1c0d565e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In the heart of the city, there is a beautiful park where people can enjoy nature and relax after a long day of work.',\n",
       " 'The rapid development of technology has brought about significant changes in our daily lives, making it more convenient for us to communicate and access information.',\n",
       " 'Despite the challenges we face, we should always maintain an optimistic attitude towards life, believing that every problem has a solution.',\n",
       " 'With the increasing popularity of online shopping, more and more people are choosing to purchase goods through the internet, which not only saves time but also offers a wider range of choices.']"
      ]
     },
     "execution_count": 908,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d437859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "id": "52296be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 909,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokenizer.convert_token_to_id(['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4263abd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "id": "97d23df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = process([d['input'] for d in dataset],input_tokenizer,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "id": "63b52489",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = process([d['output'] for d in dataset],output_tokenizer,30,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ecd93b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 912,
   "id": "a4c81886",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.LongTensor(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 913,
   "id": "7ed15993",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.LongTensor(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 928,
   "id": "f2a93cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "input_dim = 1600\n",
    "head_size = 40\n",
    "hidden_dim = input_dim // head_size\n",
    "input_vocab_size = len(input_tokenizer.id2token)\n",
    "output_vocab_size = len(output_tokenizer.id2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "id": "977eb9e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 929,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "id": "6f4f707f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 930,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 931,
   "id": "d3671cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(n,input_dim,head_size,hidden_dim,input_vocab_size,output_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 932,
   "id": "95aee4c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 30, 129])"
      ]
     },
     "execution_count": 932,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer(x,y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "id": "de488600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 934,
   "id": "0db6fcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = AdamW(transformer.parameters(),lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 935,
   "id": "10fd1987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建交叉熵损失函数\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "id": "ce3e86fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.0338, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.7059, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.1730, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.0553, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.3269, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.8729, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.3969, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.6215, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.7034, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.7398, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.6002, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.4396, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.3256, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.2670, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.2502, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.2205, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.1258, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.0079, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.9277, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8923, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8890, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.9150, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.9494, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.9436, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8963, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8505, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8270, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8230, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8313, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8432, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8486, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8437, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8342, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8255, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8191, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8144, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8095, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8052, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8043, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8062, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8076, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8067, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8033, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8003, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7990, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7979, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7974, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7973, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7970, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7964, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7953, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7945, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7940, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7937, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7934, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7929, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7925, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7920, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7918, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7916, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7916, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7915, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7915, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7911, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7906, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7903, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7903, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7904, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7905, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7905, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7903, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7900, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7898, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7897, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7898, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7898, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7899, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7898, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7897, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7895, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7894, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7895, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7895, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7895, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7895, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7894, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7894, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7893, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7893, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7893, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7893, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7893, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7893, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7893, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7892, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7892, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7892, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7892, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7892, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7892, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7892, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7892, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7892, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7892, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7891, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7891, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7891, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7891, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7891, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7891, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7891, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7891, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7891, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7891, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7891, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7891, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7891, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7891, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7891, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7891, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7890, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7890, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7890, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7890, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7890, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7890, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7890, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7890, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7890, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7890, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7890, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7890, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7890, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7890, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7890, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7890, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7890, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7890, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7890, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7890, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7890, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7890, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7890, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7890, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7889, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7889, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7889, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7889, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7889, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7889, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7889, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7889, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7889, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7889, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7889, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7889, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7889, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7889, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7889, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7889, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7889, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7889, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7889, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7889, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7889, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7889, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7889, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7889, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7889, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7889, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7889, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7889, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7889, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7889, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7889, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7889, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7889, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7889, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7888, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7887, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7887, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7887, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7887, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7887, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[936], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     11\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 12\u001b[0m \u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mem0/lib/python3.10/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mem0/lib/python3.10/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mem0/lib/python3.10/site-packages/torch/optim/adamw.py:220\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    207\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    209\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    210\u001b[0m         group,\n\u001b[1;32m    211\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m         state_steps,\n\u001b[1;32m    218\u001b[0m     )\n\u001b[0;32m--> 220\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mem0/lib/python3.10/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mem0/lib/python3.10/site-packages/torch/optim/adamw.py:782\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    780\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 782\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mem0/lib/python3.10/site-packages/torch/optim/adamw.py:427\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    425\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m         denom \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    429\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for _ in range(1000):\n",
    "    y_inputs = y[:,:-1]\n",
    "    y_targets = y[:,1:]\n",
    "    logits = transformer(x,y_inputs)\n",
    "    B,T = y_targets.shape\n",
    "    # 计算损失\n",
    "    loss = criterion(logits.reshape(B*T,-1), y_targets.reshape(B*T))\n",
    "    print(loss)\n",
    "    \n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 937,
   "id": "9ebad4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,inputs):\n",
    "    ids = input_tokenizer.convert_token_to_id(inputs)\n",
    "    x = torch.LongTensor([ids])\n",
    "    y = torch.LongTensor([output_tokenizer.convert_token_to_id(['<bos>'])])\n",
    "    for _ in range(100):\n",
    "        logits = model(x,y)\n",
    "        ### logits B,T,vocab_size\n",
    "        logits = logits[:,-1,:]\n",
    "        ### logits B,T,vocab_size\n",
    "        predicts = logits.argmax(dim=-1,keepdim=True) # B,1\n",
    "        y = torch.cat((y,predicts),dim=-1)\n",
    "    print(y.shape)\n",
    "    for b in range(y.shape[0]):\n",
    "        for i in y[b]:\n",
    "            print(output_tokenizer.convert_id_to_token([int(i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 938,
   "id": "9a2e580f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 101])\n",
      "['<bos>']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n",
      "['的']\n"
     ]
    }
   ],
   "source": [
    "predict(transformer,'The rapid development of technology has brought about significant changes in our daily lives, making it more convenient for us to communicate and access information.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e53bf1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6c4a58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7ff27b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc75daa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaebd3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mem0",
   "language": "python",
   "name": "mem0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
