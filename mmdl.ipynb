{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4118a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional,Tuple\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c82fa02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file implements a Vision Transformer (ViT) model that processes images by:\n",
    "# 1. Splitting the image into patches\n",
    "# 2. Converting each patch into an embedding\n",
    "# 3. Adding positional embeddings\n",
    "# 4. Processing through transformer layers\n",
    "# 5. Outputting final image features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb683f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionConfig:\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size=768,  # Size of the embeddings used throughout the model\n",
    "        intermediate_size=3072,  # Size of the intermediate layer in MLP\n",
    "        num_hidden_layers=12,  # Number of transformer layers\n",
    "        num_attention_heads=12,  # Number of attention heads in each transformer layer\n",
    "        num_channels=3,  # Number of input image channels (3 for RGB)\n",
    "        image_size=224,  # Input image size (224x224 pixels)\n",
    "        patch_size=16,  # Size of each image patch (16x16 pixels)\n",
    "        layer_norm_eps=1e-6,  # Small constant for numerical stability in layer norm\n",
    "        attention_dropout=0.0,  # Dropout rate for attention\n",
    "        num_image_tokens: int = None,  # Number of image tokens (patches) - calculated as (image_size/patch_size)^2\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.num_channels = num_channels\n",
    "        self.patch_size = patch_size\n",
    "        self.image_size = image_size\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.num_image_tokens = num_image_tokens\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68fab2bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Converts input images into patch embeddings and adds positional embeddings'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Converts input images into patch embeddings and adds positional embeddings\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18e6966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionEmbeddings(nn.Module):\n",
    "    \n",
    "    def __init__(self,config: VisionConfig):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.image_size = config.image_size\n",
    "        self.patch_size = config.patch_size\n",
    "        \n",
    "        self.patch_embedding = nn.Conv2d(\n",
    "            in_channels=config.num_channels,\n",
    "            out_channels=self.embed_dim,\n",
    "            kernel_size=self.patch_size,\n",
    "            stride=self.patch_size,\n",
    "            padding='valid'\n",
    "        )\n",
    "        \n",
    "        self.num_patches = (self.image_size // self.patch_size) ** 2\n",
    "        self.num_positions = self.num_patches\n",
    "        \n",
    "        self.position_embedding = nn.Embedding(self.num_positions,self.embed_dim)\n",
    "        \n",
    "        self.register_buffer(\n",
    "            \"position_ids\",\n",
    "            torch.arange(self.num_positions).expand((1,-1)),\n",
    "            persistent=False\n",
    "        )\n",
    "        \n",
    "    def forward(self,pixel_values: torch.FloatTensor):\n",
    "        \n",
    "        _,_,height,width = pixel_values.shape\n",
    "        \n",
    "        patch_embeds = self.patch_embedding(pixel_values)\n",
    "        print('patch1 shape',patch_embeds.shape)\n",
    "        \n",
    "        embeddings = patch_embeds.flatten(2)\n",
    "        print('emb shape',embeddings.shape)\n",
    "        \n",
    "        embeddings = embeddings.transpose(1,2)\n",
    "        print('emb1 shape',embeddings.shape)\n",
    "        \n",
    "        embeddings = embeddings + self.position_embedding(self.position_ids)\n",
    "        print('embeddings shape',embeddings.shape)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "254123e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = VisionConfig(hidden_size=128, image_size=256, patch_size=16, num_channels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85d7e8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch1 shape torch.Size([1, 128, 16, 16])\n",
      "emb shape torch.Size([1, 128, 256])\n",
      "emb1 shape torch.Size([1, 256, 128])\n",
      "embeddings shape torch.Size([1, 256, 128])\n",
      "Output shape: torch.Size([1, 256, 128])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the VisionEmbeddings model\n",
    "model = VisionEmbeddings(config)\n",
    "\n",
    "# Create a sample input tensor (batch_size=1, channels=3, height=256, width=256)\n",
    "input_tensor = torch.randn(1, 3, 256, 256)\n",
    "\n",
    "# Pass the input tensor through the model\n",
    "output = model(input_tensor)\n",
    "\n",
    "print(\"Output shape:\", output.shape)  # Should be (1, num_patches, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23204c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self,config):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        \n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.dropout = config.attention_dropout\n",
    "        \n",
    "        self.k_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.v_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.q_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        \n",
    "        self.out_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        \n",
    "    \n",
    "    def forward(self,hidden_states: torch.Tensor) -> Tuple[torch.Tensor,\n",
    "                                                          Optional[torch.Tensor]]:\n",
    "        \n",
    "        \"\"\"Apply multi-headed self-attention to the input.\n",
    "        \n",
    "        Args:\n",
    "            hidden_states (torch.Tensor): Input tensor of shape [Batch_Size, Num_Patches, Embed_Dim]\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, Optional[torch.Tensor]]: \n",
    "                - Attention output of shape [Batch_Size, Num_Patches, Embed_Dim]\n",
    "                - Attention weights of shape [Batch_Size, Num_Heads, Num_Patches, Num_Patches]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = hidden_states.size()\n",
    "        \n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        \n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        \n",
    "        value_states = self.v_proj(hidden_states)\n",
    "        \n",
    "        query_states = query_states.view(batch_size, seq_len,self.num_heads,self.head_dim).transpose(1,2)\n",
    "        \n",
    "        key_states = key_states.view(batch_size, seq_len,self.num_heads,self.head_dim).transpose(1,2)\n",
    "\n",
    "        value_states = value_states.view(batch_size, seq_len,self.num_heads,self.head_dim).transpose(1,2)\n",
    "        \n",
    "        \n",
    "        attn_weights = (torch.matmul(query_states,key_states.transpose(2,3))*self.scale )\n",
    "        \n",
    "        if attn_weights.size() != (batch_size, self.num_heads, seq_len, seq_len):\n",
    "            raise ValueError(\n",
    "                f\"Attention weights should be of size {(batch_size, self.num_heads, seq_len, seq_len)}, but is\"\n",
    "                f\" {attn_weights.size()}\"\n",
    "            )\n",
    "        \n",
    "        attn_weights = nn.functional.softmax(attn_weights,dim=-1,dtype=torch.float32).to(query_states.dtype)\n",
    "        \n",
    "        attn_weights = nn.functional.dropout(attn_weights,p=self.dropout,training=self.training)\n",
    "        \n",
    "        attn_output = torch.matmul(attn_weights,value_states)\n",
    "        \n",
    "        \n",
    "        if attn_output.size() != (batch_size, self.num_heads, seq_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(batch_size, self.num_heads, seq_len, self.head_dim)}, but is\"\n",
    "                f\" {attn_output.size()}\"\n",
    "            )\n",
    "        \n",
    "        attn_output = attn_output.transpose(1,2).contiguous()\n",
    "        \n",
    "        attn_output = attn_output.reshape(batch_size,seq_len,self.embed_dim)\n",
    "\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "        \n",
    "        return attn_output,attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d49fced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 128])\n",
      "torch.Size([32, 8, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "config = VisionConfig(hidden_size = 128,num_attention_heads = 8,attention_dropout = 0.1)\n",
    "attention_layer = Attention(config)\n",
    "# 假设输入的 hidden_states 形状为 [batch_size, seq_len, embed_dim]\n",
    "hidden_states = torch.randn(32, 10, 128)  # batch_size=32, seq_len=10, embed_dim=128\n",
    "\n",
    "# 前向传播\n",
    "attn_output, attn_weights = attention_layer(hidden_states)\n",
    "\n",
    "# 输出的形状\n",
    "print(attn_output.shape)  # 应该是 [batch_size, seq_len, embed_dim]\n",
    "print(attn_weights.shape)  # 应该是 [batch_size, num_heads, seq_len, seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a04506d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
