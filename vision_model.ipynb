{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5cac2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional,Tuple\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71b1ca29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file implements a Vision Transformer (ViT) model that processes images by:\n",
    "# 1. Splitting the image into patches\n",
    "# 2. Converting each patch into an embedding\n",
    "# 3. Adding positional embeddings\n",
    "# 4. Processing through transformer layers\n",
    "# 5. Outputting final image features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6a57987",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionConfig:\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size=768,  # Size of the embeddings used throughout the model\n",
    "        intermediate_size=3072,  # Size of the intermediate layer in MLP\n",
    "        num_hidden_layers=12,  # Number of transformer layers\n",
    "        num_attention_heads=12,  # Number of attention heads in each transformer layer\n",
    "        num_channels=3,  # Number of input image channels (3 for RGB)\n",
    "        image_size=224,  # Input image size (224x224 pixels)\n",
    "        patch_size=16,  # Size of each image patch (16x16 pixels)\n",
    "        layer_norm_eps=1e-6,  # Small constant for numerical stability in layer norm\n",
    "        attention_dropout=0.0,  # Dropout rate for attention\n",
    "        num_image_tokens: int = None,  # Number of image tokens (patches) - calculated as (image_size/patch_size)^2\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.num_channels = num_channels\n",
    "        self.patch_size = patch_size\n",
    "        self.image_size = image_size\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.num_image_tokens = num_image_tokens\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "626edc89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Converts input images into patch embeddings and adds positional embeddings'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Converts input images into patch embeddings and adds positional embeddings\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "028ba38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionEmbeddings(nn.Module):\n",
    "    \n",
    "    def __init__(self,config: VisionConfig):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.image_size = config.image_size\n",
    "        self.patch_size = config.patch_size\n",
    "        \n",
    "        self.patch_embedding = nn.Conv2d(\n",
    "            in_channels=config.num_channels,\n",
    "            out_channels=self.embed_dim,\n",
    "            kernel_size=self.patch_size,\n",
    "            stride=self.patch_size,\n",
    "            padding='valid'\n",
    "        )\n",
    "        \n",
    "        self.num_patches = (self.image_size // self.patch_size) ** 2\n",
    "        self.num_positions = self.num_patches\n",
    "        \n",
    "        self.position_embedding = nn.Embedding(self.num_positions,self.embed_dim)\n",
    "        \n",
    "        self.register_buffer(\n",
    "            \"position_ids\",\n",
    "            torch.arange(self.num_positions).expand((1,-1)),\n",
    "            persistent=False\n",
    "        )\n",
    "        \n",
    "    def forward(self,pixel_values: torch.FloatTensor):\n",
    "        \n",
    "        _,_,height,width = pixel_values.shape\n",
    "        \n",
    "        patch_embeds = self.patch_embedding(pixel_values)\n",
    "        print('patch1 shape',patch_embeds.shape)\n",
    "        \n",
    "        embeddings = patch_embeds.flatten(2)\n",
    "        print('emb shape',embeddings.shape)\n",
    "        \n",
    "        embeddings = embeddings.transpose(1,2)\n",
    "        print('emb1 shape',embeddings.shape)\n",
    "        \n",
    "        embeddings = embeddings + self.position_embedding(self.position_ids)\n",
    "        print('embeddings shape',embeddings.shape)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e042f358",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = VisionConfig(hidden_size=128, image_size=256, patch_size=16, num_channels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fdd8ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch1 shape torch.Size([1, 128, 16, 16])\n",
      "emb shape torch.Size([1, 128, 256])\n",
      "emb1 shape torch.Size([1, 256, 128])\n",
      "embeddings shape torch.Size([1, 256, 128])\n",
      "Output shape: torch.Size([1, 256, 128])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the VisionEmbeddings model\n",
    "model = VisionEmbeddings(config)\n",
    "\n",
    "# Create a sample input tensor (batch_size=1, channels=3, height=256, width=256)\n",
    "input_tensor = torch.randn(1, 3, 256, 256)\n",
    "\n",
    "# Pass the input tensor through the model\n",
    "output = model(input_tensor)\n",
    "\n",
    "print(\"Output shape:\", output.shape)  # Should be (1, num_patches, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da303834",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self,config):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        \n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.dropout = config.attention_dropout\n",
    "        \n",
    "        self.k_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.v_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.q_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        \n",
    "        self.out_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        \n",
    "    \n",
    "    def forward(self,hidden_states: torch.Tensor) -> Tuple[torch.Tensor,\n",
    "                                                          Optional[torch.Tensor]]:\n",
    "        \n",
    "        \"\"\"Apply multi-headed self-attention to the input.\n",
    "        \n",
    "        Args:\n",
    "            hidden_states (torch.Tensor): Input tensor of shape [Batch_Size, Num_Patches, Embed_Dim]\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, Optional[torch.Tensor]]: \n",
    "                - Attention output of shape [Batch_Size, Num_Patches, Embed_Dim]\n",
    "                - Attention weights of shape [Batch_Size, Num_Heads, Num_Patches, Num_Patches]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = hidden_states.size()\n",
    "        \n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        \n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        \n",
    "        value_states = self.v_proj(hidden_states)\n",
    "        \n",
    "        query_states = query_states.view(batch_size, seq_len,self.num_heads,self.head_dim).transpose(1,2)\n",
    "        \n",
    "        key_states = key_states.view(batch_size, seq_len,self.num_heads,self.head_dim).transpose(1,2)\n",
    "\n",
    "        value_states = value_states.view(batch_size, seq_len,self.num_heads,self.head_dim).transpose(1,2)\n",
    "        \n",
    "        \n",
    "        attn_weights = (torch.matmul(query_states,key_states.transpose(2,3))*self.scale )\n",
    "        \n",
    "        if attn_weights.size() != (batch_size, self.num_heads, seq_len, seq_len):\n",
    "            raise ValueError(\n",
    "                f\"Attention weights should be of size {(batch_size, self.num_heads, seq_len, seq_len)}, but is\"\n",
    "                f\" {attn_weights.size()}\"\n",
    "            )\n",
    "        \n",
    "        attn_weights = nn.functional.softmax(attn_weights,dim=-1,dtype=torch.float32).to(query_states.dtype)\n",
    "        \n",
    "        attn_weights = nn.functional.dropout(attn_weights,p=self.dropout,training=self.training)\n",
    "        \n",
    "        attn_output = torch.matmul(attn_weights,value_states)\n",
    "        \n",
    "        \n",
    "        if attn_output.size() != (batch_size, self.num_heads, seq_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(batch_size, self.num_heads, seq_len, self.head_dim)}, but is\"\n",
    "                f\" {attn_output.size()}\"\n",
    "            )\n",
    "        \n",
    "        attn_output = attn_output.transpose(1,2).contiguous()\n",
    "        \n",
    "        attn_output = attn_output.reshape(batch_size,seq_len,self.embed_dim)\n",
    "\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "        \n",
    "        return attn_output,attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74c3293d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 128])\n",
      "torch.Size([32, 8, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "config = VisionConfig(hidden_size = 128,num_attention_heads = 8,attention_dropout = 0.1)\n",
    "attention_layer = Attention(config)\n",
    "# 假设输入的 hidden_states 形状为 [batch_size, seq_len, embed_dim]\n",
    "hidden_states = torch.randn(32, 10, 128)  # batch_size=32, seq_len=10, embed_dim=128\n",
    "\n",
    "# 前向传播\n",
    "attn_output, attn_weights = attention_layer(hidden_states)\n",
    "\n",
    "# 输出的形状\n",
    "print(attn_output.shape)  # 应该是 [batch_size, seq_len, embed_dim]\n",
    "print(attn_weights.shape)  # 应该是 [batch_size, num_heads, seq_len, seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d01abecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self,config):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        self.fc1 = nn.Linear(config.hidden_size,config.intermediate_size)\n",
    "        self.fc2 = nn.Linear(config.intermediate_size,config.hidden_size)\n",
    "        \n",
    "    \n",
    "    def forward(self,hidden_states:torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        hidden_states = self.fc1(hidden_states)\n",
    "        \n",
    "        hidden_states = nn.functional.gelu(hidden_states,approximate='tanh')\n",
    "        \n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        \n",
    "        return hidden_states\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f5c39a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "        \n",
    "config = config = VisionConfig(hidden_size = 128,intermediate_size = 64)\n",
    "mlp_layer = MLP(config)\n",
    "\n",
    "# 假设输入的 hidden_states 形状为 [batch_size, seq_len, embed_dim]\n",
    "hidden_states = torch.randn(32, 10, 128)  # batch_size=32, seq_len=10, embed_dim=128\n",
    "\n",
    "# 前向传播\n",
    "output = mlp_layer(hidden_states)\n",
    "\n",
    "# 输出的形状\n",
    "print(output.shape)  # 应该是 [batch_size, seq_len, hidden_size] = [32, 10, 128]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "46a001bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self,config:VisionConfig):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.self_attn = Attention(config)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.embed_dim,eps=config.layer_norm_eps)\n",
    "        self.mlp = MLP(config)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.embed_dim,eps=config.layer_norm_eps)\n",
    "    \n",
    "    \n",
    "    def forward(self,hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        residual = hidden_states\n",
    "        hidden_states = self.layer_norm1(hidden_states)\n",
    "        \n",
    "        hidden_states,_ = self.self_attn(hidden_states=hidden_states)\n",
    "        \n",
    "        hidden_states = residual + hidden_states\n",
    "        \n",
    "        residual = hidden_states\n",
    "        \n",
    "        hidden_states = self.layer_norm2(hidden_states)\n",
    "        \n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        \n",
    "        hidden_states = residual + hidden_states\n",
    "        \n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ddcb2786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "39c992c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "# 创建配置对象\n",
    "config = VisionConfig()\n",
    "\n",
    "# 创建EncoderLayer对象\n",
    "encoder_layer = EncoderLayer(config)\n",
    "\n",
    "# 创建一个随机输入张量，假设batch size为10，序列长度为20\n",
    "hidden_states = torch.randn(20, 10, config.hidden_size)\n",
    "\n",
    "# 传入编码器层\n",
    "output = encoder_layer(hidden_states)\n",
    "\n",
    "print(output.shape)  # 输出的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b3fdd040",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,config: VisionConfig):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layers = nn.ModuleList([\n",
    "          EncoderLayer(config)  for _ in range(config.num_hidden_layers)\n",
    "        ])\n",
    "        \n",
    "    def forward(self,input_embeds: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        hidden_states = input_embeds\n",
    "        \n",
    "        for encoder in self.layers:\n",
    "            hidden_states = encoder(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "675d22a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "# 创建配置对象\n",
    "config = VisionConfig()\n",
    "\n",
    "# 创建EncoderLayer对象\n",
    "encoder_layer = Encoder(config)\n",
    "\n",
    "# 创建一个随机输入张量，假设batch size为10，序列长度为20\n",
    "hidden_states = torch.randn(20, 10, config.hidden_size)\n",
    "\n",
    "# 传入编码器层\n",
    "output = encoder_layer(hidden_states)\n",
    "\n",
    "print(output.shape)  # 输出的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d0bb7830",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(self,config: VisionConfig):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        embed_dim = config.hidden_size\n",
    "        \n",
    "        self.embeddings = VisionEmbeddings(config)\n",
    "        self.encoder = Encoder(config)\n",
    "        self.post_layernorm = nn.LayerNorm(embed_dim,eps=config.layer_norm_eps)\n",
    "    \n",
    "    def forward(self,pixel_values: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        hidden_states = self.embeddings(pixel_values)\n",
    "        \n",
    "        last_hidden_state = self.encoder(input_embeds=hidden_states)\n",
    "        \n",
    "        last_hidden_state = self.post_layernorm(last_hidden_state)\n",
    "        \n",
    "        return last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2c8c6dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch1 shape torch.Size([1, 768, 14, 14])\n",
      "emb shape torch.Size([1, 768, 196])\n",
      "emb1 shape torch.Size([1, 196, 768])\n",
      "embeddings shape torch.Size([1, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "config = VisionConfig()\n",
    "\n",
    "model = VisionTransformer(config)\n",
    "\n",
    "hidden_state = model(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0efacf32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 196, 768])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a580d008",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionModel(nn.Module):\n",
    "    \n",
    "    def __init__(self,config: VisionConfig):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.vision_model = VisionTransformer(config)\n",
    "    \n",
    "    \n",
    "    def forward(self,pixel_values) -> Tuple:\n",
    "        \n",
    "        return self.vision_model(pixel_values=pixel_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f9e27d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch1 shape torch.Size([10, 768, 14, 14])\n",
      "emb shape torch.Size([10, 768, 196])\n",
      "emb1 shape torch.Size([10, 196, 768])\n",
      "embeddings shape torch.Size([10, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.randn(10, 3, 224, 224)\n",
    "\n",
    "config = VisionConfig()\n",
    "\n",
    "model = VisionModel(config)\n",
    "\n",
    "hidden_state = model(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cb483b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 196, 768])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e9b77c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
